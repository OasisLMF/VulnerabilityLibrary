import pandas as pd
import xml.etree.ElementTree as ET
import os
import itertools
from itertools import permutations
import numpy as np
import csv
from scipy import interpolate
from scipy.stats import beta
import time

# tree = ET.parse('./GEM/vulnerability_structural.xml')
# root = tree.getroot()
# print (root[0][2][0].attrib['imt'])
# # print (np.linspace(0, 1, num=100, endpoint=True).round(2))

#Refer to numbers using .text
#Refer to dictionary using .attrib
#Intensity Type is root[0][i][0].attrib['imt']
#Taxonomy is root[0][i].attrib['id']
#Intensity is root[0][i][0].text
#MeanLR is root[0][i][1].text
#CoV is root[0][i][2].text
    
def iteration():
    #array = [0 for x in range (len(root[0]))]
    #parray = [0 for x in range (len(root[0]))]
    array = []
    parray=[]
    some = ['for', 'him']
    where = ['as', 'tech']
    count = 0 
    for each, element in zip (some, where):
        if count > 0:
            break
        for i in range (len(root[0])):
            print (each, element)
            array += [i+1]
            parray = array[::-1]
        count+=1
    return array, parray

def indexing():
    array = [0 for x in range (len(root[0])-1)]
    for i in range (len(array)):
        array[i] = root[0][i+1][2].text
    print (array[-1])

def json_headers():
    IMT=[]
    TAX=[]
    INT=[]
    headers = [IMT, TAX, INT]
    length = [[0] * (len(root[0])-1) for i in range(len(headers))]
    for i in range (len(headers)):
        headers[i] = length[i]
    for i in range (len(IMT)):
        IMT[i] = root[0][i+1][0].attrib['imt']
        TAX[i] = root[0][i+1][0].text
        INT[i] = root[0][i+1].attrib['id']
    return IMT, TAX, INT

def directories():
    rootdir = "./global_vulnerability_model"
    CONT = []
    COUNT = []
    paths = []
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            subdir = subdir.replace('\\', '/')
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_structural.xml"):
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                paths += [filepath]
                CONT += [location[0]]
                COUNT += [location[1]]
    return paths, CONT, COUNT


# Improving familiarity with tuples in dictionaries
""" path = ''
location_dict = {("continent", "country"): path}
location1 = ('Africa', 'Algeria')
location_dict[location1] = '/global.xml'
location2 = ('Africa', 'Zimbabwe')
location_dict[location2] = '/global2.xml'
print (location_dict[('Africa', 'Zimbabwe')]) """

def directories_improved():
    rootdir = "./global_vulnerability_model"
    path = ''
    location_dict = {}
    count = 0
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            subdir = subdir.replace('\\', '/')
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_structural.xml"):
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                location_dict [location[0], location[1]] = filepath
                count = count + 1
            # elif count > 2:
            #     break
    return location_dict, 'structural'

def row_to_column():
    data = '1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 6.37791 4.6492 3.54552 2.84342 2.39961 2.11914 1.93862 1.81631 1.72522 1.64806 1.57385 1.49596 1.41106 1.31843 1.21919 1.11554 1.01005 0.905108 0.802719 0.704423 0.611361 0.524368 0.444067 0.370933 0.305331 0.247515 0.197621 0.155641 0.118911 0.0881118 0.0638218 0.0451459 0.0311658 0.0209857 0.0137781 1e-08 1e-08 1e-08'
    data = data.strip().split(' ')
    with open('./GEM/converted.csv', 'a') as file:
        for row in data:
            file.write(row)
            file.write('\n')

def compare_dict():
    # If have something unique, use that as the key
    cur_vuln_id = 1
    vuln_ids = {}

    Continents = ['Africa', 'Africa', 'Europe' ]
    Countries = ['Algeria', 'Angola', 'UK']
    CT = ['Structural']

    for (continent, country) in zip (Continents, Countries):
        if (continent, country) not in vuln_ids:
            vuln_ids[(continent, country)] = cur_vuln_id
            cur_vuln_id += 1
        
        # for each in id_list:
        #     if  rec['Continent', 'Country'] not in each.values():
        #         id_list.append(rec)

    return vuln_ids

def directories():
    rootdir = "./global_vulnerability_model"
    location_dict = {}
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            subdir = subdir.replace('\\', '/')
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_structural.xml"):
                coverage_type = 'Structural'
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                location_dict [location[0], location[1]] = filepath
    return location_dict

def get_taxonomies(location_list):
    tax_ids= {}
    cur_tax_id = 1
    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
        
        for i in range (len(root[0])-1):
            for j in root[0][i+1][0].text.strip().split(' '):
                taxonomy = root[0][i+1].attrib['id']
                if taxonomy not in tax_ids:
                    tax_ids[taxonomy] = cur_tax_id
                    cur_tax_id += 1
                    rec = {
                        # "TaxID": tax_ids[taxonomy], 
                        "Taxonomy": taxonomy
                    }
                    yield rec

# print (np.linspace(0, 100, num=1000, endpoint=True).round(1))

def run_beta_dist(alpha_val, beta_val):
    damage_bin_list = np.linspace(0,1,100, endpoint=True)

    # several options to compute beta distribution
    # probs = np.linspace(beta.ppf(0.01, alpha_val, beta_val), beta.cdf(1, alpha_val, beta_val), 100)
    vals = beta.cdf(damage_bin_list, alpha_val, beta_val)
    # unit test - way to check values sensible
    # new_vals = np.append(vals, [0])
    # res = float(vals[None, :]) - float(new_vals[:, None])
    newarr = np.diff(vals, axis=0)
    total = np.sum(newarr)
    # check: total of cumulative differences should be 1 
    # print (total)
    return newarr 

def run_beta_dist(alpha_val, beta_val):
    damage_bin_list = np.linspace(0,1,100, endpoint=True)
    
    # several options to compute beta distribution- either use beta.ppf or beta.cdf 
    # np.linspace(beta.ppf(0.01, alpha_val, beta_val), beta.cdf(1, alpha_val, beta_val), 100)
    vals = beta.cdf(damage_bin_list, alpha_val, beta_val)

    # compute difference between cumulative probabilities
    cum_diff_vals = np.diff(vals, axis=0)
    
    # check: total of cumulative differences should be 1 
    # print (np.sum(newarr))
    return cum_diff_vals

def create_int_bins():
    with open('./GEM/intensity_bins.txt') as bins_file:
        int_val_lb = [float(0)]
        int_val_ub = []
        int_val_mp = []
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_val_lb.append(float(line))
                int_val_ub.append(float(line))
        int_val_ub.append(float (int_val_lb[-1]))
        for i, j in zip (int_val_lb, int_val_ub):
            int_val_mp.append((i+j) / 2)

def generate_bins(location_list):
    damage_bin_list = np.linspace(0,1,100, endpoint=True)

    create_int_bins()
    damage_bins = np.linspace(0, 1, num=100, endpoint=True).round(2)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1])
    a,b = 0,1

    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
    
        for i in range (len(root[0])-1):
            int_vals = []
            mean_LRs = []
            std_dev_vals = []
            beta_params = []
            
            # for j, (intensity, meanLR, CoV, m, n) in enumerate (zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' '), int_val_lb, int_val_ub)):
            for intensity, meanLR, CoV in zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' ')):
                meanLR = float (meanLR)
                intensity = float (intensity)
                CoV = float (CoV)

                int_vals.append (intensity)
                mean_LRs.append (meanLR)
                std_dev_val = (meanLR * CoV)
                std_dev_vals.append(meanLR * CoV)

            int_vs_mean = interpolate.interp1d(int_vals, mean_LRs, kind='cubic')
            int_vs_std= interpolate.interp1d(int_vals, std_dev_vals, kind='cubic')
            interp_mean = int_vs_mean (int_vals)
            interp_std_dev = int_vs_std(int_vals)
            
            # def damage_probs()
            for mean_val, std_dev_val in zip (interp_mean, interp_std_dev):
                alpha_val = (mean_val-a)/(b-a) * mean_val*(1-mean_val) / (std_dev_val - 1)
                beta_val = alpha_val * (1-mean_val) / mean_val
                # beta function automatically runs has start and end interval from 0 to 1
                # see if 0.01 can be replaced by 0 - I expect it will not work 
                # probs = np.linspace(beta.ppf(0.01, alpha, beta), beta.ppf(1, alpha, beta), 100)
                # probs = run_beta_dist(alpha_val, beta_val)
                vals = beta.cdf(damage_bin_list, alpha_val, beta_val)
                np.diff(vals, axis=0)
                
                for i, prob in enumerate (vals):
                    damage_prob = {'damage_bin_index': i, 'bin_from': damage_bins[i+1], 'bin_to': damage_bins[i+2], 'probabilities': prob}
                    yield damage_prob          

def run_taxonomies():    
    directory = directories()
    location_list = directory
    header = get_taxonomies(location_list)
    df = pd.DataFrame(header)
    # print (df.head(5))
    df.to_csv('GEM/taxonomies.csv', index=False)

# how to add number onto end of list created by numpy
def append_to_list():
    # x = np.arange(0,1,0.1) 
    # y = np.append(x, [0])
    # print (y)
    damage_bins = np.linspace(0, 1, num=100, endpoint=True).round(2)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1])
    print (damage_bins)

def concatenate_generators(gen1, gen2):
    yield from gen1
    yield from gen2
    # return "Generators concatenated successfully."
 
# for result in concatenate_generators(range(3), range(10,12)):
#     print(result)
#     df = pd.DataFrame(result)

def something():
    a = run_beta_dist(1.01, 0.672)
    b = {}
    for i, prob in enumerate(a):
        b = {'bin_no': i, 'probability': prob}
        yield b

def pass_data(df):
    df['random'] = [x for x in np.linspace(0,1,99)]
    df['random2'] = [x for x in np.linspace(1,0,99)]

def to_remove():
    rootdir = "./global_vulnerability_model"
    location_dict = {}
    # Count included so that not all files are analysed every single run
    count = 0
    coverage_types = ['structural', 'nonstructural','contents']
    # Only looking at data for vulnerability_structural files now
    coverage_type = coverage_types [0]
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            subdir = subdir.replace('\\', '/')
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_{cov_type}.xml".format(cov_type = coverage_type)):
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                # location[0] represents a continent 
                # location[1] represents a country
                location_dict [location[0], location[1]] = filepath
                count += 1
            if count > 5:
                break
    print( location_dict, coverage_type)

def create_vuln_dict():
    # Open vulnerability dictionary file from the 'Inputs' folder 
    header = True
    with open('./VulnerabilityLibrary/Inputs/vulnerability_dict.csv') as file:
        vuln_dict = {}
        vuln_dict_two = {}
        # Read in bins data from opened file
        for line in file:
            line = line.replace('\n', '').split(',')
            if header == True:
                line = np.asarray(line).tolist()
                vuln_dict_two[line[line.index('Vulnerability_ID')]] = line[line.index('Vulnerability_ID')]
                header = False
                print (line)
            # if line:
            #     int_vals_lb.append(float(line))
            #     int_vals_ub.append(float(line))
            vuln_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
    return vuln_dict, vuln_dict_two


def create_vuln_ids(location_list):
    vuln_ids = {}
    cur_vuln_id = 1
    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()

        for i in range (len(root[0])-1):
            int_mes_type = root[0][i+1][0].attrib['imt']
            taxonomy = root[0][i+1].attrib['id']
            if (continent, country) not in vuln_ids:
                vuln_ids[(continent, country, int_mes_type, taxonomy)] = cur_vuln_id
                cur_vuln_id += 1
                id_dict = {
                    "ID": vuln_ids[(continent, country, int_mes_type, taxonomy)],
                    "Continent": continent,
                    "Country": country,
                    "IntensityMeasurementType": int_mes_type,
                    "Taxonomy": taxonomy
                }
                yield id_dict
""" int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
int_vals_ub.append(float (int_vals_lb[-1]))
# Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
    int_vals_mp.append((val_lb + val_ub) / 2)
    int_bin_dict = {'bin_from': int_vals_lb, 
                    'bin_to': int_vals_ub, 
                    'interpolation': int_vals_mp
                    }
# Format data into a table and output csv file
intensity_df = pd.DataFrame(int_bin_dict) """

def create_footprint(location_list, vuln_ids):
    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()

        for i in range (len(root[0])-1):
            int_mes_type = root[0][i+1][0].attrib['imt']
            taxonomy = root[0][i+1].attrib['id']
            # for j, (intensity, MLR, CoV) in enumerate (zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' '))):
            # wrong - for j, each in enumerate(root[0][i+1][0].text.strip().split(' ')):
            for j in range (22):
                for k in range(52):
                    rec = {
                        # converted valuation_id to string so that an integer would be displayed
                        "valuation_id":  str(vuln_ids[(continent, country, int_mes_type, taxonomy)]),
                        "intensity_bin_id": j+1,
                        "damage_bin_id": "{:.0f}".format(k), 
                        }
                    yield rec

def create_footprint(location_list, vuln_ids):
    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()

        for i in range (len(root[0])-1):
            int_mes_type = root[0][i+1][0].attrib['imt']
            taxonomy = root[0][i+1].attrib['id']
            attribute_2 = taxonomy
            # for j, (intensity, MLR, CoV) in enumerate (zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' '))):
            # wrong - for j, each in enumerate(root[0][i+1][0].text.strip().split(' ')):
            for j in range (22):
                for k in range(52):
                    rec = {
                        # converted valuation_id to string so that an integer would be displayed
                        "valuation_id":  str(vuln_ids[(continent, country, int_mes_type, taxonomy)]),
                        "intensity_bin_id": j+1,
                        "damage_bin_id": "{:.0f}".format(k), 
                        }
                    yield rec

def get_height_codes():
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open('./VulnerabilityLibrary/Inputs/height_dict.csv') as file:
        height_dict = {}
        # Read in bins data from opened file
        for line in file:
            line = line.replace('\n', '').split(',')
            # height_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
            height_dict[line[1]] = line[2]
    print (height_dict['H25'])
    return height_dict

def get_constr_codes():
    with open('./VulnerabilityLibrary/Inputs/construction_dict.csv') as file:
        constr_dict = {}
        # Read in bins data from opened file
        for line in file:
            line = line.replace('\n', '').split(',')
            # height_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
            constr_dict[line[3]] = line[4]
    print (constr_dict['INF'])
    return constr_dict

def improved_get_constr_codes():
    header = True
    constr_dict = {}
    with open(r'VulnerabilityLibrary\ParisWindstormVariant\keys_data\construction_dict.csv') as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                constr_type = data.index('attribute2')
                constr_id = data.index('attribute2_id')
                header = False
            else:
                constr_dict[data[constr_type]] = data[constr_id]
    return constr_dict

def find_nearest(array_vals, val):
    array_vals = np.asarray(array_vals)
    idx = (np.abs(array_vals - val)).argmin()
    yield idx

def get_bin_index():
    with open('./VulnerabilityLibrary/Inputs/intensity_bins.txt') as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        int_vals_mp = []
        # Read in bins data from opened file
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_bin_vals = int_vals_lb[1:]
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
        for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
            int_vals_mp.append((val_lb + val_ub) / 2)
    return int_bin_vals, int_vals_mp

def correct_int_bin(location_list, int_list):
    num_damage_bins = 100
    peril_id = 'QEQ'
    count = 0 
    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
   
        for i in range (len(root[0])-1):
            array_vals = root[0][i+1][0].text.strip().split(' ').round(6)
            count += 1
            if count > 2:
                break
            for j, int_value in enumerate(array_vals):
                    a = find_nearest(int_list, float(int_value))
                    b = list(a)
                    for k in range(3):
                        print (k, b, int_value)

def correct_bin(int_mp_vals):
    # midpoint of bins 1 to 6 represented below
    bins = [0,3.5,6.5,7]
    # int_list = []
    # for i in range (len(bins) - 1):
    #     mid_val = (bins[i] + bins [i+1])/2
    #     int_list.append(mid_val)
    
    # int_list = [0.375,1,125,1.875,2.625,3.375,4.125]
    for int_value in bins:
        bin_index = list(find_nearest(int_mp_vals, float(int_value)))[0]
        bin_index += 1
        if int_value > min(bins) and int_value < max(bins):
            print (int_value, 'belongs in bin', bin_index)
        else:
            print('Intensity value of', int_value, 'is not within range of intensity bins inputed')
        # print (bin_index+1)
            
def get_occup_dict():
    header = True
    occup_dict = {}
    with open (r'VulnerabilityLibrary\ParisWindstormVariant\keys_data\occupancy_dict.csv', "r") as file:
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                occup_type = data.index('attribute6')
                occup_id = data.index('attribute6_id')
                header = False
            else:
                occup_dict[data[occup_type]] = data[occup_id]
    return occup_dict

def get_vuln_ids():
    header = True
    vuln_dict = {}
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open('./VulnerabilityLibrary/Inputs/vulns_dict.csv') as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                vuln_id = data.index('vulnerability_id')
                vuln_continent = data.index('Continent')
                vuln_country = data.index('Country')
                vuln_taxonomy = data.index('Taxonomy')
                header = False
            else:
                vuln_dict[(data[vuln_continent], data[vuln_country], data[vuln_taxonomy])] = data[vuln_id]
                # vuln_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
        return vuln_dict
    
def get_data(location_list, vuln_dict):
    with open ('./VulnerabilityLibrary/Dicts/vulnerability_dict.csv', 'wb') as dict_file:
        header = True
        current_location = ('','')
        for i, (continent, country, taxonomy) in enumerate(vuln_dict):
            if (continent, country) != current_location:
                print ('running', continent, country)
                current_location = (continent, country)
                pathway = location_list[(continent, country)]
                tree = ET.parse(pathway)
                root = tree.getroot()
                count = 1
            int_mes_type = root[0][i+1][0].attrib['imt']
            int_vals = root[0][i+1][0].text
            mean_LRs = root[0][i+1][1].text
            coeff_vars = root[0][i+1][2].text
            count += 1

def partial_match(key, d):
    for k, v in d.items():
        temp = ('','')
        if all(temp != (k1,k2) for k1, k2 in zip(k, key)):
            print (v)


def get_height_codes():
    header = True
    height_dict = {}
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open(r'VulnerabilityLibrary\ParisWindstormVariant\keys_data\height_dict.csv') as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                height_code = data.index('attribute4')
                height_id = data.index('attribute4_id')
                header = False
            else:
                height_dict[data[height_code]] = data[height_id]
            # height_dict[line[1]] = line[2]
    return height_dict
            

def get_constr_codes():
    header = True
    constr_dict = {}
    with open(r'VulnerabilityLibrary\ParisWindstormVariant\keys_data\construction_dict.csv') as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                constr_type = data.index('attribute2')
                constr_id = data.index('attribute2_id')
                header = False
            else:
                constr_dict[data[constr_type]] = data[constr_id]
    return constr_dict

def get_occup_codes():
    # occup_dict = {'RES': 1, 'COM': 2, 'IND': 3}
    # return occup_dict
    header = True
    occup_dict = {}
    with open (r'VulnerabilityLibrary\ParisWindstormVariant\keys_data\occupancy_dict.csv', "r") as file:
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                occup_type = data.index('attribute6')
                occup_id = data.index('attribute6_id')
                header = False
            else:
                occup_dict[data[occup_type]] = data[occup_id]
    return occup_dict

def get_cov_type_codes():
    cov_type_dict = {'structural': 1, 'nonstructural': 2, 'contents': 3}
    return cov_type_dict

def main():
    vuln = get_vuln_ids()
    for i, (continent, country, taxonomy) in enumerate(vuln):
        list(partial_match((continent, country), vuln))
    # location_list = directories_improved()[0]
    # get_data(location_list, vuln_dict)

incr = 0.01
print (np.arange(incr,1 + incr, incr))


""" for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
    
        for i in range (len(root[0])-1):
            int_vals = []
            mean_LRs = []
            cov_vals = []
            for intensity, meanLR, CoV in zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' ')):
                int_vals.append (float(intensity))
                mean_LRs.append (float(meanLR))
                cov_vals.append(float(CoV))
            interp_cov_val = interpolate.interp1d(int_vals, cov_vals, kind = 'cubic') """