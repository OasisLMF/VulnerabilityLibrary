""" global flood damage functions """

# implement functions originally used for the code to compute the GEM Vulnerability Model 
from FunctionsModel import *

parser.add_argument('-imt', "--int-mes-types", default='flood_depth_metres', type=str, help="enter the unique intensity measurement types in the model data")
parser.add_argument('-F', "--data-path", default="global_flood_damage_functions.xlsx", help = "input name of file containing the vulnerability functions")

exposure_flie_path = r'C:\Users\Oyeda\OneDrive\Documents\Oasis_Internship\VulnerabilityLibrary\global_flood_damage_functions.xlsx'

def create_vuln_ids(region_list, occup_dict, damage_classes):
    vuln_ids = {}
    cur_vuln_id = 1    

    for continent in region_list:
        for damage_class in damage_classes:
            if (continent, damage_class) not in vuln_ids:
                vuln_ids[(continent, damage_class)] = cur_vuln_id
                cur_vuln_id += 1
                occup_type = damage_class[:3].upper()
                if occup_type in occup_dict:
                    oed_occup_code = occup_dict[occup_type]
                id_dict = {
                    "vulnerability_id": vuln_ids[(continent, damage_class)],
                    "region": continent,
                    "occupancy_code": oed_occup_code
                }
                yield id_dict

def get_vuln_ids(vulns_df):
    vuln_dict = {}

    for index, row in vulns_df.iterrows():
        vuln_id = row['vulnerability_id']
        vuln_cont = row['region']
        vuln_type = row['occupancy_code']
        vuln_dict[(vuln_cont, vuln_type)] = vuln_id
    return vuln_dict

def use_excel(data_path):
    df_xlsx = pd.ExcelFile(data_path)
    data = pd.read_excel(df_xlsx, 'Damage functions', na_filter=False, header=[1,2])
    damage_classes = []
    int_vals_dict = {}
    region_dict = {}
    probs_list_dict = {}
    std_devs_dict = {} 
    excel_rows = []

    for dam_class in data['Damage\nclass'].iloc[:, [0][0]]:
        if dam_class:
            damage_classes.append(dam_class)

    for region in data['Damage function']:
        location = ''
        cur_region = region
        if '\n' in region:
            region = ' '.join(region.split('\n'))

        if ' ' in region:
            for word in region.split():
                word = word.capitalize()
                if not location:
                    location = word
                else:
                    location = ' '.join((location, word))
        else:
            location = region.capitalize()
        region_dict[cur_region] = location
    
    header = True
    for i, dam_class in enumerate (data['Damage\nclass'].iloc[:, [0][0]]):
        if header:
            start = i
        if dam_class and not (header):
            end = i
            excel_rows.append((start, end))
            start = end
        header = False
    # create list of list of intensity bin values for each damage class - SHOULD THIS BE GENERAL?
    for region in data['Damage function']:
        for damage_class in damage_classes:
            for index in excel_rows:
                vals_set = []
                for i, val in enumerate(data['Flood depth,\n[m]'].iloc[:, [0][0]]):
                    if i == index[1]:
                        int_vals_dict[(region_dict[region], damage_class)] = vals_set
                        break
                    vals_set.append(val)
                break
    
    for region in data['Damage function']:
        for dam_class in damage_classes:
            region_probs = []
            for prob in data['Damage function'][region]:
                if type(prob) == int or type(prob) == float:
                    region_probs.append(prob)
                else:
                    region_probs.append(0)
            cum_region_probs = list(np.diff(region_probs, axis=0, prepend=0, append=region_probs[-1]))
            probs_list_dict[region_dict[region], dam_class] = cum_region_probs
    
    for region in data['Damage function']:
        for dam_class in damage_classes:
            region_std_devs = []
            # is standard deviations neccessary since damage functions already give probabilities?
            for std_dev in data['Standard deviation'][region]:
                if type(std_dev) == int or type(std_dev) == float:
                    region_std_devs.append(std_dev)
                else: 
                    region_std_devs.append(0)
            std_devs_dict[region_dict[region], dam_class] = region_std_devs

    return damage_classes, int_vals_dict, region_dict, probs_list_dict, std_devs_dict

def analyse_exposure():
    df_xlsx = pd.ExcelFile(exposure_file_path)
    data = pd.read_excel(df_xlsx, 'Damage functions', na_filter=False, header=[1,2])
    return "value"

def main(working_folder, num_damage_bins, oasis_model_folder, data_path, country_specific, continent_specific, int_mes_types):
    t_zero = time.time()
    incr = 1/num_damage_bins
    int_mes_types = int_mes_types.split()

    # only true if both true - check documentation
    if continent_specific:
        print ("Cannot input both specific countries and continents")
        sys.exit()
    
    # produces string that is the name of the folder in which all the model's data and code is contained - default is 'VulnerabilityLibrary'
    path_stem = init_run(working_folder)

    # hazard intensity bins created and a dictionary is outputted 
    int_bins, int_mp_vals = init_int_bins(path_stem)
    int_bins_dict = create_int_bins(int_bins, int_mp_vals, int_mes_types)

    int_bins_df = pd.DataFrame(int_bins_dict)
    int_bin_dict_path = PurePath.joinpath(path_stem, int_dict_path)
    int_bins_df.to_csv(int_bin_dict_path, index=False)
    
    # damage bin list does not have repeated start or end - length 102 - same as number of damage bins
    damage_bins, damage_bins_list = init_damage_bins(incr)
    dam_bins_dict = create_damage_bins(damage_bins)

    damage_bins_df = pd.DataFrame(dam_bins_dict)
    damage_bins_path = PurePath.joinpath(path_stem, oasis_model_folder, dam_bin_path)
    damage_bins_df.to_csv(damage_bins_path, index=False)

    # list of all locations with data in vulnerability models folder stored in dictionary as key and the value is the relative path
    rootdir = PurePath.joinpath(path_stem, data_path)
    damage_classes, int_vals_list, region_dict, probs_list_dict, std_devs_dict = use_excel(rootdir)
    region_list = [each[1] for each in region_dict.items()]

    # check that continent/country specified is within data - also prevents entering a country name using the continent arge parse command and vice versa
    for cont in continent_specific:
        if cont not in [location for location in region_list]:
            print ("Must specify continent within model data")
            sys.exit()

    # creates dictionary to pair attrbiutes with attribute ids
    occup_path = PurePath.joinpath(path_stem, oasis_model_folder, rel_occup_path)
    occup_dict = get_codes('occupancy', occup_path, id_suffix='_code')

    # output vulnerability id dictionary - each type of building within each country given unique id 
    vuln_ids = create_vuln_ids(region_list, occup_dict, damage_classes)

    vuln_ids_path = PurePath.joinpath(path_stem, oasis_model_folder, vuln_dict_path)
    vulns_df = pd.DataFrame(vuln_ids)
    vulns_df.to_csv(vuln_ids_path, index=False)

    # create dictionary with location, and building type as keys and the vulnerability id as the value 
    vuln_dict = get_vuln_ids(vulns_df)
    print (vuln_dict)
    return "nothing"
    t_three = time.time() - t_zero
    t_one =  time.time()
    with open (PurePath.joinpath(path_stem, oasis_model_folder, vulnerability_path), 'wb') as dict_file:
        header = True
        cur_damage_class = ''
        
        for j, ((continent, attribute6_id), damage_class) in enumerate(zip(vuln_dict, damage_classes)):
            if damage_class != cur_damage_class:
                cur_damage_class = damage_class

                count = 1
                length = len( [(x,y) for x,y in zip(vulns_df['region'],vulns_df['attribute6_id']) if (x,y) == (continent, attribute6_id)] )
            print ('running', continent, damage_class, ': ', count, '/', length)
            count+=1
            # return list of bin index values for each intensity value used to calulcate a probability
            bin_index_list = get_bin_index(int_bins, int_mp_vals, int_mes_types, damage_class)

            vuln_id = vuln_dict[(continent, attribute6_id)]
            # create vulnerability file with vulnerability ids, correct intensity bin indexes and damage bin indexes
            vuln_data = create_vuln_dict (vuln_id, bin_index_list, damage_bins)
            vuln_temp_df = pd.DataFrame(vuln_data)

            # function compute_probs() computes damage functuion values using beta distribution
            int_vals = int_vals_list
            mean_LRs = probs_list_dict [(continent, damage_class)]
            std_devs = std_devs_dict [(continent, damage_class)]
            probs_data = compute_probs(int_vals, int_mp_vals, mean_LRs, std_devs, damage_bins_list)
            probs_df = pd.DataFrame(probs_list_dict[(continent, damage_class)])

            vuln_df = pd.concat([vuln_temp_df, probs_df], axis = 1)        
            vuln_df.to_csv(dict_file, index=False, header=header)       
            header = False
        print (t_three)
        print(time.time() - t_one)

kwargs = vars(parser.parse_args())
main(**kwargs)
