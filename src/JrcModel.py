""" global flood damage functions """

# implement functions originally used for the code to compute the GEM Vulnerability Model 
import pandas as pd
import xml.etree.ElementTree as ET
import numpy as np
from scipy import interpolate
from scipy.stats import beta
import time
import sys
import argparse
from pathlib import PurePosixPath, PurePath, Path

# include functional test that runs on 2 files - don't test intermediary file

parser = argparse.ArgumentParser(description = "Set up paths to receive inputs and output files to for vulnerability function data")
parser.add_argument('-W', "--working-folder", default='VulnerabilityLibrary', help = "input the name of the folder containing the source code and the oasis lmf model")
parser.add_argument('-d', "--num-damage-bins", default=100, type=int, help = "input number of damage bins")
parser.add_argument('-M', "--oasis-model-folder", default='global_flooding_variants', help = "input name of folder containing the files and data to run oasis lmf model")
parser.add_argument('-count', "--country-specific", default='', nargs='+', help = "to analyse specific countries, input the name of which one to run the model on")
parser.add_argument('-cont', "--continent-specific", default='', nargs='+', help = "to run the model on specific continents, input names from one of the following: Africa, Asia, Centr&south America, Europe, North America or Global`")
parser.add_argument('-imt', "--int-mes-types", default='flood_depth_metres', type=str, help="enter the unique intensity measurement types in the model data")
parser.add_argument('-F', "--data-path", default="global_flood_damage_functions.xlsx", help = "input name of file containing the vulnerability functions")
parser.add_argument('-e', "--jrc-input-path", default="example_jrc_exposure.xlsx", help = "input name of file containing the exposure file for the JRC data containing headers: 'country_code' and 'oed_occupancy_code'")

# default paths for input/output files
int_input_path = 'intensity_bins_input.csv'
int_dict_path = 'intensity_bin_dict.csv'
dam_bin_path = 'model_data/damage_bin_dict.csv'
vuln_dict_path = 'keys_data/vulnerability_dict.csv'
vulnerability_path = 'model_data/vulnerability.csv'
rel_height_path = 'keys_data/height_dict.csv'
rel_occup_path = 'keys_data/occupancy_dict.csv'
rel_constr_path = 'keys_data/construction_dict.csv'
rel_countr_path = 'keys_data/country_dict.csv'

# Notes on references for using tree and root method to analyse xml files
    # Intensity Measurement Type is root[0][i][0].attrib['imt']
    # Taxonomy is root[0][i].attrib['id']
        # Taxonomy [0]
        # Taxonomy [1]
        # Taxonomy [-1] 
    # Intensity is root[0][i][0].text
    # Mean Loss Ratio is root[0][i][1].text
    # Coefficient of Variation is root[0][i][2].text

def init_run(working_folder):
    model_path = PurePath(__file__)
    parent = model_path.parents
    for each in parent:
        if PurePosixPath(each).name == working_folder:
            path_stem = each
    return path_stem

def init_int_bins(path_stem, max_int_val):
    # create three lists for lower bound, upper bound and mid point point values of intensity bins
    int_bin_vals = []
    int_bins = []
    int_mp_vals = []
    with open(PurePath.joinpath(path_stem, int_input_path)) as bins_file:
        # Read in bins data from opened file
        for i, line in enumerate (bins_file):
            line = line.strip()
            if i and float(line) <= max_int_val:
                int_bin_vals.append(float(line))
            # condition to repeat first value
            if not (i-1) and float(line) <= max_int_val:
                int_bin_vals.append(float(line))
            else:
                continue

        # repeat last intensity value
        int_bin_vals.append(int_bin_vals[-1])
        for i in range (len(int_bin_vals)-1):
            int_bins.append((int_bin_vals[i], int_bin_vals[i+1]))
        int_bins = np.array(int_bins, dtype = float)
        for int_bin in int_bins:
            int_mp_val = (int_bin[0] + int_bin[1]) / 2
            int_mp_vals.append(round(int_mp_val, 16))

    # check for monotonicity
    if sorted(int_bin_vals) != int_bin_vals:
        print ('input intensity bins must be arranged in order of size...')
        sys.exit()
    return int_bins, int_mp_vals

def create_int_bins(int_bins, int_mp_vals, int_mes_types):
    # create function to find unique intensity measurement types
    index_num = 1
    for type in int_mes_types:
            for int_bin, int_mp_val in zip(int_bins, int_mp_vals):
                int_bin_dict = {'bin_index': index_num,
                                'intensity_measurement_type': type,
                                'bin_from': int_bin[0], 
                                'bin_to': int_bin[1], 
                                'interpolation': int_mp_val
                                }
                index_num += 1
                yield int_bin_dict  

def init_damage_bins(incr):
    damage_bins = []
    # create list of values in the damage bins - first and last values repeated so bin creation is simpler
    dam_bin_vals = np.arange(0, 1 + incr, incr)
    dam_bin_vals = np.append (dam_bin_vals[0], dam_bin_vals)
    dam_bin_vals = np.append(dam_bin_vals, dam_bin_vals[-1])

    # create damage bins [a,b)
    for i in range (len(dam_bin_vals)-1):
        damage_bins.append((dam_bin_vals[i], dam_bin_vals[i+1]))
    damage_bins = np.array(damage_bins, dtype = float)

    # rounding decimal places results in division accuracy errors
    dam_bin_vals = [np.round(dam_bin, 12) for dam_bin in dam_bin_vals]
    damage_bins_list = [dam for dam in dam_bin_vals]
    # damage_bins_list returns list of damage bin values not including the repeated start and end values 
    return damage_bins, damage_bins_list[1:-1]

def create_damage_bins(damage_bins):
    damage_bin_dict = {}
    index_num = 1
    for dam_bin in damage_bins:
        interp_val = (dam_bin[0] + dam_bin[1]) / 2
        interp_val = round(interp_val, 16)
        damage_bin_dict = {'bin_index': index_num,
                        'bin_from': "{:.6f}".format(dam_bin[0]),
                        'bin_to': "{:.6f}".format(dam_bin[1]), 
                        'interpolation': "{:.6f}".format(interp_val)
                        }
        index_num += 1
        yield damage_bin_dict

def get_bin_index(int_bins, int_mp_vals, int_mes_types, intensity_measure):
    bin_index_list = []
    start_index = 1
    num_intensity_bins = len(int_bins)

    # Intensity bin indexes range: e.g. from 1-20 (flood depth) if 20 input hazard intensity bins
    for i, int_mes_type in enumerate(int_mes_types):
        if intensity_measure == int_mes_type:
            start_index += i * num_intensity_bins
    for int_mp_val in int_mp_vals:
        for i in range(len(int_bins)):
            # edge case: if hazard intensity value is below or equal to lower bound of smallest bin
            if (not(i) and int_mp_val <= int_bins[0][0]):
                bin_index = start_index
                break
            # edge case: if hazard intensity value is greater than or equal to upper bound of largest bin
            elif int_mp_val >= int_bins[-1][0]:
                bin_index = start_index + (len(int_bins) - 1)
                break
            elif int_bins[i][0] <= int_mp_val < int_bins[i][1]:
                bin_index = start_index + (i)
                break
        bin_index_list.append(bin_index)
    return bin_index_list

def create_vuln_dict(val_id, bin_index_list, damage_bins):
    for bin_index in bin_index_list:
        for k in range(len(damage_bins)):
            rec = {
                "vulnerability_id": val_id,
                "intensity_bin_id": bin_index,
                "damage_bin_id": "{:.0f}".format(k+1),
                }
            yield rec

def compute_probs(int_vals, int_mp_vals, mean_LRs, std_devs, damage_bins_list, cont, occup):
    a=0
    b=1
    probs = {}

    # edge cases: bounds error not raised and fill value is used when int_mp_vals (intensity bins) are above or below the range of intensity values 
    int_vs_mean = interpolate.interp1d(int_vals, mean_LRs, kind='linear', bounds_error=False, fill_value= (mean_LRs[0], mean_LRs[-1]))
    int_vs_std = interpolate.interp1d(int_vals, std_devs, kind='linear', bounds_error=False, fill_value= (std_devs[0], std_devs[-1]))
    # function created above can take in list of values to use for interpolation - seen below
    interp_mean = int_vs_mean (int_mp_vals)
    interp_std_dev = int_vs_std (int_mp_vals)

    for j, (mean_val, std_dev_val) in enumerate (zip(interp_mean, interp_std_dev)):
        
        # edge case if parametrisation cannot be made due to zero mean loss ratio and standard deviation of loss ratio
        if mean_val == 0 and std_dev_val == 0:
            cum_diff_vals = np.append(1, np.zeros(len(damage_bins_list)))
        # damage_bins_list[0] == 0 should always be true so line below really only checks if the current intensity bin index being run is hazard intensity zero
        elif j==0 and damage_bins_list[0] == 0:
            cum_diff_vals = np.append(1, np.zeros(len(damage_bins_list)))
        else:
            if mean_val == 0:
                mean_val = 1e-10
            if std_dev_val == 0:
                std_dev_val = 1e-10
            if mean_val == 1.:
                mean_val = 1-(1e-15)
            alpha_val = (mean_val-a)/(b-a) * ((mean_val*(1-mean_val)/std_dev_val**2) - 1)
            beta_val = alpha_val * (1-mean_val) / mean_val
            # Beta function automatically runs has start and end interval from 0 to 1
            prob_vals = beta.cdf(damage_bins_list, alpha_val, beta_val)
            # Generally, first damage bin has proabbility zero since it is a single value not a range- same for last damage bin that only represents single damage value
            # Exception for hazard intensity bin 1 representing zero intensity => no damage
            cum_diff_vals = list(np.diff(prob_vals, axis=0, prepend=0, append=prob_vals[-1]))
        
        for prob in cum_diff_vals:
            probs = {'probabilities': "{:.6f}".format(prob)
                }
            yield probs

def get_codes(header_name, path, extra_keys_in=None, id_suffix='_id'):
    mapping_dict = {}
    header = True
    if extra_keys_in == None:
        extra_keys = []
    else:
        extra_keys = extra_keys_in

    if id_suffix == 'Code':
        temp_header_name = header_name.capitalize()
    else:
        temp_header_name = header_name

    with open(path, "r") as file: 
        for line in file:
            data = line.strip().split(',')
            if header:    
                header_index = data.index(header_name)
                id_index = data.index(f'{temp_header_name}{id_suffix}')
                extra_index = [data.index(extra_key) for extra_key in extra_keys]
                header = False
                continue
            if extra_keys:
                key = tuple([data[header_index]] + [data[index] for index in extra_index])
            else:
                key = data[header_index]
            mapping_dict[key] = data[id_index]
    return mapping_dict

def create_vuln_ids(cont_list, occup_to_id, damage_classes, cont_to_id):
    vuln_ids = {}
    cur_vuln_id = 1    

    for continent in cont_list:
        for damage_class in damage_classes:
            occup = damage_class[:3].upper()
            occup_id = occup_to_id[occup]
            if (continent, damage_class) not in vuln_ids:
                vuln_ids[(continent, damage_class)] = cur_vuln_id
                cur_vuln_id += 1

                id_dict = {
                    "vulnerability_id": vuln_ids[(continent, damage_class)],
                    "continent": continent,
                    "continent_id": cont_to_id[continent],
                    "occupancy": occup,
                    "occupancy_id": occup_id
                }
                yield id_dict

def get_vuln_ids(vulns_df):
    vuln_dict = {}

    for index, row in vulns_df.iterrows():
        vuln_id = row['vulnerability_id']
        vuln_cont = row['continent_id']
        vuln_type = row['occupancy_id']
        vuln_dict[(vuln_cont, vuln_type)] = vuln_id
    return vuln_dict

def use_excel(data_path, cont_to_id, occup_to_id):
    df_xlsx = pd.ExcelFile(data_path)
    data = pd.read_excel(df_xlsx, 'Damage functions', na_filter=False, header=[1,2])
    damage_classes = []
    int_vals_dict = {}
    region_dict = {}
    probs_list_dict = {}
    std_devs_dict = {} 
    excel_rows = []
    for dam_class in data['Damage\nclass'].iloc[:, [0][0]]:
        if dam_class:
            damage_classes.append(dam_class)

    for region in data['Damage function']:
        location = ''
        cur_region = region
        if '\n' in region:
            region = ' '.join(region.split('\n'))

        if ' ' in region:
            for word in region.split():
                word = word.capitalize()
                if not location:
                    location = word
                else:
                    location = ' '.join((location, word))
        else:
            location = region.capitalize()
        region_dict[cur_region] = location
    
    header = True
    for i, dam_class in enumerate (data['Damage\nclass'].iloc[:, [0][0]]):
        if header:
            start = i
        if dam_class and not (header):
            end = i
            excel_rows.append((start, end))
            start = end
        header = False
    end = i+1
    excel_rows.append((start, end))

    # create list of list of intensity bin values for each damage class - SHOULD THIS BE GENERAL?
    for region in data['Damage function']:
        fted_region = region_dict[region]
        for index, damage_class in zip(excel_rows, damage_classes):
            occup = damage_class[:3].upper()
            length_class = index[1] - index[0]
            vals_set = []
            for i, val in enumerate(data['Flood depth,\n[m]'].iloc[index[0]:index[1],[0][0]]):
                vals_set.append(val)
                if len(vals_set) == length_class:
                    int_vals_dict[(cont_to_id[fted_region], occup_to_id[occup])] = vals_set[:length_class]
                    continue
                
    for region in data['Damage function']:
        fted_region = region_dict[region]
        for index, damage_class in zip(excel_rows, damage_classes):
            occup = damage_class[:3].upper()
            region_probs = []
            region_std_devs = []
            length_class = index[1] - index[0]
            for i, (prob, std_dev) in enumerate(zip(data['Damage function'].loc[index[0]:index[1], [region][0]], data['Standard deviation'].loc[index[0]:index[1], [region][0]])):
                if type(prob) == int or type(prob) == float:
                    region_probs.append(round(prob,12))
                else:
                    region_probs.append(0) 
                if type(std_dev) == int or type(std_dev) == float:
                        region_std_devs.append(std_dev)
                else: 
                    region_std_devs.append(0) 
                if len(region_probs) == length_class:                    
                    probs_list_dict[(cont_to_id[fted_region], occup_to_id[occup])] = region_probs[:length_class]
                    std_devs_dict[(cont_to_id[fted_region], occup_to_id[occup])] = region_std_devs[:length_class]
                    continue

    return damage_classes, int_vals_dict, region_dict, probs_list_dict, std_devs_dict

def analyse_exposure(vulns_df):
    input_data = []
    
    for index, data in vulns_df.iterrows():
        input_data.append((data['continent_id'], data['occupancy_id']))

    return input_data
    
def main(working_folder, num_damage_bins, oasis_model_folder, data_path, country_specific, continent_specific, int_mes_types, jrc_input_path):
    t_zero = time.time()
    incr_size = 1/num_damage_bins
    int_mes_types = int_mes_types.split()

    # only true if both true - check documentation
    if continent_specific and country_specific:
        print ("Cannot input both specific countries and continents")
        sys.exit()
    
    # produces string that is the name of the folder in which all the model's data and code is contained - default is 'VulnerabilityLibrary'
    path_stem = init_run(working_folder)

    # creates dictionary to pair attrbiutes with attribute ids
    occup_path = PurePath.joinpath(path_stem, oasis_model_folder, rel_occup_path)
    countr_path = PurePath.joinpath(path_stem, oasis_model_folder, rel_countr_path)
    occup_dict = get_codes('occupancy', occup_path, extra_keys_in=['OccupancyCode'])
    location_dict = get_codes('continent', countr_path, extra_keys_in=['CountryCode'])

    # generate dictionaries required to convert from/to attribute ids
    countr_code_to_cont_id = {country_code: location_dict[(continent, country_code)] for (continent, country_code) in location_dict}
    occup_code_to_id = {occupancy_code: occup_dict[(occupancy, occupancy_code)] for (occupancy, occupancy_code) in occup_dict}
    id_to_occup = {occup_dict[(occupancy, occupancy_code)]: occupancy for (occupancy, occupancy_code) in occup_dict}
    occup_to_id = {occupancy: occup_dict[(occupancy, occupancy_code)] for (occupancy, occupancy_code) in occup_dict}
    id_to_cont = {location_dict[(continent, country_code)]: continent for (continent, country_code) in location_dict}
    cont_to_id = {continent: location_dict[(continent, country_code)] for (continent, country_code) in location_dict}

    # list of all locations with data in vulnerability models folder stored in dictionary as key and the value is the relative path
    rootdir = PurePath.joinpath(path_stem, data_path)
    damage_classes, int_vals_dict, region_dict, probs_list_dict, std_devs_dict = use_excel(rootdir, cont_to_id, occup_to_id)
    cont_list = [each[1] for each in region_dict.items()]
    max_int_val = max([max(int_list) for int_list in int_vals_dict.values()])

    # hazard intensity bins created and a dictionary is outputted where maximum intensity value cannot exceed max. in sorce data
    int_bins, int_mp_vals = init_int_bins(path_stem, max_int_val)
    int_bins_dict = create_int_bins(int_bins, int_mp_vals, int_mes_types)

    int_bins_df = pd.DataFrame(int_bins_dict)
    int_bin_dict_path = PurePath.joinpath(path_stem, int_dict_path)
    int_bins_df.to_csv(int_bin_dict_path, index=False)

    # damage bin list does not have repeated start or end - length 102 - same as number of damage bins
    damage_bins, damage_bins_list = init_damage_bins(incr_size)
    dam_bins_dict = create_damage_bins(damage_bins)

    damage_bins_df = pd.DataFrame(dam_bins_dict)
    damage_bins_path = PurePath.joinpath(path_stem, oasis_model_folder, dam_bin_path)
    damage_bins_df.to_csv(damage_bins_path, index=False)

    # check that continent/country specified is within data - also prevents entering a country name using the continent arge parse command and vice versa
    for cont in continent_specific:
        if cont not in [location for location in cont_list]:
            print ("Must specify continent(s) within model data")
            sys.exit()

    # output vulnerability id dictionary - each occupancy type within each continent is given unique id 
    vuln_ids = create_vuln_ids(cont_list, occup_to_id, damage_classes, cont_to_id)
    vuln_ids_path = PurePath.joinpath(path_stem, oasis_model_folder, vuln_dict_path)
    vulns_df = pd.DataFrame(vuln_ids)
    vulns_df.to_csv(vuln_ids_path, index=False)
    vuln_dict = get_vuln_ids(vulns_df)

    jrc_input_path = PurePath.joinpath(path_stem, jrc_input_path)
    # list of entries as tuple containing (continent_id, occupancy_id) extracted
    exposure_data = analyse_exposure(vulns_df)

    t_three = time.time() - t_zero
    t_one =  time.time()
    with open (PurePath.joinpath(path_stem, oasis_model_folder, vulnerability_path), 'wb') as dict_file:
        header = True
        counter = 1
    
        if continent_specific:
            length = len([cont_id for (cont_id, _) in exposure_data if id_to_cont[cont_id] in continent_specific])
        else:
            length = len(exposure_data)

        for j, exp_entry in enumerate(exposure_data):
            cont_id = exp_entry[0]
            occup_id = exp_entry[1]
            continent = id_to_cont[cont_id]
            vuln_id = vuln_dict[cont_id, occup_id]
            cur_data = (cont_id, occup_id)

            if (continent_specific and continent in continent_specific) or not continent_specific:
                print ('running', continent, id_to_occup[occup_id], ': ', counter, '/', length)
                counter+=1
            else:
                continue
            
            # return list of bin index values for each intensity value used to calulcate a probability
            bin_index_list = get_bin_index(int_bins, int_mp_vals, int_mes_types, 'flood_depth_meters')
            # create vulnerability file with vulnerability ids, correct intensity bin indexes and damage bin indexes
            vuln_data = create_vuln_dict (vuln_id, bin_index_list, damage_bins)
            vuln_temp_df = pd.DataFrame(vuln_data)

            # function compute_probs() computes damage functuion values using beta distribution
            int_vals = int_vals_dict [cur_data]
            mean_LRs = probs_list_dict [cur_data]
            std_devs = std_devs_dict [cur_data]
            probs_data = compute_probs(int_vals, int_mp_vals, mean_LRs, std_devs, damage_bins_list, continent, id_to_occup[occup_id])
            probs_df = pd.DataFrame(probs_data)
            vuln_df = pd.concat([vuln_temp_df, probs_df], axis = 1)  
            vuln_df.to_csv(dict_file, index=False, header=header)       
            header = False

    print (t_three)
    print(time.time() - t_one)

kwargs = vars(parser.parse_args())
main(**kwargs)
