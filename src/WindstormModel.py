from pathlib import PurePath, PurePosixPath
import sys
import numpy as np
import pandas as pd
import argparse

parser = argparse.ArgumentParser(description = "Set up paths to receive inputs and output files to for vulnerability function data")
parser.add_argument('-W', "--working-folder", default='VulnerabilityLibrary', help = "input the name of the folder containing the source code and the oasis lmf model")
parser.add_argument('-M', "--oasis-model-folder", default="global_windstorm_variants", help = "input name of folder containing the files and data to run oasis lmf model")
parser.add_argument('-imt', "--int-mes-types", default="windspeed_mph", type=str, help="enter the unique intensity measurement types in the model data")
parser.add_argument('-F', "--data-path", default="2018_Michael_windgrid.csv", help = "input name of file containing the vulnerability functions")
parser.add_argument('-d', "--num-damage-bins", default=100, type=int, help = "input number of damage bins")
parser.add_argument('-v', "--max_int_val", default=150, type=int, help = "input maximum hazard intensity value")
parser.add_argument('-thresh', "--v-thresh", default=25.7, type=float, help = "input hazard intensity value threshold")
parser.add_argument('-half', "--v-half", default=74.7, type=float, help = r"input half hazard intensity value required for 50% damage")

# default paths for input/output files
int_input_path = 'intensity_bins_input.csv'
int_dict_path = 'intensity_bin_dict.csv'
vulnerability_path = 'model_data/vulnerability.csv'
footprint_path = 'model_data/footprint.csv'
rel_areaperil_path = 'keys_data/areaperil_dict.csv'

def init_run(working_folder):
    model_path = PurePath(__file__)
    parent = model_path.parents
    for each in parent:
        if PurePosixPath(each).name == working_folder:
            path_stem = each
    return path_stem

def init_int_bins(path_stem, max_int_val):
    # create three lists for lower bound, upper bound and mid point point values of intensity bins
    int_bin_vals = []
    int_bins = []
    int_mp_vals = []

    with open(PurePath.joinpath(path_stem, int_input_path)) as bins_file:
        # Read in bins data from opened file
        for i, line in enumerate (bins_file):
            line = line.strip()
            if i and float(line) <= max_int_val:
                int_bin_vals.append(float(line))
            # condition to repeat first value
            if not (i-1) and float(line) <= max_int_val:
                int_bin_vals.append(float(line))
            else:
                continue

        # repeat last intensity value
        int_bin_vals.append(int_bin_vals[-1])
        for i in range (len(int_bin_vals)-1):
            int_bins.append((int_bin_vals[i], int_bin_vals[i+1]))
        int_bins = np.array(int_bins, dtype = float)
        for int_bin in int_bins:
            int_mp_val = (int_bin[0] + int_bin[1]) / 2
            int_mp_vals.append(round(int_mp_val, 16))

    # check for monotonicity
    if sorted(int_bin_vals) != int_bin_vals:
        print ('input intensity bins must be arranged in order of size...')
        sys.exit()
    return int_bins, int_mp_vals

def create_int_bins(int_bins, int_mp_vals, int_mes_types):
    # create function to find unique intensity measurement types
    index_num = 1
    for type in int_mes_types:
            for int_bin, int_mp_val in zip(int_bins, int_mp_vals):
                int_bin_dict = {'bin_index': index_num,
                                'intensity_measurement_type': type,
                                'bin_from': int_bin[0], 
                                'bin_to': int_bin[1], 
                                'interpolation': int_mp_val
                                }
                index_num += 1
                yield int_bin_dict

def init_damage_bins(incr):
    damage_bins = []
    # create list of values in the damage bins - first and last values repeated so bin creation is simpler
    dam_bin_vals = np.arange(0, 1 + incr, incr)
    dam_bin_vals = np.append (dam_bin_vals[0], dam_bin_vals)
    dam_bin_vals = np.append(dam_bin_vals, dam_bin_vals[-1])

    # create damage bins [a,b)
    for i in range (len(dam_bin_vals)-1):
        damage_bins.append((dam_bin_vals[i], dam_bin_vals[i+1]))
    damage_bins = np.array(damage_bins, dtype = float)

    # rounding decimal places results in division accuracy errors
    dam_bin_vals = [np.round(dam_bin, 12) for dam_bin in dam_bin_vals]
    damage_bins_list = [dam for dam in dam_bin_vals]
    # damage_bins_list returns list of damage bin values not including the repeated start and end values 
    return damage_bins, damage_bins_list[1:-1]

def get_int_bin_id(int_bin_df, intensity):
    copy_df = int_bin_df.copy()
    copy_df.drop(index = 0, inplace = True)
    copy_df.drop(index = len(copy_df), inplace = True)

    if intensity <= copy_df.iloc[0,2]:
        intensity_bin_id = copy_df.iloc[1,0] - 1
    elif intensity >= copy_df.iloc[len(copy_df) - 1, 3]:
        intensity_bin_id = copy_df.iloc[len(copy_df) - 1, 0] + 1
    else:
        differences = abs(copy_df['interpolation'] - intensity)
        intensity_bin_id = copy_df['bin_index'][differences.idxmin()]

    return intensity_bin_id

def create_damage_bins(damage_bins):
    damage_bin_dict = {}
    index_num = 1
    for dam_bin in damage_bins:
        interp_val = (dam_bin[0] + dam_bin[1]) / 2
        interp_val = round(interp_val, 16)
        damage_bin_dict = {'bin_index': index_num,
                        'bin_from': "{:.6f}".format(dam_bin[0]),
                        'bin_to': "{:.6f}".format(dam_bin[1]), 
                        'interpolation': float("{:.6f}".format(interp_val))
                        }
        index_num += 1
        yield damage_bin_dict

def windstorm_function(v_thresh, v_half,intensity):
    if intensity < v_thresh:
        intensity = v_thresh
    v_ij = (intensity - v_thresh) / (v_half - v_thresh)
    damage = (v_ij**3) / (1 + v_ij**3)

    return damage

def get_damage_bin(damage_bin_df,damage):
    copy_df = damage_bin_df.copy()
    copy_df.drop(index = 0, inplace = True)
    copy_df.drop(index = 101, inplace = True)

    if damage <= 0:
        damage_bin_id = 1
    elif damage >= 1:
        damage_bin_id = 102
    else:
        differences = abs(copy_df['interpolation'] - damage)
        damage_bin_id = copy_df['bin_index'][differences.idxmin()]

    return damage_bin_id

def create_vulnerability(damage_bin_df,int_bins_df, v_thresh, v_half):
    for index1, data1 in int_bins_df.iterrows():

        intensity = data1['interpolation']
        damage = windstorm_function(v_thresh, v_half, intensity)
        damage_bin_id = get_damage_bin(damage_bin_df, damage)
        for index2, data2 in damage_bin_df.iterrows():
            if damage_bin_id == index2 + 1:
                probability = 1
            else:
                probability = 0
            vulnerability = {
                            'vulnerability_id': 1, 
                            'intensity_bin_id': data1['bin_index'], 
                            'damage_bin_id': index2 + 1, 
                            'probability': probability
                            }
            yield vulnerability

def convert_footprint(input_footprint, int_bin_df):
    fp_df = pd.read_csv(input_footprint)
    
    areaperil_df = fp_df[['latitude','longitude']].copy()
    areaperil_df.insert(loc = 0, column = 'area_peril_id', value = np.arange(1, len(areaperil_df)+1))
    
    footprint_df = fp_df[['Vg_mph']].copy()
    footprint_df.insert(loc = 0, column = 'event_id', value = 1)
    footprint_df.insert(loc = 1, column = 'area_peril_id', value = np.arange(1, len(footprint_df)+1))
    footprint_df['intensity_bin_id'] = footprint_df['Vg_mph'].apply( lambda x: get_int_bin_id(int_bin_df, x))
    footprint_df.insert(loc = 4, column = 'probability', value = 1)
    footprint_df.drop(columns='Vg_mph', inplace = True)

    return  areaperil_df, footprint_df

def main(working_folder, oasis_model_folder, data_path, int_mes_types, max_int_val, num_damage_bins, v_half, v_thresh):
    incr_size = 1/num_damage_bins
    path_stem = init_run(working_folder)

    int_bins, int_mp_vals = init_int_bins(path_stem, max_int_val)
    int_bins_dict = create_int_bins(int_bins, int_mp_vals, int_mes_types)
    int_bins_df = pd.DataFrame(int_bins_dict)

    damage_bins, damage_bins_list = init_damage_bins(incr_size)
    dam_bins_dict = create_damage_bins(damage_bins)
    damage_bins_df = pd.DataFrame(dam_bins_dict)

    vuln_csv_dest = PurePath.joinpath(path_stem, oasis_model_folder, vulnerability_path)
    vulnerability = pd.DataFrame(create_vulnerability(damage_bins_df, int_bins_df, v_thresh, v_half))
    vulnerability.to_csv(vuln_csv_dest, index=False)

    input_footprint = PurePath.joinpath(path_stem, data_path)
    output_footprint = PurePath.joinpath(path_stem, oasis_model_folder, footprint_path)
    areaperil_dict_path = PurePath.joinpath(path_stem, oasis_model_folder, rel_areaperil_path)
    areaperil_df, footprint_df = convert_footprint(input_footprint, int_bins_df)
    areaperil_df.to_csv(areaperil_dict_path, index=False)
    footprint_df.to_csv(output_footprint, index=False)
    
kwargs = vars(parser.parse_args())
main(**kwargs)