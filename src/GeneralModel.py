import pandas as pd
import xml.etree.ElementTree as ET
import numpy as np
from scipy import interpolate
from scipy.stats import beta
import time
import sys
import argparse
from pathlib import PurePosixPath, PurePath, Path

class BaseModel():
    def __init__(self, kwargs):
        self.path_stem = self.init_run(kwargs['working_folder'])
        self.incr_size = 1/(kwargs['num_damage_bins'])
        self.country_specific = kwargs['country_specific']
        self.continent_specific = kwargs['continent_specific']
        self.oasis_model_folder = kwargs['oasis_model_folder']
        self.data_path = kwargs['data_path']
        self.int_mes_types = kwargs['int_mes_types'].split()

        self.int_input_path = 'intensity_bins_input.csv'
        self.int_dict_path = 'intensity_bin_dict.csv'
        self.dam_bin_dict_path = 'model_data/damage_bin_dict.csv'
        self.vuln_dict_path = 'keys_data/vulnerability_dict.csv'
        self.vulnerability_path = 'model_data/vulnerability.csv'

        self.rel_countr_path = 'keys_data/country_dict.csv'
        self.rel_occup_path = 'keys_data/occupancy_dict.csv'
        self.rel_occup_path = 'keys_data/occupancy_dict.csv'
        self.rel_countr_path = 'keys_data/country_dict.csv'

        self.int_bin_dict_path = PurePath.joinpath(self.path_stem, self.int_dict_path)
        # self.__dict__.update(**kwargs)
        # self.kwargs = kwargs

    def init_run(self, working_folder):
        model_path = PurePath(__file__)
        parent = model_path.parents
        for each in parent:
            if PurePosixPath(each).name == working_folder:
                path_stem = each
        return path_stem

    def init_damage_bins(self, incr_size):
        damage_bins = []
        # create list of values in the damage bins - first and last values repeated so bin creation is simpler
        dam_bin_vals = np.arange(0, 1 + incr_size, incr_size)
        dam_bin_vals = np.append (dam_bin_vals[0], dam_bin_vals)
        dam_bin_vals = np.append(dam_bin_vals, dam_bin_vals[-1])

        # create damage bins [a,b)
        for i in range (len(dam_bin_vals)-1):
            damage_bins.append((dam_bin_vals[i], dam_bin_vals[i+1]))
        damage_bins = np.array(damage_bins, dtype = float)

        # rounding decimal places results in division accuracy errors
        dam_bin_vals = [np.round(dam_bin, 12) for dam_bin in dam_bin_vals]
        damage_bins_list = [dam for dam in dam_bin_vals]
        # damage_bins_list returns list of damage bin values not including the repeated start and end values 
        return damage_bins, damage_bins_list[1:-1]
    
    def create_damage_bins(self, damage_bins):

        damage_bin_dict = {}
        index_num = 1
        for dam_bin in damage_bins:
            interp_val = (dam_bin[0] + dam_bin[1]) / 2
            interp_val = round((interp_val), 16)
            damage_bin_dict = {'bin_index': index_num,
                            'bin_from': "{:.6f}".format(dam_bin[0]),
                            'bin_to': "{:.6f}".format(dam_bin[1]), 
                            'interpolation': float("{:.6f}".format(interp_val))
                            }
            index_num += 1
            yield damage_bin_dict
     
    def sense_check(self):
        if self.continent_specific and self.country_specific:
            print ("Cannot input both specific countries and continents")
            sys.exit()

    def init_int_bins(self, path_stem, max_int_val):
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_bin_vals = []
        int_bins = []
        int_mp_vals = []

        with open(PurePath.joinpath(path_stem, self.int_input_path)) as bins_file:
            # Read in bins data from opened file
            for i, line in enumerate (bins_file):
                line = line.strip()
                if i and float(line) <= max_int_val:
                    int_bin_vals.append(float(line))
                # condition to repeat first value
                if not (i-1) and float(line) <= max_int_val:
                    int_bin_vals.append(float(line))
                else:
                    continue

            # repeat last intensity value
            int_bin_vals.append(int_bin_vals[-1])
            for i in range (len(int_bin_vals)-1):
                int_bins.append((int_bin_vals[i], int_bin_vals[i+1]))
            int_bins = np.array(int_bins, dtype = float)
            for int_bin in int_bins:
                int_mp_val = (int_bin[0] + int_bin[1]) / 2
                int_mp_vals.append(round(int_mp_val, 16))

        # check for monotonicity
        if sorted(int_bin_vals) != int_bin_vals:
            print ('input intensity bins must be arranged in order of size...')
            sys.exit()
        return int_bins, int_mp_vals

    def create_int_bins(self, int_bins, int_mp_vals, int_mes_types):
        # create function to find unique intensity measurement types
        index_num = 1
        for type in int_mes_types:
                for int_bin, int_mp_val in zip(int_bins, int_mp_vals):
                    int_bin_dict = {'bin_index': index_num,
                                    'intensity_measurement_type': type,
                                    'bin_from': int_bin[0], 
                                    'bin_to': int_bin[1], 
                                    'interpolation': int_mp_val
                                    }
                    index_num += 1
                    yield int_bin_dict
    
    def create_vuln_dict(self, vuln_id, bin_index_list, damage_bins):
        for bin_index in bin_index_list:
            for k in range(len(damage_bins)):
                rec = {
                    "vulnerability_id": vuln_id,
                    "intensity_bin_id": bin_index,
                    "damage_bin_id": "{:.0f}".format(k+1),
                    }
                yield rec

    def compute_probs(self, int_vals, int_mp_vals, mean_LRs, std_devs, damage_bins_list):
        a=0
        b=1
        probs = {}

        # edge cases: bounds error is used when int_mp_vals (intensity bins) are above or below the range of intensity values 
        int_vs_mean = interpolate.interp1d(int_vals, mean_LRs, kind='linear', bounds_error=False, fill_value= (mean_LRs[0], mean_LRs[-1]))
        int_vs_std = interpolate.interp1d(int_vals, std_devs, kind='linear', bounds_error=False, fill_value= (std_devs[0], std_devs[-1]))
        # function created above can take in list of values to use for interpolation - seen below
        interp_mean = int_vs_mean (int_mp_vals)
        interp_std_dev = int_vs_std (int_mp_vals)

        for j, (mean_val, std_dev_val) in enumerate (zip(interp_mean, interp_std_dev)):
            # edge case if parametrisation cannot be made due to zero mean loss ratio and standard deviation of loss ratio
            if mean_val == 0 and std_dev_val == 0:
                cum_diff_vals = np.append(1, np.zeros(len(damage_bins_list)))
            # damage_bins_list[0] == 0 should always be true- hence following code simply checks if the current intensity bin index being run is hazard intensity zero
            elif j==0 and damage_bins_list[0] == 0:
                cum_diff_vals = np.append(1, np.zeros(len(damage_bins_list)))
            
            else:
                if mean_val == 0:
                    mean_val = 1e-10
                if std_dev_val == 0:
                    std_dev_val = 1e-10
                if mean_val == 1.:
                    mean_val = 1-(1e-15)
                alpha_val = (mean_val-a)/(b-a) * ((mean_val*(1-mean_val)/std_dev_val**2) - 1)
                beta_val = alpha_val * (1-mean_val) / mean_val
                # Beta function automatically runs has start and end interval from 0 to 1
                prob_vals = beta.cdf(damage_bins_list, alpha_val, beta_val)
                # Generally, first damage bin has proabbility zero since it is a single value not a range- same for last damage bin that only represents single damage value
                # Exception for hazard intensity bin 1 representing zero intensity => no damage
                cum_diff_vals = list(np.diff(prob_vals, axis=0, prepend=0, append=prob_vals[-1]))

            for prob in cum_diff_vals:
                probs = {'probabilities': "{:.6f}".format(prob)
                    }
                yield probs

    # not used for now - need to generalise
    def get_vuln_ids(self):
        pass

    def get_bin_index(self, int_bins, int_mp_vals, int_mes_types, intensity_measure):
        bin_index_list = []
        start_index = 1
        num_intensity_bins = len(int_bins)

        # Intensity bin indexes range: e.g. from 1-20 (PGA), 21-40 (SA 0.3), 41-60 (SA 0.6), 61-80 (SA 1.0) if 20 input hazard intensity bins
        for i, int_mes_type in enumerate(int_mes_types):
            if intensity_measure == int_mes_type:
                start_index += i * num_intensity_bins
        for int_mp_val in int_mp_vals:
            for i in range(len(int_bins)):
                # edge case: if hazard intensity value is below or equal to lower bound of smallest bin
                if (not(i) and int_mp_val <= int_bins[0][0]):
                    bin_index = start_index
                    break
                # edge case: if hazard intensity value is greater than or equal to upper bound of largest bin
                elif int_mp_val >= int_bins[-1][0]:
                    bin_index = start_index + (len(int_bins) - 1)
                    break
                elif int_bins[i][0] <= int_mp_val < int_bins[i][1]:
                    bin_index = start_index + (i)
                    break
            bin_index_list.append(bin_index)
            
        return bin_index_list
    
    def get_codes(self, header_name, path, extra_keys_in=None, id_suffix='_id'):
        mapping_dict = {}
        header = True
        if extra_keys_in == None:
            extra_keys = []
        else:
            extra_keys = extra_keys_in

        if id_suffix == 'Code':
            temp_header_name = header_name.capitalize()
        else:
            temp_header_name = header_name

        with open(path, "r") as file: 
            for line in file:
                data = line.strip().split(',')
                if header:    
                    header_index = data.index(header_name)
                    id_index = data.index(f'{temp_header_name}{id_suffix}')
                    extra_index = [data.index(extra_key) for extra_key in extra_keys]
                    header = False
                    continue
                if extra_keys:
                    key = tuple([data[header_index]] + [data[index] for index in extra_index])
                else:
                    key = data[header_index]
                mapping_dict[key] = data[id_index]
        return mapping_dict

    def run(self):
        incr_size = self.incr_size
        self.sense_check()

        self.damage_bins, self.damage_bins_list = self.init_damage_bins(incr_size)
        dam_bins_dict = self.create_damage_bins(self.damage_bins)
        self.damage_bins_df = pd.DataFrame(dam_bins_dict)

class GemModel(BaseModel):
    def __init__(self, kwargs):
        super().__init__(kwargs) 
        super().run()
        self.cov_type = kwargs['coverage_type']

        self.rel_height_path = 'keys_data/height_dict.csv'
        self.rel_constr_path = 'keys_data/construction_dict.csv'

    def directories(self, rootdir, coverage_type):
        location_dict = {}
        for file in Path(rootdir).glob('**/*.xml'):
            filepath = Path(file).absolute()
            cov_type_path = Path(file).name
            if PurePosixPath(cov_type_path).stem == f"vulnerability_{coverage_type}":
                location_continent = Path(file).parent.parent.name
                location_country = Path(file).parent.name
                location_dict [location_continent, location_country] = filepath
        return location_dict
    
    def create_vuln_ids(self, location_dict, height_dict, occup_dict, constr_dict, countr_dict):
        vuln_ids = {}
        cur_vuln_id = 1    

        for country in location_dict:
            pathway = location_dict[country]
            tree = ET.parse(pathway)
            root = tree.getroot()
            for i in range (1, len(root[0])):
                taxonomy = root[0][i].attrib['id']
                tax = taxonomy.split('/')
                tax.append(tax[0] + '/' + tax[1])
                # example of tax: ['CR', 'LDUAL+CDL+DUM', 'H1', 'RES', 'CR/LDUAL+CDL+DUM']
                if (country, taxonomy) not in vuln_ids:
                    vuln_ids[(country, taxonomy)] = cur_vuln_id
                    cur_vuln_id += 1
                    for (attribute2_3, peril_type, ranking) in constr_dict:
                        if attribute2_3 in tax:
                            attribute2_3_id = constr_dict[attribute2_3, peril_type, ranking]
                            peril_id = peril_type
                            rank = ranking
                    for attribute6 in occup_dict:
                        if attribute6 in tax:
                            attribute6_id = occup_dict[attribute6]
                    for attribute4 in height_dict:
                        if attribute4 in tax:
                            no_storeys = height_dict[attribute4]
                    country_id = countr_dict[country]
                    id_dict = {
                        "vulnerability_id": vuln_ids[(country, taxonomy)],
                        "peril_id": peril_id,
                        "country_id": country_id,
                        "taxonomy": taxonomy,
                        "node_id": i,
                        "attribute2_3_id": attribute2_3_id,
                        "ranking": rank,
                        "attribute4_id": no_storeys,
                        "attribute6_id": attribute6_id
                    }
                    yield id_dict

    def get_vuln_ids(self, vulns_df, countr_dict, location_dict):
        vuln_dict = {}
        # Reverse key and values in countr_dict 
        rev_countr_dict = {countr_dict[country]: country for country in countr_dict}
        # Map country to continent
        countr_to_cont = {country: continent for (continent, country) in location_dict}
        
        for index, row in vulns_df.iterrows():
            vuln_id = row['vulnerability_id']
            vuln_country_id = row['country_id']
            vuln_taxonomy = row['taxonomy']
            vuln_node_id = row['node_id']
            country = rev_countr_dict[str(vuln_country_id)]
            continent = countr_to_cont[country]
            vuln_dict[(continent, country, vuln_taxonomy, vuln_node_id)] = [vuln_id]

        return vuln_dict

    def filter_vuln_dict(self, vulns_df, countr_dict, country_specific):
        rev_countr_dict = {countr_dict[country]: country for country in countr_dict}
        copy_vulns_df = vulns_df.copy()
        copy_vulns_df[['country_id','attribute2_3_id', 'ranking', 'attribute4_id', 'attribute6_id']] = copy_vulns_df[['country_id','attribute2_3_id', 'ranking', 'attribute4_id', 'attribute6_id']].astype(int)
        
        # indexes of rows preserved
        # dataframe country_code sorted last
        sorted_vulns_df = copy_vulns_df.sort_values(by=['country_id','attribute6_id','attribute4_id','attribute2_3_id','ranking'], inplace = False)
        header = True

        if country_specific == '':
            unique_df = sorted_vulns_df['country_id'].unique()
        else:
            country_specific_ids = [int(countr_dict[key]) for key in country_specific if key in countr_dict]
            unique_df = sorted_vulns_df[sorted_vulns_df['country_id'].isin(country_specific_ids)]['country_id'].unique()
            copy_vulns_df = copy_vulns_df[copy_vulns_df.loc[:,'country_id'].isin(country_specific_ids)]
        
        for country_id in unique_df:
            length = sorted_vulns_df['country_id'].value_counts()[country_id]
            header = True
            for num, cur_data in sorted_vulns_df[(sorted_vulns_df['country_id'] == country_id)].iterrows():
                if header:
                    counter = 1
                    print (f'filtering {rev_countr_dict[str(country_id)]}: ', counter, '/', length)
                    prev_data = cur_data
                    counter+=1
                    header = False
                    continue

                print (f'filtering {rev_countr_dict[str(country_id)]}: ', counter, '/', length)
                counter+=1
                if (prev_data['country_id'] == cur_data['country_id']) & (prev_data['attribute4_id'] == cur_data['attribute4_id']) & (prev_data['attribute6_id'] == cur_data['attribute6_id']) & (prev_data['attribute2_3_id'] == cur_data['attribute2_3_id']):
                    copy_vulns_df.drop(index=num, inplace=True)
                elif cur_data['attribute2_3_id'] == 0:
                    copy_vulns_df.drop(index=num, inplace=True)
                else:
                    prev_data = cur_data

        copy_vulns_df.loc[:,'vulnerability_id'] = np.arange(1, len(copy_vulns_df) + 1)
        return copy_vulns_df
    
    def sense_check_again(self, continent_specific, country_specific, locations_dict):
        for count in continent_specific:
            if continent_specific and (count not in [location[0] for (location, filepath) in locations_dict.items()]):
                print ("Must specify continent within model data")
                sys.exit()

        for count in country_specific:
            if count and (count not in [location[1] for (location, filepath) in locations_dict.items()]):
                print ("Must specify country within model data")
                sys.exit()
    
    def run(self):
        path_stem = self.path_stem
        oasis_model_folder = self.oasis_model_folder
        int_bin_dict_path = self.int_bin_dict_path
        damage_bins_path = PurePath.joinpath(path_stem, oasis_model_folder, self.dam_bin_dict_path)
        rootdir = PurePath.joinpath(path_stem, self.data_path)
        height_path = PurePath.joinpath(path_stem, oasis_model_folder, self.rel_height_path)
        occup_path = PurePath.joinpath(path_stem, oasis_model_folder, self.rel_occup_path)
        constr_path = PurePath.joinpath(path_stem, oasis_model_folder, self.rel_constr_path)
        countr_path = PurePath.joinpath(path_stem, oasis_model_folder, self.rel_countr_path)
        vulnerability_dict_path = PurePath.joinpath(path_stem, oasis_model_folder, self.vuln_dict_path)
        vulnerability_path = PurePath.joinpath(path_stem, self.oasis_model_folder, self.vulnerability_path)

        int_bins, int_mp_vals = self.init_int_bins(path_stem, max_int_val=1e22)
        int_bins_dict = self.create_int_bins(int_bins, int_mp_vals, self.int_mes_types)
        int_bins_df = pd.DataFrame(int_bins_dict)
        int_bins_df.to_csv(int_bin_dict_path, index=False)

        self.damage_bins_df.to_csv(damage_bins_path, index=False)

        locations_dict = self.directories(rootdir, self.cov_type)
        location_dict = {country: locations_dict[(continent, country)] for (continent, country) in locations_dict}
        self.sense_check_again(self.continent_specific, self.country_specific, locations_dict)

        height_dict = self.get_codes('attribute4', height_path)
        occup_dict = self.get_codes('attribute6', occup_path)
        constr_dict = self.get_codes('attribute2_3', constr_path, extra_keys_in = ['peril_id', 'ranking'])
        countr_dict = self.get_codes('country', countr_path)

        vuln_ids = self.create_vuln_ids(location_dict, height_dict, occup_dict, constr_dict, countr_dict)
        vulns_df = pd.DataFrame(vuln_ids)
        # vulns_df = self.filter_vuln_dict(vulns_df, countr_dict, self.country_specific)
        vuln_dict = self.get_vuln_ids(vulns_df, countr_dict, locations_dict)
        vulns_df.to_csv(vulnerability_dict_path, index=False)

        with open (PurePath.joinpath(path_stem, oasis_model_folder, vulnerability_path), 'wb') as dict_file:
            header = True
            current_location = ('','')
            prev_country = 0
            for (continent, country, taxonomy, node_id) in vuln_dict:
                if country != prev_country:
                    counter = 1
                # not () and not () functions as NAND logic gate
                if (not(self.country_specific) and not(self.continent_specific)) or continent in self.continent_specific or country in self.country_specific:
                    if country != current_location:
                        current_location = country
                        pathway = location_dict[country]
                        tree = ET.parse(pathway)
                        root = tree.getroot()
                    # node id easier to understand than using complicated logic to track progress
                    print ('running', continent, country, ': ', counter, '/', sum(1 for key in vuln_dict if key[1] == country))
                    counter += 1
                    prev_country = country
                    intensity_measure = root[0][int(node_id)][0].attrib['imt']
                    int_vals = list(map(float,root[0][int(node_id)][0].text.split()))
                    mean_LRs = list(map(float,root[0][int(node_id)][1].text.split()))
                    coeff_vars = list(map(float,root[0][int(node_id)][2].text.split()))
                    std_devs = [mean_LR*coeff_var for mean_LR, coeff_var in zip(mean_LRs, coeff_vars)]
                    
                    # return list of bin index values for each intensity value used to calulcate a probability
                    bin_index_list = self.get_bin_index(int_bins, int_mp_vals, self.int_mes_types, intensity_measure)
                    # vulnerability id can be found with either taxonomy or node id 
                    vuln_id = vuln_dict[(continent, country, taxonomy, node_id)]
                    # create vulnerability file with vulnerability ids, correct intensity bin indexes and damage bin indexes
                    vuln_data = self.create_vuln_dict(vuln_id, bin_index_list, self.damage_bins)
                    vuln_temp_df = pd.DataFrame(vuln_data)
                    
                    # function compute_probs() computes probabilities using beta distribution
                    probs_data = self.compute_probs(int_vals, int_mp_vals, mean_LRs, std_devs, self.damage_bins_list)
                    probs_df = pd.DataFrame(probs_data)

                    vuln_df = pd.concat([vuln_temp_df, probs_df], axis = 1)        
                    vuln_df.to_csv(dict_file, index=False, header=header)       
                    header = False
            
class JrcModel(BaseModel):
    def __init__(self, kwargs):
        super().__init__(kwargs)
        super().run()

    def create_vuln_ids(self, cont_list, occup_to_id, damage_classes, cont_to_id):
        vuln_ids = {}
        cur_vuln_id = 1    

        for continent in cont_list:
            for damage_class in damage_classes:
                occup = damage_class[:3].upper()
                occup_id = occup_to_id[occup]
                if (continent, damage_class) not in vuln_ids:
                    vuln_ids[(continent, damage_class)] = cur_vuln_id
                    cur_vuln_id += 1

                    id_dict = {
                        "vulnerability_id": vuln_ids[(continent, damage_class)],
                        "continent": continent,
                        "continent_id": cont_to_id[continent],
                        "occupancy": occup,
                        "occupancy_id": occup_id
                    }
                    yield id_dict

    def get_vuln_ids(self, vulns_df):
        vuln_dict = {}

        for index, row in vulns_df.iterrows():
            vuln_id = row['vulnerability_id']
            vuln_cont = row['continent_id']
            vuln_type = row['occupancy_id']
            vuln_dict[(vuln_cont, vuln_type)] = vuln_id
        return vuln_dict

    def use_excel(self, data_path, cont_to_id, occup_to_id):
        df_xlsx = pd.ExcelFile(data_path)
        data = pd.read_excel(df_xlsx, 'Damage functions', na_filter=False, header=[1,2])
        damage_classes = []
        int_vals_dict = {}
        region_dict = {}
        probs_list_dict = {}
        std_devs_dict = {} 
        excel_rows = []
        for dam_class in data['Damage\nclass'].iloc[:, [0][0]]:
            if dam_class:
                damage_classes.append(dam_class)

        for region in data['Damage function']:
            location = ''
            cur_region = region
            if '\n' in region:
                region = ' '.join(region.split('\n'))

            if ' ' in region:
                for word in region.split():
                    word = word.capitalize()
                    if not location:
                        location = word
                    else:
                        location = ' '.join((location, word))
            else:
                location = region.capitalize()
            region_dict[cur_region] = location
        
        header = True
        for i, dam_class in enumerate (data['Damage\nclass'].iloc[:, [0][0]]):
            if header:
                start = i
            if dam_class and not (header):
                end = i
                excel_rows.append((start, end))
                start = end
            header = False
        end = i+1
        excel_rows.append((start, end))

        # create list of list of intensity bin values for each damage class - SHOULD THIS BE GENERAL?
        for region in data['Damage function']:
            fted_region = region_dict[region]
            for index, damage_class in zip(excel_rows, damage_classes):
                occup = damage_class[:3].upper()
                length_class = index[1] - index[0]
                vals_set = []
                for i, val in enumerate(data['Flood depth,\n[m]'].iloc[index[0]:index[1],[0][0]]):
                    vals_set.append(val)
                    if len(vals_set) == length_class:
                        int_vals_dict[(cont_to_id[fted_region], occup_to_id[occup])] = vals_set[:length_class]
                        continue
                    
        for region in data['Damage function']:
            fted_region = region_dict[region]
            for index, damage_class in zip(excel_rows, damage_classes):
                occup = damage_class[:3].upper()
                region_probs = []
                region_std_devs = []
                length_class = index[1] - index[0]
                for i, (prob, std_dev) in enumerate(zip(data['Damage function'].loc[index[0]:index[1], [region][0]], data['Standard deviation'].loc[index[0]:index[1], [region][0]])):
                    if type(prob) == int or type(prob) == float:
                        region_probs.append(round(prob,12))
                    else:
                        region_probs.append(0) 
                    if type(std_dev) == int or type(std_dev) == float:
                            region_std_devs.append(std_dev)
                    else: 
                        region_std_devs.append(0) 
                    if len(region_probs) == length_class:                    
                        probs_list_dict[(cont_to_id[fted_region], occup_to_id[occup])] = region_probs[:length_class]
                        std_devs_dict[(cont_to_id[fted_region], occup_to_id[occup])] = region_std_devs[:length_class]
                        continue

        return damage_classes, int_vals_dict, region_dict, probs_list_dict, std_devs_dict

    def analyse_exposure(self, vulns_df):
        input_data = []
        
        for index, data in vulns_df.iterrows():
            input_data.append((data['continent_id'], data['occupancy_id']))

        return input_data

    def run(self):
        path_stem = self.path_stem
        oasis_model_folder = self.oasis_model_folder
        int_bin_dict_path = self.int_bin_dict_path
        damage_bins_path = PurePath.joinpath(path_stem, oasis_model_folder, self.dam_bin_dict_path)
        vulnerability_dict_path = PurePath.joinpath(path_stem, oasis_model_folder, self.vuln_dict_path)
        vulnerability_path = PurePath.joinpath(path_stem, self.oasis_model_folder, self.vulnerability_path)

        # creates dictionary to pair attrbiutes with attribute ids
        occup_path = PurePath.joinpath(path_stem, oasis_model_folder, self.rel_occup_path)
        countr_path = PurePath.joinpath(path_stem, oasis_model_folder, self.rel_countr_path)
        occup_dict = self.get_codes('occupancy', occup_path, extra_keys_in=['OccupancyCode'])
        location_dict = self.get_codes('continent', countr_path, extra_keys_in=['CountryCode'])

        # generate dictionaries required to convert from/to attribute ids
        cont_to_id = {continent: location_dict[(continent, country_code)] for (continent, country_code) in location_dict}
        id_to_cont = {location_dict[(continent, country_code)]: continent for (continent, country_code) in location_dict}
        id_to_occup = {occup_dict[(occupancy, occupancy_code)]: occupancy for (occupancy, occupancy_code) in occup_dict}
        occup_to_id = {occupancy: occup_dict[(occupancy, occupancy_code)] for (occupancy, occupancy_code) in occup_dict}

        # list of all locations with data in vulnerability models folder stored in dictionary as key and the value is the relative path
        rootdir = PurePath.joinpath(path_stem, self.data_path)
        damage_classes, int_vals_dict, region_dict, probs_list_dict, std_devs_dict = self.use_excel(rootdir, cont_to_id, occup_to_id)
        cont_list = [each[1] for each in region_dict.items()]
        max_int_val = max([max(int_list) for int_list in int_vals_dict.values()])

        # hazard intensity bins created and a dictionary is outputted where maximum intensity value cannot exceed max. in sorce data
        int_bins, int_mp_vals = self.init_int_bins(path_stem, max_int_val)
        int_bins_dict = self.create_int_bins(int_bins, int_mp_vals, self.int_mes_types)
        int_bins_df = pd.DataFrame(int_bins_dict)
        int_bins_df.to_csv(int_bin_dict_path, index=False)

        # damage bin list does not have repeated start or end - length 102 - same as number of damage bins
        self.damage_bins_df.to_csv(damage_bins_path, index=False)

        # output vulnerability id dictionary - each occupancy type within each continent is given unique id 
        vuln_ids = self.create_vuln_ids(cont_list, occup_to_id, damage_classes, cont_to_id)
        vuln_ids_path = PurePath.joinpath(path_stem, oasis_model_folder, vulnerability_dict_path)
        vulns_df = pd.DataFrame(vuln_ids)
        vulns_df.to_csv(vuln_ids_path, index=False)
        # create object which is dictionary of vulnerability ids with continent_id, occupancy_id as keys
        vuln_dict = self.get_vuln_ids(vulns_df)

        # list of entries in location_csv extracted as tuple (continent_id, occupancy_id)
        exposure_data = self.analyse_exposure(vulns_df)

        with open (PurePath.joinpath(path_stem, oasis_model_folder, vulnerability_path), 'wb') as dict_file:
            header = True
            counter = 1
        
            if self.continent_specific:
                length = len([cont_id for (cont_id, _) in exposure_data if id_to_cont[cont_id] in self.continent_specific])
            else:
                length = len(exposure_data)

            for j, exp_entry in enumerate(exposure_data):
                cont_id = exp_entry[0]
                occup_id = exp_entry[1]
                continent = id_to_cont[cont_id]
                vuln_id = vuln_dict[(cont_id, occup_id)]
                cur_data = (cont_id, occup_id)

                if (self.continent_specific and continent in self.continent_specific) or not self.continent_specific:
                    print ('running', continent, id_to_occup[occup_id], ': ', counter, '/', length)
                    counter+=1
                else:
                    continue
                
                # return list of bin index values for each intensity value used to calulcate a probability
                bin_index_list = self.get_bin_index(int_bins, int_mp_vals, self.int_mes_types, 'flood_depth_meters')
                # create vulnerability file with vulnerability ids, correct intensity bin indexes and damage bin indexes
                vuln_data = self.create_vuln_dict (vuln_id, bin_index_list, self.damage_bins)
                vuln_temp_df = pd.DataFrame(vuln_data)

                # function compute_probs() computes damage functuion values using beta distribution
                int_vals, mean_LRs, std_devs = int_vals_dict [cur_data], probs_list_dict [cur_data], std_devs_dict [cur_data]
                probs_data = self.compute_probs(int_vals, int_mp_vals, mean_LRs, std_devs, self.damage_bins_list)
                probs_df = pd.DataFrame(probs_data)
                vuln_df = pd.concat([vuln_temp_df, probs_df], axis=1)  
                vuln_df.to_csv(dict_file, index=False, header=header)       
                header = False

class WindstormModel(BaseModel):
    def __init__(self, kwargs):
        super().__init__(kwargs)
        super().run()

        self.max_int_val = kwargs['max_int_val']
        self.v_thresh = kwargs['v_thresh']
        self.v_half = kwargs['v_half']

        self.footprint_path = 'model_data/footprint.csv'
        self.rel_areaperil_path = 'keys_data/areaperil_dict.csv'

    def get_int_bin_id(self, int_bin_df, intensity):
        int_bin_df = int_bin_df
        copy_df = int_bin_df.copy()
        copy_df.drop(index = 0, inplace = True)
        copy_df.drop(index = len(copy_df), inplace = True)

        if intensity <= copy_df.iloc[0,2]:
            intensity_bin_id = copy_df.iloc[1,0] - 1
        elif intensity >= copy_df.iloc[len(copy_df) - 1, 3]:
            intensity_bin_id = copy_df.iloc[len(copy_df) - 1, 0] + 1
        else:
            differences = abs(copy_df['interpolation'] - intensity)
            intensity_bin_id = copy_df['bin_index'][differences.idxmin()]

        return intensity_bin_id
    
    def create_damage_bins(self, damage_bins):
        damage_bin_dict = {}
        index_num = 1
        for dam_bin in damage_bins:
            interp_val = (dam_bin[0] + dam_bin[1]) / 2
            interp_val = round(interp_val, 16)
            damage_bin_dict = {'bin_index': index_num,
                            'bin_from': "{:.6f}".format(dam_bin[0]),
                            'bin_to': "{:.6f}".format(dam_bin[1]), 
                            'interpolation': float("{:.6f}".format(interp_val))
                            }
            index_num += 1
            yield damage_bin_dict
    
    def windstorm_function(self, v_thresh, v_half,intensity):
        if intensity < v_thresh:
            intensity = v_thresh
        v_ij = (intensity - v_thresh) / (v_half - v_thresh)
        damage = (v_ij**3) / (1 + v_ij**3)

        return damage
    
    def get_damage_bin(self, damage_bin_df,damage):
        copy_df = damage_bin_df.copy()
        copy_df.drop(index = 0, inplace = True)
        copy_df.drop(index = 101, inplace = True)

        if damage <= 0:
            damage_bin_id = 1
        elif damage >= 1:
            damage_bin_id = 102
        else:
            differences = abs(copy_df['interpolation'] - damage)
            damage_bin_id = copy_df['bin_index'][differences.idxmin()]

        return damage_bin_id
    
    def create_vulnerability(self, damage_bin_df,int_bins_df, v_thresh, v_half):
        for index1, data1 in int_bins_df.iterrows():

            intensity = data1['interpolation']
            damage = self.windstorm_function(v_thresh, v_half, intensity)
            damage_bin_id = self.get_damage_bin(damage_bin_df, damage)
            for index2, data2 in damage_bin_df.iterrows():
                if damage_bin_id == index2 + 1:
                    probability = 1
                else:
                    probability = 0
                vulnerability = {
                                'vulnerability_id': 1, 
                                'intensity_bin_id': data1['bin_index'], 
                                'damage_bin_id': index2 + 1, 
                                'probability': probability
                                }
                yield vulnerability
    
    def convert_footprint(self, input_footprint, int_bin_df):
        fp_df = pd.read_csv(input_footprint)
        
        areaperil_df = fp_df[['latitude','longitude']].copy()
        areaperil_df.insert(loc = 0, column = 'area_peril_id', value = np.arange(1, len(areaperil_df)+1))
        
        footprint_df = fp_df[['Vg_mph']].copy()
        footprint_df.insert(loc = 0, column = 'event_id', value = 1)
        footprint_df.insert(loc = 1, column = 'area_peril_id', value = np.arange(1, len(footprint_df)+1))
        footprint_df['intensity_bin_id'] = footprint_df['Vg_mph'].apply( lambda x: self.get_int_bin_id(int_bin_df, x))
        footprint_df.insert(loc = 4, column = 'probability', value = 1)
        footprint_df.drop(columns='Vg_mph', inplace = True)

        return  areaperil_df, footprint_df
    
    def run(self):        
        path_stem = self.path_stem
        oasis_model_folder = self.oasis_model_folder
        int_bin_dict_path = self.int_bin_dict_path
        # vuln_csv_dest is vulnerability path
        damage_bins_path = PurePath.joinpath(path_stem, oasis_model_folder, self.dam_bin_dict_path)
        vulnerability_dict_path = PurePath.joinpath(path_stem, oasis_model_folder, self.vuln_dict_path)
        vulnerability_path = PurePath.joinpath(path_stem, self.oasis_model_folder, self.vulnerability_path)
        input_footprint = PurePath.joinpath(self.path_stem, self.data_path)
        output_footprint = PurePath.joinpath(self.path_stem, self.oasis_model_folder, self.footprint_path)
        areaperil_dict_path = PurePath.joinpath(path_stem, oasis_model_folder, self.rel_areaperil_path)

        int_bins, int_mp_vals = self.init_int_bins(path_stem, self.max_int_val)
        int_bins_dict = self.create_int_bins(int_bins, int_mp_vals, self.int_mes_types)
        int_bins_df = pd.DataFrame(int_bins_dict)
        int_bins_df.to_csv(int_bin_dict_path, index=False)

        self.damage_bins_df.to_csv(damage_bins_path, index=False)
        
        int_bins, int_mp_vals = self.init_int_bins(path_stem, self.max_int_val)
        int_bins_dict = self.create_int_bins(int_bins, int_mp_vals, self.int_mes_types)
        int_bins_df = pd.DataFrame(int_bins_dict)
        self.convert_footprint(input_footprint, int_bins_df)

        vuln_data = self.create_vulnerability(self.damage_bins_df, int_bins_df, self.v_thresh, self.v_half)
        vuln_df = pd.DataFrame(vuln_data)
        vuln_df.to_csv(vulnerability_path, index=False)

        areaperil_df, footprint_df = self.convert_footprint(input_footprint, int_bins_df)
        areaperil_df.to_csv(areaperil_dict_path, index=False)
        footprint_df.to_csv(output_footprint, index=False)

def str_to_class(classname):
    return getattr(sys.modules[__name__], classname)

def main(to_run):
    if to_run == 'GemModel':
        parser.add_argument('-M', "--oasis-model-folder", default='global_earthquake_variants', help="input name of folder containing the files and data to run oasis lmf model")
        parser.add_argument('-imt', "--int-mes-types", default='PGA SA(0.3) SA(0.6) SA(1.0)', type=str, help="enter the unique intensity measurement types in the model data with a space in between each type as a string")
        parser.add_argument('-F', "--data-path", default="global_earthquake_data", help="input name of folder containing the vulnerability functions")
    
    if to_run == 'JrcModel':
        parser.add_argument('-M', "--oasis-model-folder", default='global_flooding_variants', help="input name of folder containing the files and data to run oasis lmf model")
        parser.add_argument('-imt', "--int-mes-types", default='flood_depth_metres', type=str, help="enter the unique intensity measurement types in the model data")
        parser.add_argument('-F', "--data-path", default="global_flooding_data.xlsx", help="input name of file containing the vulnerability functions")
    
    if to_run == 'WindstormModel':
        parser.add_argument('-M', "--oasis-model-folder", default="global_windstorm_variants", help="input name of folder containing the files and data to run oasis lmf model")
        parser.add_argument('-imt', "--int-mes-types", default="windspeed_mph", type=str, help="enter the unique intensity measurement types in the model data")
        parser.add_argument('-F', "--data-path", default="2018_Michael_windgrid.csv", help = "input name of file containing the vulnerability functions")
        parser.add_argument('-v', "--max_int_val", default=150, type=int, help="input maximum hazard intensity value")
        parser.add_argument('-thresh', "--v-thresh", default=25.7, type=float, help="input hazard intensity value threshold")
        parser.add_argument('-half', "--v-half", default=74.7, type=float, help=r"input half hazard intensity value required for 50% damage")

    kwargs = vars(parser.parse_args())
    model = str_to_class(to_run)(kwargs)
    model.run()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description = "Set up paths to receive inputs and output files to for vulnerability function data")
    parser.add_argument('-W', "--working-folder", default='VulnerabilityLibrary', help="input the name of the folder containing the source code and the oasis lmf model")
    parser.add_argument('-d', "--num-damage-bins", default=100, type=int, help="input number of damage bins")
    parser.add_argument('-count', "--country-specific", default='', nargs='+', help="to analyse specific countries, input the name of which one to run the model on")
    parser.add_argument('-cont', "--continent-specific", default='', nargs='+', help="to analyse specific continents, input the name of which one to run the model on")
    parser.add_argument('-cov', "--coverage-type", default='structural', help="input structural, contents or non_structural")

main('WindstormModel')
