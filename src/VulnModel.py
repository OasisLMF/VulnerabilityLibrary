import csv
import os
import pandas as pd
import xml.etree.ElementTree as ET
import itertools
import numpy as np
from scipy import interpolate
from scipy.stats import beta
from pathlib import Path
import time

# Bin sizes for each IMT
# Intensity bins input should be csv file 
# Do I have enough information to create a footprint file

# set number of damage bins
num_damage_bins = int (100)
incr = 0.01

# Intensity Type is root[0][i][0].attrib['imt']
# Taxonomy is root[0][i].attrib['id']pi
# Intensity is root[0][i][0].text
# Mean Loss Ratio is root[0][i][1].text
# Coefficient of Variation is root[0][i][2].text

def create_int_bins():
    # Open the 'intensity_bins.txt' file in the 'Inputs' folder where user manually inputs the intensity bins
    with open('.\\VulnerabilityLibrary\\Inputs\\intensity_bins.txt') as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        int_vals_mp = []
        # Read in bins data from opened file
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
        for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
            # int_vals_mp.append((val_lb + val_ub) / 2)
            int_val_mp = (val_lb + val_ub) / 2
            int_bin_dict = {'bin_from': val_lb, 
                            'bin_to': val_ub, 
                            'interpolation': int_val_mp
                            }
            yield int_bin_dict

def create_damage_bins():
    damage_bin_dict = {}
    damage_bins = np.linspace(0, 1, num_damage_bins, endpoint=True).round(2)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1])
    # damage_bins = np.append(damage_bins, damage_bins[-1]).round(2)
    for i in range (1, len(damage_bins)):
        interp_val = (damage_bins[i-1] + damage_bins[i]) / 2
        interp_val = interp_val.round(3)
        damage_bin_dict = {'bin_from': "{:.6f}".format(damage_bins[i-1]),
                     'bin_to': "{:.6f}".format(damage_bins[i]),
                     'interpolation': "{:.6f}".format(interp_val)
                     }
        yield damage_bin_dict

def directories():
    # Use argparse to write this line
    rootdir = "./global_vulnerability_model"
    location_dict = {}
    # Count included so that not all files are analysed every single run
    count = 0
    coverage_types = ['structural', 'nonstructural', 'contents']
    # Only looking at data for vulnerability_structural files now
    coverage_type = coverage_types [0]
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            # don't need to replace - can use double backslash or r'\path'
            subdir = subdir.replace('\\', '/')
            # can use pathlib works with windows and linux: p = Path(subdir, file)
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_{cov_type}.xml".format(cov_type = coverage_type)):
                # can call parent of path - try .parent[-1]
                # os.path and pathlib libraries good for windows and linux
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                # location[0] represents a continent 
                # location[1] represents a country
                location_dict [location[0], location[1]] = filepath
                count += 1
            if count > 1:
                break
    return location_dict, coverage_type

def find_nearest(array, value):
    yield 0
    # array = np.asarray(array)
    # idx = (np.abs(array - value)).argmin()
    # yield idx

def get_bin_index():
    with open('./VulnerabilityLibrary/Inputs/intensity_bins.txt') as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        int_vals_mp = []
        # Read in bins data from opened file
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
        for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
            int_vals_mp.append((val_lb + val_ub) / 2)
    return int_vals_mp[1:-1]

def get_vuln_ids():
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open('./VulnerabilityLibrary/Inputs/vulnerability_dict.csv') as file:
        vuln_dict = {}
        # Read in bins data from opened file
        for line in file:
            line = line.replace('\n', '').split(',')
            # if line:
            #     int_vals_lb.append(float(line))
            #     int_vals_ub.append(float(line))
            vuln_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
    return vuln_dict

def get_height_codes():
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open('./VulnerabilityLibrary/Inputs/height_dict.csv') as file:
        height_dict = {}
        # Read in bins data from opened file
        for line in file:
            line = line.replace('\n', '').split(',')
            # height_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
            height_dict[line[1]] = line[2]
    return height_dict

def get_constr_codes():
    with open('./VulnerabilityLibrary/Inputs/construction_dict.csv') as file:
        constr_dict = {}
        # Read in bins data from opened file
        for line in file:
            line = line.replace('\n', '').split(',')
            constr_dict[line[3]] = line[4]
    return constr_dict

def get_occup_codes():
    occup_dict = {'RES': 1, 'COM': 2, 'IND': 3}
    return occup_dict

def get_cov_type_codes():
    cov_type_dict = {'structural': 1, 'nonstructural': 2, 'contents': 3}
    return cov_type_dict

def create_vuln_dict(root, vuln_dict, height_dict, constr_dict, occup_dict, cov_type_dict, coverage_type, int_list):
    peril_id = 'QEQ'
    coverage_type = str(cov_type_dict[coverage_type])
    for i in range (len(root[0])-1):
        taxonomy = root[0][i+1].attrib['id'] 
        taxonomy = taxonomy.strip().split('/')
        attribute_2 = constr_dict[taxonomy[0]]
        attribute_6 = str(occup_dict[taxonomy[-1]])
        if taxonomy[-2][0] == 'H':
            attribute_4 = height_dict[taxonomy[-2]]
        elif taxonomy[-3][0] == 'H':
            attribute_4 = height_dict[taxonomy[-3]]
        # for j, (intensity, MLR, CoV) in enumerate (zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' '))):
        # wrong - for j, each in enumerate(root[0][i+1][0].text.strip().split(' ')):
        for j, int_value in enumerate(root[0][i+1][0].text.strip().split(' ')):
            bin_index = list(find_nearest(int_list, float(int_value)))[0]
            bin_index += 1
            
            for k in range(1, num_damage_bins+1):
                # val_id = vuln_dict.get((attribute_2, peril_id, coverage_type, attribute_4, attribute_6), "None: {something}".format(something = (attribute_2, peril_id, coverage_type, attribute_4, attribute_6)))
                val_id = vuln_dict.get((attribute_2, peril_id, coverage_type, attribute_4, attribute_6), "Unknown")
                rec = {
                    # converted valuation_id to string so that an integer would be displayed
                    "vulnerability_id":  val_id,
                    "intensity_bin_id": int(bin_index),
                    "damage_bin_id": "{:.0f}".format(k), 
                    }
                yield rec

def compute_probs(root, int_bin_index):
    a=0
    b=1
    probs = {}
    damage_bin_list = np.arange(incr,1 + incr, incr)
    for i in range (len(root[0])-1):
        int_vals = []
        mean_LRs = []
        std_dev_vals = []
        # is it best to (1) compute all alpha, beta values then compute cdf iteratively or (2) find alpha and beta then compute cdf repeatedly? I say (2)
        for intensity, meanLR, CoV in zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' ')):
            meanLR = float (meanLR)
            intensity = float (intensity)
            CoV = float (CoV)
            # rounded to 18 digits to avoid bit error 
            std_dev_val = round((meanLR * CoV), 18)
            int_vals.append (intensity)
            mean_LRs.append (meanLR)
            std_dev_vals.append(std_dev_val)
        # better to use cubic or linear interpolation?
        # note: cubic has slower run time than linear
        int_vs_mean = interpolate.interp1d(int_vals, mean_LRs, kind='linear')
        int_vs_std= interpolate.interp1d(int_vals, std_dev_vals, kind='linear')
        # function that can take in list of values to be interpolated has been created above using scipy 
        interp_mean = int_vs_mean (int_bin_index)
        interp_std_dev = int_vs_std (int_bin_index)            

        for j, (mean_val, std_dev_val) in enumerate (zip (interp_mean, interp_std_dev)):
            # beta paramters for last few bins are very large, indicating dirac delta function shape which makes sense
            alpha_val = (mean_val-a)/(b-a) * ((mean_val*(1-mean_val)/std_dev_val**2) - 1)
            beta_val = alpha_val * (1-mean_val) / mean_val

            # beta function automatically runs has start and end interval from 0 to 1?
            vals = beta.cdf(damage_bin_list, alpha_val, beta_val)
            vals_temp = np.append(0, vals)
            cum_diff_vals = list(np.diff(vals_temp, axis=0))

            for i, prob in enumerate(cum_diff_vals):
                probs = {'probabilities': "{:.6f}".format(prob)
                    }
                yield probs


def main():
    #print conutry contininet and number you're at out of how many
    print ('create intensity bins')
    init_int_bins = create_int_bins()
    # Format data into a table and output csv file
    intensity_df = pd.DataFrame(init_int_bins)
    # Use argparse to write this line
    intensity_df.to_csv('./VulnerabilityLibrary/Dicts/intensity_bin_dict.csv', index=True, index_label='bin_index')

    print ('create damage bins')
    init_dam_bins = create_damage_bins()
    damage_df = pd.DataFrame(init_dam_bins)
    damage_df.to_csv('./VulnerabilityLibrary/Dicts/damage_bin_dict.csv', index=True, index_label='damage_bin_index')

    print ('get directories')
    directory = directories()
    location_list = directory [0]
    coverage_type = directory [1]

    print ('get info')
    vuln_dict = get_vuln_ids()
    height_dict = get_height_codes()
    constr_dict = get_constr_codes()
    occup_dict = get_occup_codes()
    cov_type_dict = get_cov_type_codes()

    print ('create bin index')
    int_bin_index = get_bin_index()

    print ('create vulnerability dict')
    with open ('./VulnerabilityLibrary/Dicts/vulnerability_dict.csv', 'wb') as dict_file:
        header = True
        for (continent, country) in location_list:
            print ('running', continent, country)
            pathway = location_list[(continent, country)]
            tree = ET.parse(pathway)
            root = tree.getroot()
            vuln_data = create_vuln_dict (root, vuln_dict, height_dict, constr_dict, occup_dict, cov_type_dict, coverage_type, int_bin_index)
            t_zero = time.time()
            vuln_temp_df = pd.DataFrame(vuln_data)
            print (time.time() - t_zero)
            print ('running probabilities')
            probs_data = compute_probs(root, int_bin_index)
            t_one = time.time()
            probs_df = pd.DataFrame(probs_data)
            print (probs_df)
            print (time.time() - t_one)
            print('concat')
            t_two = time.time()
            vuln_df = pd.concat([vuln_temp_df, probs_df], axis = 1)
            print (time.time() - t_two)
            t_three = time.time()
            vuln_df.to_csv(dict_file, index=False, header=header)
            print (time.time() - t_three)
            header = False
main()
    
    # compute_probs(df, location_list)
    # print (df.head(30))
    # print (df2.head(30))
    # df2 = pd.DataFrame(damage_bin_dict)
    # df = pd.concat([df2, df])
    # df.to_csv('./VulnerabilityLibrary/Dicts/damage_bin_dict.csv', index=False)
    # df_footrpint = generate_footprint(location_list, coverage_type)
    # df = pd.DataFrame(df_footrpint)
    # df = df.head(10000)
    # df_vuln_ids = generate_vuln_ids(location_list)
    # df2 = pd.DataFrame(df_vuln_ids)
    # df2 = df2.head(10000)
    # df.to_csv('GEM/footprint.csv', index=False)
    # df2.to_csv('GEM/vuln_ids.csv', index=False)