import csv
import os
import pandas as pd
import xml.etree.ElementTree as ET
import itertools
import numpy as np
from scipy import interpolate
from scipy.stats import beta
from pathlib import Path
import time
import sys

# Bin sizes for each IMT
# Intensity bins input should be csv file 

# set number of damage bins
num_damage_bins = int (100)
incr = 0.01

# Intensity Type is root[0][i][0].attrib['imt']
# Taxonomy is root[0][i].attrib['id']pi
# Intensity is root[0][i][0].text
# Mean Loss Ratio is root[0][i][1].text
# Coefficient of Variation is root[0][i][2].text

def create_int_bins():
    # Open the 'intensity_bins.json' file in the 'Inputs' folder where user manually inputs the intensity bins
    with open('.\\VulnerabilityLibrary\\Inputs\\intensity_bins.json') as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        int_vals_mp = []
        # Read in bins data from opened file
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
        for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
            # int_vals_mp.append((val_lb + val_ub) / 2)
            int_val_mp = (val_lb + val_ub) / 2
            int_bin_dict = {'bin_from': val_lb, 
                            'bin_to': val_ub, 
                            'interpolation': int_val_mp
                            }
            yield int_bin_dict

def create_damage_bins():
    damage_bin_dict = {}
    damage_bins = np.linspace(0, 1, num_damage_bins, endpoint=True).round(2)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1])
    # damage_bins = np.append(damage_bins, damage_bins[-1]).round(2)
    for i in range (1, len(damage_bins)):
        interp_val = (damage_bins[i-1] + damage_bins[i]) / 2
        interp_val = interp_val.round(3)
        damage_bin_dict = {'bin_from': "{:.6f}".format(damage_bins[i-1]),
                     'bin_to': "{:.6f}".format(damage_bins[i]),
                     'interpolation': "{:.6f}".format(interp_val)
                     }
        yield damage_bin_dict

def directories():
    # Use argparse to write this line
    rootdir = "./global_vulnerability_model"
    location_dict = {}
    # Count included so that not all files are analysed every single run
    count = 0
    coverage_types = ['structural', 'nonstructural', 'contents']
    # Only looking at data for vulnerability_structural files now
    coverage_type = coverage_types [0]
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            # don't need to replace - can use double backslash or r'\path'
            subdir = subdir.replace('\\', '/')
            # can use pathlib works with windows and linux: p = Path(subdir, file)
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_{cov_type}.xml".format(cov_type = coverage_type)):
                # can call parent of path - try .parent[-1]
                # os.path and pathlib libraries good for windows and linux
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                # location[0] represents a continent 
                # location[1] represents a country
                location_dict [location[0], location[1]] = filepath
                count += 1
            if count > 1:
                break
    return location_dict, coverage_type

def find_nearest(array, value):
    array = np.asarray(array)
    idx = (np.abs(array - value)).argmin()
    yield idx

def get_bin_index():
    with open('./VulnerabilityLibrary/Inputs/intensity_bins.json') as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        int_vals_mp = []
        # Read in bins data from opened file
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
        for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
            int_vals_mp.append((val_lb + val_ub) / 2)
    int_bin_vals = int_vals_lb[1:]
    int_vals_mp = int_vals_mp[1:-1]
    return int_bin_vals, int_vals_mp

def get_vuln_ids():
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open('./VulnerabilityLibrary/Inputs/vulnerability_dict_old.csv') as file:
        vuln_dict = {}
        # Read in bins data from opened file
        for line in file:
            line = line.replace('\n', '').split(',')
            # if line:
            #     int_vals_lb.append(float(line))
            #     int_vals_ub.append(float(line))
            vuln_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
    return vuln_dict

def get_height_codes():
    header = True
    height_dict = {}
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open(r'VulnerabilityLibrary\ParisWindstormVariant\keys_data\height_dict.csv') as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                height_code = data.index('attribute4')
                height_id = data.index('attribute4_id')
                header = False
            else:
                height_dict[data[height_code]] = data[height_id]
            # height_dict[line[1]] = line[2]
    return height_dict
            

def get_constr_codes():
    header = True
    constr_dict = {}
    with open(r'VulnerabilityLibrary\ParisWindstormVariant\keys_data\construction_dict.csv') as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                constr_type = data.index('attribute2')
                constr_id = data.index('attribute2_id')
                header = False
            else:
                constr_dict[data[constr_type]] = data[constr_id]
    return constr_dict

def get_occup_codes():
    # occup_dict = {'RES': 1, 'COM': 2, 'IND': 3}
    # return occup_dict
    header = True
    occup_dict = {}
    with open (r'VulnerabilityLibrary\ParisWindstormVariant\keys_data\occupancy_dict.csv', "r") as file:
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                occup_type = data.index('attribute6')
                occup_id = data.index('attribute6_id')
                header = False
            else:
                occup_dict[data[occup_type]] = data[occup_id]
    return occup_dict

def get_cov_type_codes():
    cov_type_dict = {'structural': 1, 'nonstructural': 2, 'contents': 3}
    return cov_type_dict

def create_vuln_dict(root, vuln_dict, height_dict, constr_dict, occup_dict, cov_type_dict, coverage_type, int_bin_vals, int_mp_vals):
    # Is this okay- or could it be something other than 'QEQ'?
    peril_id = 'QEQ'
    coverage_type = str(cov_type_dict[coverage_type])
    for i in range (len(root[0])-1):
        IMT = root[0][i+1][0].attrib['imt']
        taxonomy = root[0][i+1].attrib['id'] 
        taxonomy = taxonomy.strip().split('/')
        attribute_2 = constr_dict[taxonomy[0]]
        attribute_6 = str(occup_dict[taxonomy[-1]])
        # ask Ben about this- is it general enough
        if taxonomy[-2][0] == 'H':
            attribute_4 = height_dict[taxonomy[-2]]
        elif taxonomy[-3][0] == 'H':
            attribute_4 = height_dict[taxonomy[-3]]
        # for j, (intensity, MLR, CoV) in enumerate (zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' '))):
        # wrong - for j, each in enumerate(root[0][i+1][0].text.strip().split(' ')):
        for j, int_value in enumerate(root[0][i+1][0].text.strip().split(' ')):
            bin_index = list(find_nearest(int_mp_vals, float(int_value)))[0]
            bin_index += 1
            if float(int_value) >= min(int_bin_vals) and float (int_value) <= max(int_bin_vals):
                # print (int_value, 'belongs in bin', bin_index)
                pass
            else:
                print('Intensity value of', int_value, 'is not within range of intensity bins inputed')
                sys.exit()
            if IMT == 'SA(0.3)':
                bin_index += len(int_mp_vals)
            if IMT == 'SA(0.6)':
                bin_index += 2*len(int_mp_vals)
            if IMT == 'SA(1.0)':
                bin_index += 3*len(int_mp_vals)
            for k in range(1, num_damage_bins+1):
                # val_id = vuln_dict.get((attribute_2, peril_id, coverage_type, attribute_4, attribute_6), "None: {something}".format(something = (attribute_2, peril_id, coverage_type, attribute_4, attribute_6)))
                val_id = vuln_dict.get((attribute_2, peril_id, coverage_type, attribute_4, attribute_6), "Unknown")
                rec = {
                    "vulnerability_id":  val_id,
                    "intensity_bin_id": int(bin_index),
                    "damage_bin_id": "{:.0f}".format(k), 
                    }
                yield rec

def compute_probs(root, int_bin_index):
    a=0
    b=1
    probs = {}
    damage_bin_list = np.arange(incr,1 + incr, incr)

    for i in range (len(root[0])-1):
        int_vals = []
        mean_LRs = []
        std_dev_vals = []
        # is it best to (1) compute all alpha, beta values then compute cdf iteratively or (2) find alpha and beta then compute cdf repeatedly? I say (2)
        for intensity, meanLR, CoV in zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' ')):
            meanLR = float (meanLR)
            intensity = float (intensity)
            CoV = float (CoV)
            # rounded to 18 digits to avoid bit error 
            std_dev_val = round((meanLR * CoV), 18)
            int_vals.append (intensity)
            mean_LRs.append (meanLR)
            std_dev_vals.append(std_dev_val)
        # better to use cubic or linear interpolation?
        # note: cubic has slower run time than linear
        int_vs_mean = interpolate.interp1d(int_vals, mean_LRs, kind='linear')
        int_vs_std= interpolate.interp1d(int_vals, std_dev_vals, kind='linear')
        # function that can take in list of values to be interpolated has been created above using scipy 
        interp_mean = int_vs_mean (int_bin_index)
        interp_std_dev = int_vs_std (int_bin_index)            

        for j, (mean_val, std_dev_val) in enumerate (zip (interp_mean, interp_std_dev)):
            # Beta paramters for last few bins are very large, indicating dirac delta function shape which makes sense
            # High intensity leads to high damage almost 100% of the time
            alpha_val = (mean_val-a)/(b-a) * ((mean_val*(1-mean_val)/std_dev_val**2) - 1)
            beta_val = alpha_val * (1-mean_val) / mean_val

            # beta function automatically runs has start and end interval from 0 to 1
            vals = beta.cdf(damage_bin_list, alpha_val, beta_val)
            vals_temp = np.append(0, vals)
            cum_diff_vals = list(np.diff(vals_temp, axis=0))

            for i, prob in enumerate(cum_diff_vals):
                probs = {'probabilities': "{:.6f}".format(prob)
                    }
                yield probs

def main():
    # Print conutry continent and number you're at out of how many
    init_int_bins = create_int_bins()
    # Format data into a table and output csv file
    intensity_df = pd.DataFrame(init_int_bins)
    # Use argparse to write this line
    # intensity_df.to_csv('./VulnerabilityLibrary/Dicts/intensity_bin_dict.csv', index=True, index_label='bin_index')

    init_dam_bins = create_damage_bins()
    damage_df = pd.DataFrame(init_dam_bins)
    # damage_df.to_csv('./VulnerabilityLibrary/Dicts/damage_bin_dict.csv', index=True, index_label='damage_bin_index')

    directory = directories()
    location_list = directory [0]
    coverage_type = directory [1]

    vuln_dict = get_vuln_ids()
    height_dict = get_height_codes()
    constr_dict = get_constr_codes()
    occup_dict = get_occup_codes()
    cov_type_dict = get_cov_type_codes()

    int_bin_vals = get_bin_index()[0]
    int_mp_vals = get_bin_index()[1]

    with open ('./VulnerabilityLibrary/Dicts/vulnerability_dict.csv', 'wb') as dict_file:
        header = True
        for (continent, country) in location_list:
            print ('running', continent, country)
            pathway = location_list[(continent, country)]
            tree = ET.parse(pathway)
            root = tree.getroot()
            vuln_data = create_vuln_dict (root, vuln_dict, height_dict, constr_dict, occup_dict, cov_type_dict, coverage_type, int_bin_vals, int_mp_vals)
            # t_zero = time.time()
            vuln_temp_df = pd.DataFrame(vuln_data)
            # print (time.time() - t_zero)
            probs_data = compute_probs(root, int_mp_vals)
            probs_df = pd.DataFrame(probs_data)
            vuln_df = pd.concat([vuln_temp_df, probs_df], axis = 1)
            vuln_df.to_csv(dict_file, index=False, header=header)
            header = False
main()
    
    # compute_probs(df, location_list)
    # print (df.head(30))
    # print (df2.head(30))
    # df2 = pd.DataFrame(damage_bin_dict)
    # df = pd.concat([df2, df])
    # df.to_csv('./VulnerabilityLibrary/Dicts/damage_bin_dict.csv', index=False)
    # df_footrpint = generate_footprint(location_list, coverage_type)
    # df = pd.DataFrame(df_footrpint)
    # df = df.head(10000)
    # df_vuln_ids = generate_vuln_ids(location_list)
    # df2 = pd.DataFrame(df_vuln_ids)
    # df2 = df2.head(10000)
    # df.to_csv('GEM/footprint.csv', index=False)
    # df2.to_csv('GEM/vuln_ids.csv', index=False)