import csv
import os
import pandas as pd
import xml.etree.ElementTree as ET
import itertools
import numpy as np
from scipy import interpolate
from scipy.stats import beta

# Do I have enough information to create a footprint file

# set number of damage bins
num_damage_bins = 50

# Intensity Type is root[0][i][0].attrib['imt']
# Taxonomy is root[0][i].attrib['id']
# Intensity is root[0][i][0].text
# Mean Loss Ratio is root[0][i][1].text
# Coefficient of Variation is root[0][i][2].text

def create_int_bins():
    # Open the 'intensity_bins.txt' file in the 'Inputs' folder where user manually inputs the intensity bins
    with open('./VulnerabilityLibrary/Inputs/intensity_bins.txt') as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        int_vals_mp = []
        # Read in bins data from opened file
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
        for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
            int_vals_mp.append((val_lb + val_ub) / 2)
            int_bin_dict = {'bin_from': int_vals_lb, 
                            'bin_to': int_vals_ub, 
                            'interpolation': int_vals_mp
                            }
    # Format data into a table and output csv file
    intensity_df = pd.DataFrame(int_bin_dict)
    intensity_df.to_csv('./VulnerabilityLibrary/Dicts/intensity_bin_dict.csv', index=True, index_label='bin_index')
    
def directories():
    rootdir = "./global_vulnerability_model"
    location_dict = {}
    # Count included so that not all files are analysed every single run
    count = 0
    coverage_types = ['structural', 'nonstructural','contents']
    # Only looking at data for vulnerability_structural files now
    coverage_type = coverage_types [0]
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            subdir = subdir.replace('\\', '/')
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_{cov_type}.xml".format(cov_type = coverage_type)):
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                # location[0] represents a continent 
                # location[1] represents a country
                location_dict [location[0], location[1]] = filepath
                count += 1
            if count > 2:
                break
    return location_dict, coverage_type

def create_footprint(location_list, vuln_dict):
    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
  
        for i in range (len(root[0])-1):
            int_mes_type = root[0][i+1][0].attrib['imt']
            taxonomy = root[0][i+1].attrib['id'] 
            taxonomy = taxonomy.strip().split('/')
            attribute_2 = taxonomy[0]
            attribute_6 = taxonomy[-1]
            if taxonomy[-2][0] == 'H':
                attribute_4 = taxonomy[-2]
            elif taxonomy[-3][0] == 'H':
                attribute_4 = taxonomy[-3]
            # for j, (intensity, MLR, CoV) in enumerate (zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' '))):
            # wrong - for j, each in enumerate(root[0][i+1][0].text.strip().split(' ')):
            for j in range (22):
                for k in range(52):
                    rec = {
                        # converted valuation_id to string so that an integer would be displayed
                        "valuation_id":  str(vuln_dict[(attribute_2, country, int_mes_type, taxonomy)]),
                        "intensity_bin_id": j+1,
                        "damage_bin_id": "{:.0f}".format(k), 
                        }
                    yield rec

def create_vuln_ids():
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open('./VulnerabilityLibrary/Inputs/vulnerability_dict.csv') as file:
        vuln_dict = {}
        # Read in bins data from opened file
        for line in file:
            line = line.replace('\n', '').split(',')
            # if line:
            #     int_vals_lb.append(float(line))
            #     int_vals_ub.append(float(line))
            vuln_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
    return vuln_dict

def compute_probs(location_list):
    # create_bin_dict()
    damage_bins = np.linspace(0, 1, num=100, endpoint=True)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1]).round(2)

    a=0
    b=1
    probs = {}

    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
        for i in range (len(root[0])-1):
            int_vals = []
            mean_LRs = []
            std_dev_vals = []
            # is it best to (1) compute all alpha, beta values then compute cdf iteratively or (2) find alpha and beta then compute cdf repeatedly? I say (2)
            # beta_params = []
            print ('1:', len(root[0][1][0].text.strip().split(' ')))
            for intensity, meanLR, CoV in zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' ')):
                meanLR = float (meanLR)
                intensity = float (intensity)
                CoV = float (CoV)
                int_vals.append (intensity)
                mean_LRs.append (meanLR)
                std_dev_val = (meanLR * CoV)
                std_dev_vals.append(meanLR * CoV)

            int_vs_mean = interpolate.interp1d(int_vals, mean_LRs, kind='cubic')
            int_vs_std= interpolate.interp1d(int_vals, std_dev_vals, kind='cubic')
            interp_mean = int_vs_mean (int_vals)
            interp_std_dev = int_vs_std(int_vals)
            print ('2:', len(interp_mean))
            # def damage_probs()
            damage_bin_list = np.linspace(0,1,100, endpoint=True)
            for j, (mean_val, std_dev_val) in enumerate (zip (interp_mean, interp_std_dev)):
                alpha_val = (mean_val-a)/(b-a) * ((mean_val*(1-mean_val)/std_dev_val**2) - 1)
                beta_val = alpha_val * (1-mean_val) / mean_val
                
                # beta function automatically runs has start and end interval from 0 to 1
                vals = beta.cdf(damage_bin_list, alpha_val, beta_val)
                cum_diff_vals = np.diff(vals, axis=0)
                cum_diff_vals = np.append(0, cum_diff_vals)
                #df['probabilities'] = cum_diff_vals
                for i, prob in enumerate(cum_diff_vals):
                    probs = {'probabilities': "{:.6f}".format(prob)
                        }
                    #yield from new_prob_col(j, df,cum_diff_vals)
                    yield probs

def create_damage_bins():
    damage_df = {}
    damage_bins = np.linspace(0, 1, num_damage_bins, endpoint=True)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1]).round(2)
    for i in range (1, len(damage_bins)):
        interp_val = (damage_bins[i-1] + damage_bins[i]) / 2
        interp_val = interp_val.round(3)
        damage_df = {'damage_bin_index': i, 
                     'bin_from': damage_bins[i-1], 
                     'bin_to': damage_bins[i],
                     'interpolation': interp_val
                     }
        #'probability{index_no}'.format(index_no=j): prob}
        yield damage_df

def main():
    # create_int_bins()
    directory = directories()
    location_list = directory [0]
    coverage_type = directory [1]
    vuln_dict = create_vuln_ids()
    # init_dam_dict = create_damage_bins()
    # init_dict = pd.DataFrame(init_dam_dict)
    header = create_footprint (location_list, vuln_dict)
    # df = pd.DataFrame(header)
    # header2 = compute_probs(location_list)
    # df2 = pd.DataFrame(header2)
    # df3 = pd.concat([df,df2], axis = 1)
    # df2.to_csv('./VulnerabilityLibrary/Dicts/footprint2.csv', index=False)
    # compute_probs(df, location_list)
    # print (df.head(30))
    # print (df2.head(30))
    # df2 = pd.DataFrame(damage_bin_dict)
    # df = pd.concat([df2, df])
    # df.to_csv('./VulnerabilityLibrary/Dicts/damage_bin_dict.csv', index=False)
    # df_footrpint = generate_footprint(location_list, coverage_type)
    # df = pd.DataFrame(df_footrpint)
    # df = df.head(10000)
    # df_vuln_ids = generate_vuln_ids(location_list)
    # df2 = pd.DataFrame(df_vuln_ids)
    # df2 = df2.head(10000)
    # df.to_csv('GEM/footprint.csv', index=False)
    # df2.to_csv('GEM/vuln_ids.csv', index=False)

main()

""" for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
    
        for i in range (len(root[0])-1):
            int_vals = []
            mean_LRs = []
            cov_vals = []
            for intensity, meanLR, CoV in zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' ')):
                int_vals.append (float(intensity))
                mean_LRs.append (float(meanLR))
                cov_vals.append(float(CoV))
            interp_cov_val = interpolate.interp1d(int_vals, cov_vals, kind = 'cubic') """