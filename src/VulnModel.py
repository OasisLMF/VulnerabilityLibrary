import csv
import os
import pandas as pd
import xml.etree.ElementTree as ET
import itertools
import numpy as np
from scipy import interpolate
from scipy.stats import beta
import time
import sys
import argparse
from pathlib import PurePosixPath, PurePath

# include functional test that runs on 2 files - don't test intermediary file
# separate parser (cli.py), common.py, compute_probs, then main file (manager)
parser = argparse.ArgumentParser(description = "Set up paths to receive inputs and output files to for vulnerability function data")
parser.add_argument('-W', "--working-folder", default='VulnerabilityLibrary', help = "input filename")
parser.add_argument('-d', "--num-damage-bins", default=100, type=int, help = "input number of damage bins")
parser.add_argument('-cov', "--coverage-type", default='structural', help = "input structural, contents or non_structural")
parser.add_argument('-M', "--oasis-model-folder", default='GlobalEarthquakeVariants', help = "input name of folder containing files to run oasis lmf model")
parser.add_argument('-F', "--data-folder", default="global_vulnerability_model", help = "input name of folder containing the vulnerability functions")
parser.add_argument('-c', "--continent-specific", default=False, help = "input the name of the continent you would like to run the model on")

# default paths
int_input_path = 'intensity_bins_input.csv'
int_dict_path = 'intensity_bin_dict.csv'
dam_bin_path = 'model_data/damage_bin_dict.csv'
vuln_dict_path = 'keys_data/vulnerability_dict.csv'
vulnerability_path = 'model_data/vulnerability.csv'
mapping_table_path = 'keys_data/MappingTable.csv'

# Intensity Measurement Type is root[0][i][0].attrib['imt']
# Taxonomy is root[0][i].attrib['id']pi
# Intensity is root[0][i][0].text
# Mean Loss Ratio is root[0][i][1].text
# Coefficient of Variation is root[0][i][2].text

def init_run(working_folder):
    model_path = PurePath(__file__)
    parent = model_path.parents
    for each in parent:
        if PurePosixPath(each).name == working_folder:
            path_stem = each
    return path_stem

    # abs_src_file = Path(__file__).absolute()
    # cur_dir = Path.cwd()
    # rel_src_file = str(abs_src_file.relative_to(cur_dir))
    # index_num =int(__file__.strip().split('/').index(working_folder))+1
    # source_path = __file__.strip().split('/')[index_num:]
    # if os.path.basename(cur_dir) != working_folder:
    #     print('Are you in the correct directory? Navigate to the folder named {}'.format(working_folder))
    #     sys.exit()

def init_int_bins(path_stem):
    # create three lists for lower bound, upper bound and mid point point values of intensity bins
    int_bin_vals = []
    int_bins = []
    int_mp_vals = []
    with open(PurePath.joinpath(path_stem, int_input_path)) as bins_file:
        # Read in bins data from opened file
        for i, line in enumerate (bins_file):
            line = line.strip()
            int_bin_vals.append(line)
            # condition to repeat first value
            if not(i):
                int_bin_vals.append(line)
        # repeat last intensity value
        int_bin_vals.append(int_bin_vals[-1])
        # create bins
        for i in range (len(int_bin_vals)-1):
            int_bins.append((int_bin_vals[i], int_bin_vals[i+1]))
        int_bins = np.array(int_bins, dtype = float)
        for int_bin in int_bins:
            int_mp_val = (int_bin[0] + int_bin[1]) / 2
            int_mp_vals.append(round(int_mp_val, 16))
    return int_bins, int_mp_vals

def create_int_bins(int_bins, int_mp_vals):
    # create function to find unique intensity measurement types
    int_mes_types = ['PGA','SA(0.3)', 'SA(0.6)', 'SA(1.0)']
    index_num = 1
    for type in int_mes_types:
            # Could use enumerate to label bin_index or built-in function of 'df.to_csv' 
            for int_bin, int_mp_val in zip(int_bins, int_mp_vals):
                int_bin_dict = {'bin_index': index_num,
                                'intensity_measurement_type': type,
                                'bin_from': int_bin[0], 
                                'bin_to': int_bin[1], 
                                'interpolation': int_mp_val
                                }
                index_num += 1
                yield int_bin_dict
    """ index_num = 1
    for type in int_mes_types:
        # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
        for (val_lb, val_ub) in zip (int_vals_lb, int_vals_ub):
            # int_vals_mp.append((val_lb + val_ub) / 2)
            int_val_mp = round((val_lb + val_ub)/2, 6)
            int_bin_dict = {'bin_index': index_num,
                            'intensity_measurement_type': type,
                            'bin_from': val_lb, 
                            'bin_to': val_ub, 
                            'interpolation': int_val_mp
                            }
            index_num += 1
            yield int_bin_dict """

def init_damage_bins(incr):
    damage_bins = []
    dam_bin_vals = np.arange(0, 1 + incr, incr)
    dam_bin_vals = np.append (dam_bin_vals[0], dam_bin_vals)
    dam_bin_vals = np.append(dam_bin_vals, dam_bin_vals[-1])
    # note length of damage bins is 103
    # create bins
    for i in range (len(dam_bin_vals)-1):
        damage_bins.append((dam_bin_vals[i], dam_bin_vals[i+1]))
    damage_bins = np.array(damage_bins, dtype = float)

    # rounding to 16 decimal places results in strange division when separating second element of list of arrays in main()
    dam_bin_vals = [round(dam_bin, 12) for dam_bin in dam_bin_vals]
    damage_bins_list = [dam[1] for dam in damage_bins]
    return damage_bins, damage_bins_list

def create_damage_bins(damage_bins):
    damage_bin_dict = {}
    """ for i in range (1, len(damage_bins)):
        interp_val = (damage_bins[i-1] + damage_bins[i]) / 2
        interp_val = round(interp_val, 15)
        damage_bin_dict = {'damage_bin_index': i,
                    'bin_from': "{:.6f}".format(damage_bins[i-1]),
                    'bin_to': "{:.6f}".format(damage_bins[i]),
                    'interpolation': "{:.6f}".format(interp_val)
                    }
        yield damage_bin_dict """
    index_num = 1
    for dam_bin in damage_bins:
        interp_val = (dam_bin[0] + dam_bin[1]) / 2
        interp_val = round(interp_val, 16)
        damage_bin_dict = {'bin_index': index_num,
                        'bin_from': "{:.6f}".format(dam_bin[0]),
                        'bin_to': "{:.6f}".format(dam_bin[1]), 
                        'interpolation': "{:.6f}".format(interp_val)
                        }
        index_num += 1
        yield damage_bin_dict

def directories(rootdir, coverage_type):
    location_dict = {}
    # Count included so that not all files are analysed every single run
    count = 0
    # Only looking at data for vulnerability_structural files now
    for subdir, dirs, files in os.walk(rootdir):
        dirs.sort()
        # arrange order of folders to be alphabetical in linux
        for file in files:
            # don't need to replace - can use double backslash or r'\path'
            # can use pathlib works with windows and linux: p = Path(subdir, file)
            # filepath = os.path.join(subdir, file)
            # print (filepath)
            # print(file)
            if os.path.realpath(file, strict=False).endswith("vulnerability_{}.xml".format(coverage_type)):
                # print(os.path.dirname(subdir))
                # os.path.split() splits path into head and tail where tail is last pathname component
                # country is os.path.split(subdir)
                 
                location_country = os.path.basename(subdir)
                location_continent = os.path.basename(os.path.dirname(subdir))
                filepath = os.path.join(subdir,file)
                # can call parent of path - try .parent[-1]
                # os.path and pathlib libraries good for windows and linux
                # location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split(' ')
                # location[0] represents a continent 
                # location[1] represents a country
                location_dict [location_continent, location_country] = filepath
                count += 1
        # if count > 0:
        #      break
    return location_dict

def create_vuln_ids(location_list, mapping_table_df):
    vuln_ids = {}
    cur_vuln_id = 1
    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()

        for i in range (1, len(root[0])):
            # Intensity measurement type not used in vulnerability dictionary 
            # It is characterised by the intensity bin indexes which range from 1-20 (PGA), 21-40 (SA 0.3), 41-60 (SA 0.6), 61-80 (SA 1.0)
            taxonomy = root[0][i].attrib['id']
            if (continent, country, taxonomy) not in vuln_ids:
                vuln_ids[(continent, country, taxonomy)] = cur_vuln_id
                cur_vuln_id += 1
                tax = taxonomy.replace("[","").replace("'","").replace("]","").split('/')

                for j in range(len(mapping_table_df)):
                    if tax[0] == mapping_table_df[j][0]:                                                      
                        attribute2_id = mapping_table_df[j][2]
                        break
                for j in range(len(mapping_table_df)):
                    if tax[-1] == mapping_table_df[j][0]:                                                     
                        attribute6_id = mapping_table_df[j][2]
                        break
            
                if tax[1][0] == 'H':                                                                        
                    no_storeys = int(tax[1].replace("H",""))
                elif tax[2][0] == 'H':                                                                        
                    no_storeys = int(tax[2].replace("H",""))
                elif tax[3][0] == 'H':
                    no_storeys = int(tax[3].replace("H",""))

                id_dict = {
                    "vulnerability_id": vuln_ids[(continent, country, taxonomy)],
                    "Peril ID": 'QEQ',
                    "Continent": continent,
                    "Country": country,
                    "Taxonomy": taxonomy,
                    "nodeID": i,
                    "attribute2_id": attribute2_id,
                    "attribute4_id": no_storeys,
                    "attribute6_id": attribute6_id
                }
            yield id_dict

    """ input_bins_path = PurePath.joinpath(path_stem, int_input_path)
    # bins_df = pd.read_csv(input_bins_path)
    # pd.read_csv(path, names=['bin_val'])
    # for rec in bins_df.to_dict(orient='records'):
    with open(input_bins_path) as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        int_vals_mp = []
        # Read in bins data from opened file
        for line in bins_file:
            # line = line.replace('\n', '')
            line = line.strip()
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
        for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
            int_vals_mp.append((val_lb + val_ub) / 2)
    int_bin_vals = int_vals_lb[1:]
    int_vals_mp = int_vals_mp[1:-1]
    return int_bin_vals, int_vals_mp """

def get_bin_index(int_bins, int_mp_vals, int_mes_type):
    bin_indices_list = []
    num_intensity_bins = len(int_bins)
    for int_mp_val in int_mp_vals:
        for i in range(len(int_bins)):
            # edge case: if first bin is below or equal to zero
            if (not(i) and (int_bins[i][0] == int_mp_val)) or int_mp_val < int_bins[0][0]:
                bin_index = 1
                break
            elif int_mp_val >= int_bins[-1][0]:
                bin_index = len(int_bins)
                break
            elif int_bins[i][0] <= int_mp_val < int_bins[i][1]:
                bin_index = i+2
        if int_mes_type == 'SA(0.3)':
            bin_index += num_intensity_bins 
        elif int_mes_type == 'SA(0.6)':
            bin_index += 2*num_intensity_bins
        elif int_mes_type == 'SA(1.0)':
            bin_index += 3*num_intensity_bins
        bin_indices_list.append(bin_index)
    return bin_indices_list

def get_vuln_ids(vuln_ids_path):
    header = True
    vuln_dict = {}
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open(vuln_ids_path) as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                vuln_id = data.index('vulnerability_id')
                vuln_continent = data.index('Continent')
                vuln_country = data.index('Country')
                vuln_taxonomy = data.index('Taxonomy')
                vuln_node_id = data.index('nodeID')
                header = False
            else:
                vuln_dict[(data[vuln_continent], data[vuln_country], data[vuln_taxonomy], data[vuln_node_id])] = data[vuln_id]
                # vuln_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
    return vuln_dict

def create_vuln_dict(val_id, bin_indices_list, damage_bins):
    # for j, (intensity, MLR, coeff_var) in enumerate (zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' '))):
    # wrong - for j, each in enumerate(root[0][i+1][0].text.strip().split(' ')):
    for bin_index in bin_indices_list:
        # length of damage bins is 103 - strange- restructuure code so using proper bins and not a list
        for k in range(len(damage_bins)):
            # val_id = vuln_dict.get((attribute_2, peril_id, coverage_type, attribute_4, attribute_6), "None: {something}".format(something = (attribute_2, peril_id, coverage_type, attribute_4, attribute_6)))
            rec = {
                "vulnerability_id": val_id,
                "intensity_bin_id": bin_index,
                "damage_bin_id": "{:.0f}".format(k+1),
                }
            yield rec

# unit test with doc test
def compute_probs(int_vals, int_mp_vals, mean_LRs, std_devs, damage_bins_list):
    a=0
    b=1
    probs = {}
    # better to use cubic or linear interpolation?
    # note: cubic has slower run time than linear
    # could use numpy interp - may be the better option
    # bounds error is used when int_mp_vals (intensity bins) are above or below the range of intensity values in a particular 
    int_vs_mean = interpolate.interp1d(int_vals, mean_LRs, kind='linear', bounds_error=False, fill_value= (mean_LRs[0], mean_LRs[-1]))
    int_vs_std = interpolate.interp1d(int_vals, std_devs, kind='linear', bounds_error=False, fill_value= (std_devs[0], std_devs[-1]))
    # function that can take in list of values to be interpolated has been created above using scipy  
    interp_mean = int_vs_mean (int_mp_vals)
    interp_std_dev = int_vs_std (int_mp_vals)
    for j, (mean_val, std_dev_val) in enumerate (zip(interp_mean, interp_std_dev)):
        # Beta paramters for last few bins are very large, indicating dirac delta function shape which makes sense
        # High intensity leads to high damage almost 100% of the time
        # zero mean, and small std deviation => beta value 1000x order of magnitude of alpha => J shaped distribution
        # print (mean_val, std_dev_val)
        alpha_val = (mean_val-a)/(b-a) * ((mean_val*(1-mean_val)/std_dev_val**2) - 1)
        beta_val = alpha_val * (1-mean_val) / mean_val
        # print (alpha_val, beta_val)
        # beta function automatically runs has start and end interval from 0 to 1
        prob_vals = beta.cdf(damage_bins_list, alpha_val, beta_val)            
        # first damage bin has proabbility zero since it is a single value not a range
                # Proability for last damage bin that only represents single damage value - not a range
        cum_diff_vals = list(np.diff(prob_vals, axis=0, prepend=0))
        #cum_diff_vals = np.append(cum_diff_vals, 0)
        # damage_bins_list[0] == 0 should always be true so line below really only checks that this is first intensity bin index
        if not (j) and damage_bins_list[0] == 0:
            cum_diff_vals = np.append(1, np.zeros(len(cum_diff_vals)-1))
        # if j == 10:
        #     print (cum_diff_vals, len(cum_diff_vals))
        for prob in cum_diff_vals:
            probs = {'probabilities': "{:.6f}".format(prob)
                }
            yield probs

def main(working_folder, num_damage_bins, coverage_type, oasis_model_folder, data_folder, continent_specific):
    incr = 1/num_damage_bins
    path_stem = init_run(working_folder)

    int_bins, int_mp_vals = init_int_bins(path_stem)
    int_bins_dict = create_int_bins(int_bins, int_mp_vals)

    int_bins_df = pd.DataFrame(int_bins_dict)
    int_bin_dict_path = PurePath.joinpath(path_stem, int_dict_path)
    int_bins_df.to_csv(int_bin_dict_path, index=False)
    
    # damage bin list does not have repeated start or end - length 102
    damage_bins, damage_bins_list = init_damage_bins(incr)
    dam_bins_dict = create_damage_bins(damage_bins)

    damage_bins_df = pd.DataFrame(dam_bins_dict)
    damage_bins_path = PurePath.joinpath(path_stem, oasis_model_folder, dam_bin_path)
    damage_bins_df.to_csv(damage_bins_path, index=False)

    rootdir = PurePath.joinpath(path_stem, data_folder)
    location_list = directories(rootdir, coverage_type)

    mapping_path = PurePath.joinpath(path_stem, oasis_model_folder, mapping_table_path)
    mapping_table_df = pd.DataFrame(pd.read_csv(mapping_path)).to_numpy()

    vuln_ids = create_vuln_ids(location_list, mapping_table_df)
    vulns_df = pd.DataFrame(vuln_ids)
    vuln_ids_path = PurePath.joinpath(path_stem, oasis_model_folder, vuln_dict_path)
    vulns_df.to_csv(vuln_ids_path, index=False)
    vuln_dict = get_vuln_ids(vuln_ids_path)

    with open (PurePath.joinpath(path_stem, oasis_model_folder, vulnerability_path), 'wb') as dict_file:
        header = True
        current_location = ('','')
        t_zero =  time.time()
        for j, (continent, country, taxonomy, nodeID) in enumerate(vuln_dict):
            if continent_specific == False or continent == continent_specific:
                if (continent, country) != current_location:
                    current_location = (continent, country)
                    pathway = location_list[(continent, country)]
                    tree = ET.parse(pathway)
                    root = tree.getroot()

                # nodeID easier to understand than using num = j-1 and i = j-m
                print ('running', continent, country, ': ', nodeID, '/', len(root[0])-1)
                int_mes_type = root[0][int(nodeID)][0].attrib['imt']
                # empty space is default value for split - read documenation
                # map is generator - used to replace entire function to convert list of strings to floats
                int_vals = list(map(float,root[0][int(nodeID)][0].text.split()))
                mean_LRs = list(map(float,root[0][int(nodeID)][1].text.split()))
                coeff_vars = list(map(float,root[0][int(nodeID)][2].text.split()))
                std_devs = [mean_LR*coeff_var for mean_LR, coeff_var in zip(mean_LRs, coeff_vars)]
                
                # int_vals, mean_LRs, std_dev_list, int_bin_list, int_mp_list = float_conversion(int_vals, mean_LRs, coeff_vars, int_bin_vals, int_mp_vals)
                # bin_indices_list = get_bin_index(int_vals, int_mp_vals, int_bins, int_mes_type)
                bin_indices_list = get_bin_index(int_bins, int_mp_vals, int_mes_type)
                val_id = vuln_dict[(continent, country, taxonomy, nodeID)]
                vuln_data = create_vuln_dict (val_id, bin_indices_list, damage_bins)
                # print (int_bin_vals, int_mp_vals)
                vuln_temp_df = pd.DataFrame(vuln_data)
                probs_data = compute_probs(int_vals, int_mp_vals, mean_LRs, std_devs, damage_bins_list)
                probs_df = pd.DataFrame(probs_data)
                vuln_df = pd.concat([vuln_temp_df, probs_df], axis = 1)        
                # table = pa.Table.from_pandas(vuln_df)
                # pq.write_table(table, dict_file)
                vuln_df.to_csv(dict_file, index=False, header=header)       
                header = False
        print(time.time() - t_zero)

kwargs = vars(parser.parse_args())
# one star for list and double star for expanding dictionary
main(**kwargs)

# compress as parquet
