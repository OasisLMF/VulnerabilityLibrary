import csv
import os
import pandas as pd
import xml.etree.ElementTree as ET
import itertools
import numpy as np
from scipy import interpolate
from scipy.stats import beta

# Do I have enough information to create a footprint file

# set number of damage bins
num_damage_bins = 50

# Intensity Type is root[0][i][0].attrib['imt']
# Taxonomy is root[0][i].attrib['id']
# Intensity is root[0][i][0].text
# Mean Loss Ratio is root[0][i][1].text
# Coefficient of Variation is root[0][i][2].text

def create_int_bins():
    # Open the 'intensity_bins.txt' file in the 'Inputs' folder where user manually inputs the intensity bins
    with open('./VulnerabilityLibrary/Inputs/intensity_bins.txt') as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        int_vals_mp = []
        # Read in bins data from opened file
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
        for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
            int_vals_mp.append((val_lb + val_ub) / 2)
            int_bin_dict = {'bin_from': int_vals_lb, 
                            'bin_to': int_vals_ub, 
                            'interpolation': int_vals_mp
                            }
    # Format data into a table and output csv file
    intensity_df = pd.DataFrame(int_bin_dict)
    intensity_df.to_csv('./VulnerabilityLibrary/Dicts/intensity_bin_dict.csv', index=True, index_label='bin_index')

def create_damage_bins():
    damage_df = {}
    damage_bins = np.linspace(0, 1, num_damage_bins, endpoint=True)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1])
    # damage_bins = np.append(damage_bins, damage_bins[-1]).round(2)
    for i in range (1, len(damage_bins)):
        interp_val = (damage_bins[i-1] + damage_bins[i]) / 2
        # interp_val = interp_val.round(3)
        dam_bin_dict = {'damage_bin_index': "{:.0f}".format(i),
                     'bin_from': "{:.6f}".format(damage_bins[i-1]),
                     'bin_to': "{:.6f}".format(damage_bins[i]),
                     'interpolation': "{:.6f}".format(interp_val)
                     }
    damage_df = pd.DataFrame(dam_bin_dict, index = [0])
    damage_df.to_csv('./VulnerabilityLibrary/Dicts/damage_bin_dict.csv', index=True, index_label='bin_index')

def directories():
    rootdir = "./global_vulnerability_model"
    location_dict = {}
    # Count included so that not all files are analysed every single run
    count = 0
    coverage_types = ['structural', 'nonstructural', 'contents']
    # Only looking at data for vulnerability_structural files now
    coverage_type = coverage_types [0]
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            subdir = subdir.replace('\\', '/')
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_{cov_type}.xml".format(cov_type = coverage_type)):
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                # location[0] represents a continent 
                # location[1] represents a country
                location_dict [location[0], location[1]] = filepath
                count += 1
            if count < 1:
                break
    return location_dict, coverage_type

def get_vuln_ids():
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open('./VulnerabilityLibrary/Inputs/vulnerability_dict.csv') as file:
        vuln_dict = {}
        # Read in bins data from opened file
        for line in file:
            line = line.replace('\n', '').split(',')
            # if line:
            #     int_vals_lb.append(float(line))
            #     int_vals_ub.append(float(line))
            vuln_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
    return vuln_dict

def get_height_codes():
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open('./VulnerabilityLibrary/Inputs/height_dict.csv') as file:
        height_dict = {}
        # Read in bins data from opened file
        for line in file:
            line = line.replace('\n', '').split(',')
            # height_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
            height_dict[line[1]] = line[2]
    return height_dict

def get_constr_codes():
    with open('./VulnerabilityLibrary/Inputs/construction_dict.csv') as file:
        constr_dict = {}
        # Read in bins data from opened file
        for line in file:
            line = line.replace('\n', '').split(',')
            constr_dict[line[3]] = line[4]
    return constr_dict

def get_occup_codes():
    occup_dict = {'RES': 1, 'COM': 2, 'IND': 3}
    return occup_dict

def get_cov_type_codes():
    cov_type_dict = {'structural': 1, 'nonstructural': 2, 'contents': 3}
    return cov_type_dict

def create_footprint(location_list, vuln_dict, height_dict, constr_dict, occup_dict, cov_type_dict,coverage_type):
    peril_id = 'QEQ'
    coverage_type = str(cov_type_dict[coverage_type])
    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
        for i in range (len(root[0])-1):
            taxonomy = root[0][i+1].attrib['id'] 
            taxonomy = taxonomy.strip().split('/')
            attribute_2 = constr_dict[taxonomy[0]]
            attribute_6 = str(occup_dict[taxonomy[-1]])
            if taxonomy[-2][0] == 'H':
                attribute_4 = height_dict[taxonomy[-2]]
            elif taxonomy[-3][0] == 'H':
                attribute_4 = height_dict[taxonomy[-3]]
            # for j, (intensity, MLR, CoV) in enumerate (zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' '))):
            # wrong - for j, each in enumerate(root[0][i+1][0].text.strip().split(' ')):
            for j in range (22):
                for k in range(52):
                    val_id = vuln_dict.get((attribute_2, peril_id, coverage_type, attribute_4, attribute_6), "None: {something}".format(something = (attribute_2, peril_id, coverage_type, attribute_4, attribute_6)))
                    rec = {
                        # converted valuation_id to string so that an integer would be displayed
                        "vulnerability_id":  val_id,
                        "intensity_bin_id": j+1,
                        "damage_bin_id": "{:.0f}".format(k), 
                        }
                    yield rec

def compute_probs(location_list):
    # create_bin_dict()
    damage_bins = np.linspace(0, 1, num=100, endpoint=True)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1]).round(2)

    a=0
    b=1
    probs = {}

    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
        for i in range (len(root[0])-1):
            int_vals = []
            mean_LRs = []
            std_dev_vals = []
            # is it best to (1) compute all alpha, beta values then compute cdf iteratively or (2) find alpha and beta then compute cdf repeatedly? I say (2)
            # beta_params = []
            print ('1:', len(root[0][1][0].text.strip().split(' ')))
            for intensity, meanLR, CoV in zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' ')):
                meanLR = float (meanLR)
                intensity = float (intensity)
                CoV = float (CoV)
                int_vals.append (intensity)
                mean_LRs.append (meanLR)
                std_dev_val = (meanLR * CoV)
                std_dev_vals.append(meanLR * CoV)

            int_vs_mean = interpolate.interp1d(int_vals, mean_LRs, kind='cubic')
            int_vs_std= interpolate.interp1d(int_vals, std_dev_vals, kind='cubic')
            interp_mean = int_vs_mean (int_vals)
            interp_std_dev = int_vs_std(int_vals)
            print ('2:', len(interp_mean))
            # def damage_probs()
            damage_bin_list = np.linspace(0,1,100, endpoint=True)
            for j, (mean_val, std_dev_val) in enumerate (zip (interp_mean, interp_std_dev)):
                alpha_val = (mean_val-a)/(b-a) * ((mean_val*(1-mean_val)/std_dev_val**2) - 1)
                beta_val = alpha_val * (1-mean_val) / mean_val
                
                # beta function automatically runs has start and end interval from 0 to 1
                vals = beta.cdf(damage_bin_list, alpha_val, beta_val)
                cum_diff_vals = np.diff(vals, axis=0)
                cum_diff_vals = np.append(0, cum_diff_vals)
                #df['probabilities'] = cum_diff_vals
                for i, prob in enumerate(cum_diff_vals):
                    probs = {'probabilities': "{:.6f}".format(prob)
                        }
                    #yield from new_prob_col(j, df,cum_diff_vals)
                    yield probs


def main():
    # init_int_bins = create_int_bins()
    # init_dam_bins = create_damage_bins()

    directory = directories()
    location_list = directory [0]
    coverage_type = directory [1]
    vuln_dict = get_vuln_ids()
    height_dict = get_height_codes()
    constr_dict = get_constr_codes()
    occup_dict = get_occup_codes()
    cov_type_dict = get_cov_type_codes()

    header = create_footprint (location_list, vuln_dict, height_dict, constr_dict, occup_dict, cov_type_dict, coverage_type)
    df = pd.DataFrame(header)
    df.to_csv('./VulnerabilityLibrary/Dicts/vulnerability_dict.csv', index=False)

    # header2 = compute_probs(location_list)
    # df2 = pd.DataFrame(header2)
    # df3 = pd.concat([df,df2], axis = 1)
    # df3.to_csv('./VulnerabilityLibrary/Dicts/footprint2.csv', index=False)
    
    # compute_probs(df, location_list)
    # print (df.head(30))
    # print (df2.head(30))
    # df2 = pd.DataFrame(damage_bin_dict)
    # df = pd.concat([df2, df])
    # df.to_csv('./VulnerabilityLibrary/Dicts/damage_bin_dict.csv', index=False)
    # df_footrpint = generate_footprint(location_list, coverage_type)
    # df = pd.DataFrame(df_footrpint)
    # df = df.head(10000)
    # df_vuln_ids = generate_vuln_ids(location_list)
    # df2 = pd.DataFrame(df_vuln_ids)
    # df2 = df2.head(10000)
    # df.to_csv('GEM/footprint.csv', index=False)
    # df2.to_csv('GEM/vuln_ids.csv', index=False)

main()
