import csv
import os
import pandas as pd
import xml.etree.ElementTree as ET
import itertools
import numpy as np
from scipy import interpolate
from scipy.stats import beta
import time
import sys
import argparse
from pathlib import PurePosixPath, PurePath

# include functional test that runs on 2 files - don't test intermediary file
# separate parser (cli.py), common.py, compute_probs, then main file (manager)
parser = argparse.ArgumentParser(description = "Set up paths to receive inputs and output files to for vulnerability function data")
parser.add_argument('-w', "--working-folder", default='VulnerabilityLibrary', help = "input filename")
parser.add_argument('-D', "--num-damage-bins", default=100, type=int, help = "input number of damage bins")
parser.add_argument('-C', "--coverage-type", default='structural', help = "input structural, contents or non_structural")
parser.add_argument('-M', "--oasis-model-folder", default='GlobalEarthquakeVariants', help = "input name of folder containing files to run oasis lmf model")
parser.add_argument('-F', "--data-folder", default="global_vulnerability_model", help = "input name of folder containing the vulnerability functions")
parser.add_argument('-CONT', "--continent-specific", default=None, type=str, help = "input the name of the continent you would like to run the model on")
parser.add_argument('-COUNT', "--country-specific", default=None, help = "input the name of the country you would like to run the model on")



# default paths
int_input_path = 'intensity_bins_input.csv'
int_dict_path = 'intensity_bin_dict.csv'
dam_bin_path = 'model_data/damage_bin_dict.csv'
vuln_dict_path = 'keys_data/vulnerability_dict.csv'
vulnerability_path = 'model_data/vulnerability.csv'
# vulnerability_path = PurePath('GlobalEarthquakeVariants/model_data/vulnerability.csv')

# Intensity Type is root[0][i][0].attrib['imt']
# Taxonomy is root[0][i].attrib['id']pi
# Intensity is root[0][i][0].text
# Mean Loss Ratio is root[0][i][1].text
# Coefficient of Variation is root[0][i][2].text

def init_run(working_folder):
    # abs_src_file = Path(__file__).absolute()
    # cur_dir = Path.cwd()
    # rel_src_file = str(abs_src_file.relative_to(cur_dir))
    model_path = PurePath(__file__)
    parent = model_path.parents
    for each in parent:
        if PurePosixPath(each).name == working_folder:
            stem = each
    # index_num =int(__file__.strip().split('/').index(working_folder))+1
    # source_path = __file__.strip().split('/')[index_num:]
    return stem
    # if os.path.basename(cur_dir) != working_folder:
    #     print('Are you in the correct directory? Navigate to the folder named {}'.format(working_folder))
    #     sys.exit()

def create_int_bins(path_stem):
    # create function to find unique intensity measurement types
    int_mes_types = ['PGA','SA(0.3)', 'SA(0.6)', 'SA(1.0)']
    # Open the 'intensity_bins.json' file in the 'Inputs' folder where user manually inputs the intensity bins
    with open(PurePath.joinpath(path_stem, int_input_path)) as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        # Read in bins data from opened file
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        index_num = 1
        for type in int_mes_types:
            # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
            for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
                # int_vals_mp.append((val_lb + val_ub) / 2)
                int_val_mp = (val_lb + val_ub) / 2
                int_bin_dict = {'bin_index': index_num,
                                'intensity_measurement_type': type,
                                'bin_from': val_lb, 
                                'bin_to': val_ub, 
                                'interpolation': int_val_mp
                                }
                index_num += 1
                yield int_bin_dict

def create_damage_bins(damage_bins):
    damage_bin_dict = {}
    # damage_bins = np.append(damage_bins, damage_bins[-1]).round(2)
    for i in range (1, len(damage_bins)):
        interp_val = (damage_bins[i-1] + damage_bins[i]) / 2
        interp_val = round(interp_val,3)
        damage_bin_dict = {'damage_bin_index': i,
                    'bin_from': "{:.6f}".format(damage_bins[i-1]),
                    'bin_to': "{:.6f}".format(damage_bins[i]),
                    'interpolation': "{:.6f}".format(interp_val)
                    }
        yield damage_bin_dict

def directories(path_stem, coverage_type, data_folder):
    
    # Use argparse to write this line
    rootdir = PurePath.joinpath(path_stem, data_folder)
    # print (rootdir)
    location_dict = {}
    # Count included so that not all files are analysed every single run
    count = 0
    # Only looking at data for vulnerability_structural files now
    for subdir, dirs, files in os.walk(rootdir):
        dirs.sort()
        # arrange order of folders to be alphabetical in linux
        for file in files:
            # don't need to replace - can use double backslash or r'\path'
            # can use pathlib works with windows and linux: p = Path(subdir, file)
            # filepath = os.path.join(subdir, file)
            # print (filepath)
            # print(file)
            if os.path.realpath(file, strict=False).endswith("vulnerability_{}.xml".format(coverage_type)):
                # print(os.path.dirname(subdir))
                # os.path.split() splits path into head and tail where tail is last pathname component
                # country is os.path.split(subdir)
                 
                location_country = os.path.basename(subdir)
                location_continent = os.path.basename(os.path.dirname(subdir))
                filepath = os.path.join(subdir,file)
                # can call parent of path - try .parent[-1]
                # os.path and pathlib libraries good for windows and linux
                # location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split(' ')
                # location[0] represents a continent 
                # location[1] represents a country
                location_dict [location_continent, location_country] = filepath
                count += 1
        if count > 0:
             break
    return location_dict

def create_vuln_ids(location_list):

    csvFile2 = pd.read_csv('VulnerabilityLibrary/GlobalEarthquakeVariants/keys_data/MappingTable.csv')  
    mapping_table = pd.DataFrame(csvFile2).to_numpy()

    vuln_ids = {}
    cur_vuln_id = 1
    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()

        for i in range (1, len(root[0])):
            # Intensity measurement type not used in vulnerability dictionary 
            # It is characterised by the intensity bin indexes which range from 1-20 (PGA), 21-40 (SA 0.3), 41-60 (SA 0.6), 61-80 (SA 1.0)
            int_mes_type = root[0][i][0].attrib['imt']
            taxonomy = root[0][i].attrib['id']
            if (continent, country, taxonomy) not in vuln_ids:
                vuln_ids[(continent, country, taxonomy)] = cur_vuln_id
                cur_vuln_id += 1
                
                tax = taxonomy.replace("[","").replace("'","").replace("]","").split('/')

                for j in range(len(mapping_table)):
                    if tax[0] == mapping_table[j][0]:                                                      
                        attribute2_id = mapping_table[j][2]
                        break
            
                for j in range(len(mapping_table)):
                    if tax[-1] == mapping_table[j][0]:                                                     
                        attribute6_id = mapping_table[j][2]
                        break
            
                if tax[1][0] == 'H':                                                                        
                    no_storeys = int(tax[1].replace("H",""))
                elif tax[2][0] == 'H':                                                                        
                    no_storeys = int(tax[2].replace("H",""))
                elif tax[3][0] == 'H':
                    no_storeys = int(tax[3].replace("H",""))

                id_dict = {
                    "vulnerability_id": vuln_ids[(continent, country, taxonomy)],
                    "Peril ID": 'QEQ',
                    "Continent": continent,
                    "Country": country,
                    "Taxonomy": taxonomy,
                    "nodeID": i,
                    "attribute2_id": attribute2_id,
                    "attribute4_id": no_storeys,
                    "attribute6_id": attribute6_id
                }
            yield id_dict

def find_nearest(array, value):
    array = np.asarray(array)
    idx = (np.abs(array - value)).argmin()
    yield idx

def create_bin_index(path_stem):
    input_bins_path = PurePath.joinpath(path_stem, int_input_path)
    # bins_df = pd.read_csv(input_bins_path)
    # pd.read_csv(path, names=['bin_val'])
    # for rec in bins_df.to_dict(orient='records'):
    with open(input_bins_path) as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        int_vals_mp = []
        # Read in bins data from opened file
        for line in bins_file:
            # line = line.replace('\n', '')
            line = line.strip()
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
        for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
            int_vals_mp.append((val_lb + val_ub) / 2)
    int_bin_vals = int_vals_lb[1:]
    int_vals_mp = int_vals_mp[1:-1]
    return int_bin_vals, int_vals_mp

def get_bin_index(int_vals, int_mp_vals, int_bin_vals, int_mes_type):
    bin_indices_list = []
    num_intensity_bins = len(int_mp_vals)
    for j, int_mp_val in enumerate(int_mp_vals):
        bin_index = list(find_nearest(int_bin_vals, int_mp_val))[0]
        # add 1 to bin index since find_nearest outputs 0 for first bin and add 1 again since first intensity bin is for value 0
        bin_index += 2
        # Check that intensity values are within the bins created by the user
        if float(int_mp_val) >= min(int_bin_vals) and float (int_mp_val) <= max(int_bin_vals):
            pass
        else:
            print('Intensity value of', int_mp_val, 'is not within range of intensity bins inputed')
            sys.exit()
        if int_mes_type == 'SA(0.3)':
            bin_index += (num_intensity_bins + 2)
        elif int_mes_type == 'SA(0.6)':
            bin_index += 2*(num_intensity_bins + 2)
        elif int_mes_type == 'SA(1.0)':
            bin_index += 3*(num_intensity_bins + 2)
        bin_indices_list.append(bin_index)
    return bin_indices_list

def get_vuln_ids(vuln_ids_path):
    header = True
    vuln_dict = {}
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open(vuln_ids_path) as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                vuln_id = data.index('vulnerability_id')
                vuln_continent = data.index('Continent')
                vuln_country = data.index('Country')
                vuln_taxonomy = data.index('Taxonomy')
                vuln_node_id = data.index('nodeID')
                header = False
            else:
                vuln_dict[(data[vuln_continent], data[vuln_country], data[vuln_taxonomy], data[vuln_node_id])] = data[vuln_id]
                # vuln_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
    return vuln_dict

def create_vuln_dict(val_id, bin_indices_list, damage_bins):
    # Is this okay- or could it be something other than 'QEQ'?
    # I don't think Peril ID is needed here
    # peril_id = 'QEQ'
    # int_vals = int_vals.strip().split(' ')
    # coverage_type = str(cov_type_dict[coverage_type])
    
    # attribute_2 = constr_dict[taxonomy[0]]
    # attribute_6 = str(occup_dict[taxonomy[-1]])
    # Make this more general
    # if taxonomy[-2][0] == 'H':
    #     attribute_4 = height_dict[taxonomy[-2]]
    # elif taxonomy[-3][0] == 'H':
    #     attribute_4 = height_dict[taxonomy[-3]]
    # for j, (intensity, MLR, coeff_var) in enumerate (zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' '))):
    # wrong - for j, each in enumerate(root[0][i+1][0].text.strip().split(' ')):
    for j, bin_index in enumerate(bin_indices_list):
        for bin in damage_bins[:-1]:
            # val_id = vuln_dict.get((attribute_2, peril_id, coverage_type, attribute_4, attribute_6), "None: {something}".format(something = (attribute_2, peril_id, coverage_type, attribute_4, attribute_6)))
            rec = {
                "vulnerability_id":  val_id,
                # "intensity_bin_number": j+1,
                "intensity_bin_id": int(bin_index),
                "damage_bin_id": "{:.0f}".format(bin), 
                }
            yield rec

# unit test with doc test
def compute_probs(int_vals_list, int_mp_list, mean_LRs_list, std_dev_list, damage_bins):
    a=0
    b=1
    probs = {}
    # better to use cubic or linear interpolation?
    # note: cubic has slower run time than linear
    int_vs_mean = interpolate.interp1d(int_vals_list, mean_LRs_list, kind='linear')
    int_vs_std= interpolate.interp1d(int_vals_list, std_dev_list, kind='linear')
    # function that can take in list of values to be interpolated has been created above using scipy 
    interp_mean = int_vs_mean (int_mp_list)
    interp_std_dev = int_vs_std (int_mp_list)   
    for j, (mean_val, std_dev_val) in enumerate (zip (interp_mean, interp_std_dev)):
        # Beta paramters for last few bins are very large, indicating dirac delta function shape which makes sense
        # High intensity leads to high damage almost 100% of the time
        alpha_val = (mean_val-a)/(b-a) * ((mean_val*(1-mean_val)/std_dev_val**2) - 1)
        beta_val = alpha_val * (1-mean_val) / mean_val
        # beta function automatically runs has start and end interval from 0 to 1
        vals = beta.cdf(damage_bins[:-1], alpha_val, beta_val)
        vals_temp = np.append(0, vals)
        cum_diff_vals = list(np.diff(vals_temp, axis=0))
        for i, prob in enumerate(cum_diff_vals):
            probs = {'probabilities': "{:.6f}".format(prob)
                }
            yield probs

def float_conversion(int_vals, mean_LRs, coeff_vars, int_bin_vals, int_mp_vals):
    int_vals_list = []
    mean_LRs_list = []
    std_dev_list = []
    int_bin_list = []
    int_mp_list = []
    for int_mp_val in int_mp_vals:
        int_mp_val = float (int_mp_val)
        int_mp_list.append(int_mp_val)
    for int_bin_val in int_bin_vals:
        int_bin_val = float (int_bin_val)
        int_bin_list.append(int_bin_val)
    # is it best to (1) compute all alpha, beta values then compute cdf iteratively or (2) find alpha and beta then compute cdf repeatedly? I say (2)
    for int_val, mean_LR, coeff_var in zip(int_vals, mean_LRs, coeff_vars):
        int_val = float (int_val)
        mean_LR = float (mean_LR)
        coeff_var = float (coeff_var)
        int_vals_list.append(int_val)
        mean_LRs_list.append(mean_LR)
        # rounded to 18 digits to avoid bit error - neccessary?
        std_dev_val = round((mean_LR * coeff_var), 18)
        # int_vals.append (intensity)
        # mean_LRs.append (meanLR)
        std_dev_list.append(std_dev_val)
    return int_vals_list, mean_LRs_list, std_dev_list, int_bin_list, int_mp_list

def main(working_folder, num_damage_bins, coverage_type, oasis_model_folder, data_folder, continent_specific, country_specific):
    incr = 1/num_damage_bins
    damage_bins = np.arange(0, 1 + incr, incr)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1])
    path_stem = init_run(working_folder)
    # source_path = 'src/VulnModel.py'
    # Print conutry continent and number you're at out of how many
    init_int_bins = create_int_bins(path_stem)
    # Format data into a table and output csv file
    intensity_df = pd.DataFrame(init_int_bins)
    # Use argparse to write this line
    intensity_path = PurePath.joinpath(path_stem, int_dict_path)
    intensity_df.to_csv(intensity_path, index=False)
    
    init_dam_bins = create_damage_bins(damage_bins)
    damage_df = pd.DataFrame(init_dam_bins)
    damage_path = PurePath.joinpath(path_stem, oasis_model_folder, dam_bin_path)
    damage_df.to_csv(damage_path, index=False)

    location_list = directories(path_stem, coverage_type, data_folder)

    vuln_ids = create_vuln_ids(location_list)
    vulns_df = pd.DataFrame(vuln_ids)
    vuln_ids_path = PurePath.joinpath(path_stem, oasis_model_folder, vuln_dict_path)
    vulns_df.to_csv(vuln_ids_path, index=False)

    vuln_dict = get_vuln_ids(vuln_ids_path)

    int_bin_vals, int_mp_vals = create_bin_index(path_stem)

    with open (PurePath.joinpath(path_stem, oasis_model_folder, vulnerability_path), 'wb') as dict_file:
        header = True
        current_location = ('','')
        t_zero =  time.time()
        for j, (continent, country, taxonomy, nodeID) in enumerate(vuln_dict):
            # if country_specific == None:
            #     country_specific = country
            # if continent_specific == None:
            #     continent_specific = continent
            # if country_specific and country == country_specific:
            if (continent, country) != current_location:
                current_location = (continent, country)
                pathway = location_list[(continent, country)]
                tree = ET.parse(pathway)
                root = tree.getroot()
            # nodeID easier to understand than using num = j-1 and i = j-m
            print ('running', continent, country, ': ', nodeID, '/', len(root[0])-1)
            int_mes_type = root[0][int(nodeID)][0].attrib['imt']
            # empty space is default value for split - read documenation
            # map is generator - used to replace entire function to convert list of strings to floats
            int_vals_list = list(map(float,root[0][int(nodeID)][0].text.split()))
            mean_LRs_list = list(map(float,root[0][int(nodeID)][1].text.split()))
            coeff_vars = list(map(float,root[0][int(nodeID)][2].text.split()))
            std_dev_list = [mean_LR*coeff_var for mean_LR, coeff_var in zip(mean_LRs_list, coeff_vars)]
            
            # int_vals_list, mean_LRs_list, std_dev_list, int_bin_list, int_mp_list = float_conversion(int_vals, mean_LRs, coeff_vars, int_bin_vals, int_mp_vals)
            bin_indices_list = get_bin_index(int_vals_list, int_mp_vals, int_bin_vals, int_mes_type)
            val_id = vuln_dict[(continent, country, taxonomy, nodeID)]
            vuln_data = create_vuln_dict (val_id, bin_indices_list, damage_bins)
            # print (int_bin_vals, int_mp_vals)
            vuln_temp_df = pd.DataFrame(vuln_data)
            probs_data = compute_probs(int_vals_list, int_mp_vals, mean_LRs_list, std_dev_list, damage_bins)
            probs_df = pd.DataFrame(probs_data)
            vuln_df = pd.concat([vuln_temp_df, probs_df], axis = 1)            
            # table = pa.Table.from_pandas(vuln_df)
            # pq.write_table(table, dict_file)
            vuln_df.to_csv(dict_file, index=False, header=header)            
            header = False
        print(time.time() - t_zero)

kwargs = vars(parser.parse_args())
# one star for list and double star for expanding dictionary
main(**kwargs)

# compress as parquet
