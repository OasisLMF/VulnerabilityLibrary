import csv
import os
import pandas as pd
import xml.etree.ElementTree as ET
import itertools
import numpy as np
from scipy import interpolate
from scipy.stats import beta
from pathlib import Path
import time
import sys
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("file", help = "input filename")
args = parser.parse_args()
filename = args.file

# Bin sizes for each int_mes_type
# Intensity bins input should be csv file 

# set number of damage bins
num_damage_bins = int (100)
incr = 0.01

# Intensity Type is root[0][i][0].attrib['imt']
# Taxonomy is root[0][i].attrib['id']pi
# Intensity is root[0][i][0].text
# Mean Loss Ratio is root[0][i][1].text
# Coefficient of Variation is root[0][i][2].text

def create_int_bins():
    # create function to find unique intensity measurement types
    int_mes_types = ['PGA','SA(0.3)', 'SA(0.6)', 'SA(1.0)']
    # Open the 'intensity_bins.json' file in the 'Inputs' folder where user manually inputs the intensity bins
    with open('.\\VulnerabilityLibrary\\Inputs\\intensity_bins.json') as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        # Read in bins data from opened file
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        index_num = 1
        for type in int_mes_types:
            # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
            for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
                # int_vals_mp.append((val_lb + val_ub) / 2)
                int_val_mp = (val_lb + val_ub) / 2
                int_bin_dict = {'bin_index': index_num,
                                'intensity_measurement_type': type,
                                'bin_from': val_lb, 
                                'bin_to': val_ub, 
                                'interpolation': int_val_mp
                                }
                index_num += 1
                yield int_bin_dict

def create_damage_bins():
    damage_bin_dict = {}
    damage_bins = np.arange(0, 1 + incr, incr)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1])
    # damage_bins = np.append(damage_bins, damage_bins[-1]).round(2)
    for i in range (1, len(damage_bins)):
        interp_val = (damage_bins[i-1] + damage_bins[i]) / 2
        interp_val = round(interp_val,3)
        damage_bin_dict = {'damage_bin_index': i,
                    'bin_from': "{:.6f}".format(damage_bins[i-1]),
                    'bin_to': "{:.6f}".format(damage_bins[i]),
                    'interpolation': "{:.6f}".format(interp_val)
                    }
        yield damage_bin_dict

def directories():
    # Use argparse to write this line
    rootdir = r".\global_vulnerability_model"
    location_dict = {}
    # Count included so that not all files are analysed every single run
    count = 0
    coverage_types = ['structural', 'nonstructural', 'contents']
    # Only looking at data for vulnerability_structural files now
    coverage_type = coverage_types [2]
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            # don't need to replace - can use double backslash or r'\path'
            subdir = subdir.replace('\\', '/')
            # can use pathlib works with windows and linux: p = Path(subdir, file)
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_{cov_type}.xml".format(cov_type = coverage_type)):
                # can call parent of path - try .parent[-1]
                # os.path and pathlib libraries good for windows and linux
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                # location[0] represents a continent 
                # location[1] represents a country
                location_dict [location[0], location[1]] = filepath
                count += 1
            if count > 2:
                break
    return location_dict, coverage_type

def create_vuln_ids(location_list):
    vuln_ids = {}
    cur_vuln_id = 1
    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()

        for i in range (len(root[0])-1):
            # Intensity measurement type not used in vulnerability dictionary 
            # It is characterised by the intensity bin indexes which range from 1-20 (PGA), 21-40 (SA 0.3), 41-60 (SA 0.6), 61-80 (SA 1.0)
            int_mes_type = root[0][i+1][0].attrib['imt']
            taxonomy = root[0][i+1].attrib['id']
            if (continent, country, taxonomy) not in vuln_ids:
                vuln_ids[(continent, country, taxonomy)] = cur_vuln_id
                cur_vuln_id += 1
                id_dict = {
                    "vulnerability_id": vuln_ids[(continent, country, taxonomy)],
                    "Continent": continent,
                    "Country": country,
                    "Taxonomy": taxonomy
                }
            yield id_dict

def find_nearest(array, value):
    array = np.asarray(array)
    idx = (np.abs(array - value)).argmin()
    yield idx

def create_bin_index():
    with open('./VulnerabilityLibrary/Inputs/intensity_bins.json') as bins_file:
        # create three lists for lower bound, upper bound and mid point point values of intensity bins
        int_vals_lb = []
        int_vals_ub = []
        int_vals_mp = []
        # Read in bins data from opened file
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_vals_lb.append(float(line))
                int_vals_ub.append(float(line))
        int_vals_lb = np.append (int_vals_lb[0], int_vals_lb)
        int_vals_ub.append(float (int_vals_lb[-1]))
        # Could use enumerate to label bin_index - built-in function of 'df.to_csv' used instead
        for val_lb, val_ub in zip (int_vals_lb, int_vals_ub):
            int_vals_mp.append((val_lb + val_ub) / 2)
    int_bin_vals = int_vals_lb[1:]
    int_vals_mp = int_vals_mp[1:-1]
    return int_bin_vals, int_vals_mp

def get_bin_index(int_vals, int_mp_vals, int_bin_vals, int_mes_type):
    bin_indices_list = []
    num_intensity_bins = len(int_mp_vals)
    for j, int_mp_val in enumerate(int_mp_vals):
        bin_index = list(find_nearest(int_bin_vals, int_mp_val))[0]
        bin_index += 1
        # Check that intensity values are within the bins created by the user
        if float(int_mp_val) >= min(int_bin_vals) and float (int_mp_val) <= max(int_bin_vals):
            pass
        else:
            print('Intensity value of', int_mp_val, 'is not within range of intensity bins inputed')
            sys.exit()
        if int_mes_type == 'SA(0.3)':
            bin_index += (num_intensity_bins + 2)
        elif int_mes_type == 'SA(0.6)':
            bin_index += 2*(num_intensity_bins + 2)
        elif int_mes_type == 'SA(1.0)':
            bin_index += 3*(num_intensity_bins + 2)
        bin_indices_list.append(bin_index)
    return bin_indices_list

def get_vuln_ids():
    header = True
    vuln_dict = {}
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open('./VulnerabilityLibrary/Dicts/vuln_dict.csv') as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                vuln_id = data.index('vulnerability_id')
                vuln_continent = data.index('Continent')
                vuln_country = data.index('Country')
                vuln_taxonomy = data.index('Taxonomy')
                header = False
            else:
                vuln_dict[(data[vuln_continent], data[vuln_country], data[vuln_taxonomy])] = data[vuln_id]
                # vuln_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
    return vuln_dict

def create_vuln_dict(vuln_dict, bin_indices_list, continent, country, taxonomy):
    # Is this okay- or could it be something other than 'QEQ'?
    # I don't think Peril ID is needed here
    # peril_id = 'QEQ'
    # int_vals = int_vals.strip().split(' ')
    # coverage_type = str(cov_type_dict[coverage_type])
    val_id = vuln_dict[(continent, country, taxonomy)]
    # attribute_2 = constr_dict[taxonomy[0]]
    # attribute_6 = str(occup_dict[taxonomy[-1]])
    # Make this more general
    # if taxonomy[-2][0] == 'H':
    #     attribute_4 = height_dict[taxonomy[-2]]
    # elif taxonomy[-3][0] == 'H':
    #     attribute_4 = height_dict[taxonomy[-3]]
    # for j, (intensity, MLR, coeff_var) in enumerate (zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' '))):
    # wrong - for j, each in enumerate(root[0][i+1][0].text.strip().split(' ')):
    for j, bin_index in enumerate(bin_indices_list):
        for k in range(1, num_damage_bins + 1):
            # val_id = vuln_dict.get((attribute_2, peril_id, coverage_type, attribute_4, attribute_6), "None: {something}".format(something = (attribute_2, peril_id, coverage_type, attribute_4, attribute_6)))
            rec = {
                "vulnerability_id":  val_id,
                # "intensity_bin_number": j+1,
                "intensity_bin_id": int(bin_index),
                "damage_bin_id": "{:.0f}".format(k), 
                }
            yield rec

def compute_probs(int_vals_list, int_mp_list, mean_LRs_list, std_dev_list):
    a=0
    b=1
    probs = {}
    damage_bin_list = np.arange(0, 1 + incr, incr)
    # print (len(root[0]))
    # better to use cubic or linear interpolation?
    # note: cubic has slower run time than linear
    int_vs_mean = interpolate.interp1d(int_vals_list, mean_LRs_list, kind='linear')
    int_vs_std= interpolate.interp1d(int_vals_list, std_dev_list, kind='linear')
    # function that can take in list of values to be interpolated has been created above using scipy 
    interp_mean = int_vs_mean (int_mp_list)
    interp_std_dev = int_vs_std (int_mp_list)   
    for j, (mean_val, std_dev_val) in enumerate (zip (interp_mean, interp_std_dev)):
        # Beta paramters for last few bins are very large, indicating dirac delta function shape which makes sense
        # High intensity leads to high damage almost 100% of the time
        alpha_val = (mean_val-a)/(b-a) * ((mean_val*(1-mean_val)/std_dev_val**2) - 1)
        beta_val = alpha_val * (1-mean_val) / mean_val
        # beta function automatically runs has start and end interval from 0 to 1
        vals = beta.cdf(damage_bin_list[1:], alpha_val, beta_val)
        vals_temp = np.append(0, vals)
        cum_diff_vals = list(np.diff(vals_temp, axis=0))
        for i, prob in enumerate(cum_diff_vals):
            probs = {'probabilities': "{:.6f}".format(prob)
                }
            yield probs

def float_conversion(int_vals, mean_LRs, coeff_vars, int_bin_vals, int_mp_vals):
    int_vals_list = []
    mean_LRs_list = []
    std_dev_list = []
    int_bin_list = []
    int_mp_list = []
    for int_mp_val in int_mp_vals:
        int_mp_val = float (int_mp_val)
        int_mp_list.append(int_mp_val)
    for int_bin_val in int_bin_vals:
        int_bin_val = float (int_bin_val)
        int_bin_list.append(int_bin_val)
    # is it best to (1) compute all alpha, beta values then compute cdf iteratively or (2) find alpha and beta then compute cdf repeatedly? I say (2)
    for int_val, mean_LR, coeff_var in zip(int_vals, mean_LRs, coeff_vars):
        int_val = float (int_val)
        mean_LR = float (mean_LR)
        coeff_var = float (coeff_var)
        int_vals_list.append(int_val)
        mean_LRs_list.append(mean_LR)
        # rounded to 18 digits to avoid bit error - neccessary?
        std_dev_val = round((mean_LR * coeff_var), 18)
        # int_vals.append (intensity)
        # mean_LRs.append (meanLR)
        std_dev_list.append(std_dev_val)
    return int_vals_list, mean_LRs_list, std_dev_list, int_bin_list, int_mp_list

def main():
    
    # Print conutry continent and number you're at out of how many
    init_int_bins = create_int_bins()
    # Format data into a table and output csv file
    intensity_df = pd.DataFrame(init_int_bins)
    # Use argparse to write this line
    # intensity_df.to_csv('./VulnerabilityLibrary/Dicts/intensity_bin_dict.csv', index=False)
    
    init_dam_bins = create_damage_bins()
    damage_df = pd.DataFrame(init_dam_bins)
    # damage_df.to_csv('./VulnerabilityLibrary/Dicts/damage_bin_dict.csv', index=False)

    directory = directories()
    location_list = directory [0]
    coverage_type = directory [1]

    vuln_ids = create_vuln_ids(location_list)
    vulns_df = pd.DataFrame(vuln_ids)
    # vulns_df.to_csv('./VulnerabilityLibrary/Dicts/vuln_dict.csv', index=False)
    vulns_df.to_csv(filename, index=False)

    vuln_dict = get_vuln_ids()

    int_bin_vals = create_bin_index()[0]
    int_mp_vals = create_bin_index()[1]

    with open (r'VulnerabilityLibrary\GlobalEarthquakeVariants\model_data\vulnerability.csv', 'wb') as dict_file:
        header = True
        first_country_complete = False
        current_location = ('','')
        num = 0
        for j, (continent, country, taxonomy) in enumerate(vuln_dict):
            
            if (continent, country) != current_location:
                # condition below used to print first time after the first country has completed its run
                if first_country_complete:
                    print (time.time() - t_one)
                print ('Starting', continent, country)
                t_one = time.time()
                first_country_complete = True
                current_location = (continent, country)
                pathway = location_list[(continent, country)]
                tree = ET.parse(pathway)
                root = tree.getroot()
                num = j-1
            i = j - num
            print ('running', continent, country, ': ', i, '/', len(root[0])-1)
            int_mes_type = root[0][i][0].attrib['imt']
            int_vals = root[0][i][0].text.strip().split(' ')
            mean_LRs = root[0][i][1].text.strip().split(' ')
            coeff_vars = root[0][i][2].text.strip().split(' ')

            float_lists = float_conversion(int_vals, mean_LRs, coeff_vars, int_bin_vals, int_mp_vals)
            int_vals_list = float_lists[0]
            mean_LRs_list = float_lists[1]
            std_dev_list = float_lists[2]
            int_bin_list = float_lists[3]
            int_mp_list = float_lists[4]
            
            bin_indices_list = get_bin_index(int_vals_list, int_mp_list, int_bin_list, int_mes_type)
            vuln_data = create_vuln_dict (vuln_dict, bin_indices_list, continent, country, taxonomy)
            # print (int_bin_vals, int_mp_vals)
            vuln_temp_df = pd.DataFrame(vuln_data)
            probs_data = compute_probs(int_vals_list, int_mp_list, mean_LRs_list, std_dev_list)
            probs_df = pd.DataFrame(probs_data)
            vuln_df = pd.concat([vuln_temp_df, probs_df], axis = 1)            
            vuln_df.to_csv(dict_file, index=False, header=header)            
            header = False
            # return "nothing"
        print (time.time() - t_one)
    

main()

# compress as parquet
# vulnerability to bin

    # compute_probs(df, location_list)
    # print (df.head(30))
    # print (df2.head(30))
    # df2 = pd.DataFrame(damage_bin_dict)
    # df = pd.concat([df2, df])
    # df.to_csv('./VulnerabilityLibrary/Dicts/damage_bin_dict.csv', index=False)
    # df_footrpint = generate_footprint(location_list, coverage_type)
    # df = pd.DataFrame(df_footrpint)
    # df = df.head(10000)
    # df_vuln_ids = generate_vuln_ids(location_list)
    # df2 = pd.DataFrame(df_vuln_ids)
    # df2 = df2.head(10000)
    # df.to_csv('GEM/footprint.csv', index=False)
    # df2.to_csv('GEM/vuln_ids.csv', index=False)