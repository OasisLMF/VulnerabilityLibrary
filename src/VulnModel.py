import csv
import os
import pandas as pd
import xml.etree.ElementTree as ET
import itertools
import numpy as np
from scipy import interpolate
from scipy.stats import beta
import time
import sys
import argparse
from pathlib import PurePosixPath, PurePath

# include functional test that runs on 2 files - don't test intermediary file

parser = argparse.ArgumentParser(description = "Set up paths to receive inputs and output files to for vulnerability function data")
parser.add_argument('-W', "--working-folder", default='VulnerabilityLibrary', help = "input the name of the folder containing the source code and the oasis lmf model")
parser.add_argument('-d', "--num-damage-bins", default=100, type=int, help = "input number of damage bins")
parser.add_argument('-cov', "--coverage-type", default='structural', help = "input structural, contents or non_structural")
parser.add_argument('-M', "--oasis-model-folder", default='GlobalEarthquakeVariants', help = "input name of folder containing the files and data to run oasis lmf model")
parser.add_argument('-F', "--data-folder", default="global_vulnerability_model", help = "input name of folder containing the vulnerability functions")
parser.add_argument('-c', "--continent-specific", default=False, help = "to analyse a specific continent, input the name of which one to run the model on")
parser.add_argument('-imt', "--int-mes-types", default='PGA SA(0.3) SA(0.6) SA(1.0)', type=str, help = "enter the unique intensity measurement types in the model data with a space in between each type")


# default paths for input/output files
int_input_path = 'intensity_bins_input.csv'
int_dict_path = 'intensity_bin_dict.csv'
dam_bin_path = 'model_data/damage_bin_dict.csv'
vuln_dict_path = 'keys_data/vulnerability_dict.csv'
vulnerability_path = 'model_data/vulnerability.csv'
rel_height_path = 'keys_data/height_dict.csv'
rel_occup_path = 'keys_data/occupancy_dict.csv'
rel_constr_path = 'keys_data/construction_dict.csv'

# Notes on references for using tree and root method to analyse xml files
    # Intensity Measurement Type is root[0][i][0].attrib['imt']
    # Taxonomy is root[0][i].attrib['id']
        # Taxonomy [0]
        # Taxonomy [1]
        # Taxonomy [-1] 
    # Intensity is root[0][i][0].text
    # Mean Loss Ratio is root[0][i][1].text
    # Coefficient of Variation is root[0][i][2].text

def init_run(working_folder):
    model_path = PurePath(__file__)
    parent = model_path.parents
    for each in parent:
        if PurePosixPath(each).name == working_folder:
            path_stem = each
    return path_stem

def init_int_bins(path_stem):
    # create three lists for lower bound, upper bound and mid point point values of intensity bins
    int_bin_vals = []
    int_bins = []
    int_mp_vals = []
    with open(PurePath.joinpath(path_stem, int_input_path)) as bins_file:
        # Read in bins data from opened file
        for i, line in enumerate (bins_file):
            line = line.strip()
            if i:
                line = float(line)
                int_bin_vals.append(line)
                # condition to repeat first value
                if not (i-1):
                    int_bin_vals.append(line)
            else:
                continue
        # repeat last intensity value
        int_bin_vals.append(int_bin_vals[-1])
        for i in range (len(int_bin_vals)-1):
            int_bins.append((int_bin_vals[i], int_bin_vals[i+1]))
        int_bins = np.array(int_bins, dtype = float)
        for int_bin in int_bins:
            int_mp_val = (int_bin[0] + int_bin[1]) / 2
            int_mp_vals.append(round(int_mp_val, 16))
    # check for monotonicity
    if sorted(int_bin_vals) != int_bin_vals:
        print ('input intensity bins must be arranged in order of size...')
        sys.exit()
    return int_bins, int_mp_vals

def create_int_bins(int_bins, int_mp_vals, int_mes_types):
    # create function to find unique intensity measurement types
    index_num = 1
    for type in int_mes_types:
            for int_bin, int_mp_val in zip(int_bins, int_mp_vals):
                int_bin_dict = {'bin_index': index_num,
                                'intensity_measurement_type': type,
                                'bin_from': int_bin[0], 
                                'bin_to': int_bin[1], 
                                'interpolation': int_mp_val
                                }
                index_num += 1
                yield int_bin_dict

def init_damage_bins(incr):
    damage_bins = []
    # create list of values in the damage bins - first and last values repeated so bin creation is simpler
    dam_bin_vals = np.arange(0, 1 + incr, incr)
    dam_bin_vals = np.append (dam_bin_vals[0], dam_bin_vals)
    dam_bin_vals = np.append(dam_bin_vals, dam_bin_vals[-1])

    # create damage bins [a,b)
    for i in range (len(dam_bin_vals)-1):
        damage_bins.append((dam_bin_vals[i], dam_bin_vals[i+1]))
    damage_bins = np.array(damage_bins, dtype = float)

    # rounding decimal places results in division accuracy errors
    dam_bin_vals = [np.round(dam_bin, 12) for dam_bin in dam_bin_vals]
    damage_bins_list = [dam for dam in dam_bin_vals]
    # damage_bins_list returns list of damage bin values not including the repeated start and end values 
    return damage_bins, damage_bins_list[1:-1]

def create_damage_bins(damage_bins):
    damage_bin_dict = {}
    index_num = 1
    for dam_bin in damage_bins:
        interp_val = (dam_bin[0] + dam_bin[1]) / 2
        interp_val = round(interp_val, 16)
        damage_bin_dict = {'bin_index': index_num,
                        'bin_from': "{:.6f}".format(dam_bin[0]),
                        'bin_to': "{:.6f}".format(dam_bin[1]), 
                        'interpolation': "{:.6f}".format(interp_val)
                        }
        index_num += 1
        yield damage_bin_dict

def directories(rootdir, coverage_type):
    location_dict = {}
    for subdir, dirs, files in os.walk(rootdir):
        # sort folders to be alphabetical when running on Linux
        dirs.sort()
        for file in files:
            # can use pathlib works with windows and linux: p = Path(subdir, file)
            if os.path.realpath(file, strict=False).endswith("vulnerability_{}.xml".format(coverage_type)):
                location_country = os.path.basename(subdir)
                location_continent = os.path.basename(os.path.dirname(subdir))
                filepath = os.path.join(subdir,file)
                location_dict [location_continent, location_country] = filepath
    return location_dict

def get_height_codes(height_path):
    header = True
    height_dict = {}
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open(height_path, "r") as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                height_code = data.index('attribute4')
                height_id = data.index('attribute4_id')
                header = False
            else:
                height_dict[data[height_code]] = data[height_id]
    return height_dict

def get_occup_codes(rel_occup_path):
    header = True
    occup_dict = {}
    with open (rel_occup_path, "r") as file:
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                occup_type = data.index('attribute6')
                occup_id = data.index('attribute6_id')
                header = False
            else:
                occup_dict[data[occup_type]] = data[occup_id]
    return occup_dict

def get_constr_codes(rel_constr_path):
    header = True
    constr_dict = {}
    with open(rel_constr_path, "r") as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                constr_type = data.index('attribute2')
                constr_id = data.index('attribute2_id')
                peril_id = data.index('peril_id')
                header = False
            else:
                constr_dict[data[constr_type], data[peril_id]] = data[constr_id]
    return constr_dict

def create_vuln_ids(location_list, heights_dict, occup_dict, constr_dict):
    vuln_ids = {}
    int_mes_types = []
    cur_vuln_id = 1    

    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
        for i in range (1, len(root[0])):
            taxonomy = root[0][i].attrib['id']
            tax = taxonomy.split('/')
            # example of tax: ['CR', 'LDUAL+CDL+DUM', 'H1', 'RES']
            if (continent, country, taxonomy) not in vuln_ids:
                vuln_ids[(continent, country, taxonomy)] = cur_vuln_id
                cur_vuln_id += 1
                for (attribute2, peril_type) in constr_dict:
                    if attribute2 in tax:
                        attribute2_id = constr_dict[attribute2, peril_type]
                        peril_id = peril_type
                for attribute6 in occup_dict:
                    if attribute6 in tax:
                        attribute6_id = occup_dict[attribute6]
                for attribute4 in heights_dict:
                    if attribute4 in tax:
                        no_storeys = heights_dict[attribute4]
                id_dict = {
                    "vulnerability_id": vuln_ids[(continent, country, taxonomy)],
                    "peril_id": peril_id,
                    "Continent": continent,
                    "Country": country,
                    "Taxonomy": taxonomy,
                    "node_id": i,
                    "attribute2_id": attribute2_id,
                    "attribute4_id": no_storeys,
                    "attribute6_id": attribute6_id
                }
                yield id_dict
    return int_mes_types

def get_bin_index(int_bins, int_mp_vals, int_mes_types, intensity_measure):
    bin_index_list = []
    start_index = 1
    num_intensity_bins = len(int_bins)

    # Intensity bin indexes range: e.g. from 1-20 (PGA), 21-40 (SA 0.3), 41-60 (SA 0.6), 61-80 (SA 1.0) if 20 input hazard intensity bins
    for i, int_mes_type in enumerate(int_mes_types):
        if intensity_measure == int_mes_type:
            start_index += i * num_intensity_bins
    for int_mp_val in int_mp_vals:
        for i in range(len(int_bins)):
            # edge case: if hazard intensity value is below or equal to lower bound of smallest bin
            if (not(i) and int_mp_val <= int_bins[0][0]):
                bin_index = start_index
                break
            # edge case: if hazard intensity value is greater than or equal to upper bound of largest bin
            elif int_mp_val >= int_bins[-1][0]:
                bin_index = start_index + (len(int_bins) - 1)
                break
            elif int_bins[i][0] <= int_mp_val < int_bins[i][1]:
                bin_index = start_index + (i)
                break
         
        bin_index_list.append(bin_index)
    return bin_index_list

def get_vuln_ids(vuln_ids_path):
    header = True
    vuln_dict = {}
    # Open vulnerability dictionary file from the 'Inputs' folder 
    with open(vuln_ids_path) as file:
        # Read in bins data from opened file
        for line in file:
            data = line.strip().split(',')
            if header == True:    
                vuln_id = data.index('vulnerability_id')
                vuln_continent = data.index('Continent')
                vuln_country = data.index('Country')
                vuln_taxonomy = data.index('Taxonomy')
                vuln_node_id = data.index('node_id')
                header = False
            else:
                vuln_dict[(data[vuln_continent], data[vuln_country], data[vuln_taxonomy], data[vuln_node_id])] = data[vuln_id]
                # vuln_dict[(line[1], line[2], line[3], line[4], line[5])] = line[0]
    return vuln_dict

def create_vuln_dict(val_id, bin_index_list, damage_bins):
    for bin_index in bin_index_list:
        for k in range(len(damage_bins)):
            rec = {
                "vulnerability_id": val_id,
                "intensity_bin_id": bin_index,
                "damage_bin_id": "{:.0f}".format(k+1),
                }
            yield rec

# unit test with doc test
def compute_probs(int_vals, int_mp_vals, mean_LRs, std_devs, damage_bins_list):
    a=0
    b=1
    probs = {}

    # could use numpy interp - check advantages
    # edge cases: bounds error is used when int_mp_vals (intensity bins) are above or below the range of intensity values 
    int_vs_mean = interpolate.interp1d(int_vals, mean_LRs, kind='linear', bounds_error=False, fill_value= (mean_LRs[0], mean_LRs[-1]))
    int_vs_std = interpolate.interp1d(int_vals, std_devs, kind='linear', bounds_error=False, fill_value= (std_devs[0], std_devs[-1]))
    # function created above can take in list of values to use for interpolation - seen below
    interp_mean = int_vs_mean (int_mp_vals)
    interp_std_dev = int_vs_std (int_mp_vals)

    for j, (mean_val, std_dev_val) in enumerate (zip(interp_mean, interp_std_dev)):
        # edge case if parametrisation cannot be made due to zero mean loss ratio and standard deviation of loss ratio
        if mean_val == 0 and std_dev_val == 0:
            cum_diff_vals = np.append(1, np.zeros(len(cum_diff_vals)-1))
        else:
            # Beta paramters for last few bins are very large, indicating dirac delta function shape which makes sense
            # High intensity leads to high damage almost 100% of the time
            # Zero mean, and small std deviation => beta value 1000x order of magnitude of alpha => J shaped distribution
            alpha_val = (mean_val-a)/(b-a) * ((mean_val*(1-mean_val)/std_dev_val**2) - 1)
            beta_val = alpha_val * (1-mean_val) / mean_val
            # Beta function automatically runs has start and end interval from 0 to 1
            prob_vals = beta.cdf(damage_bins_list, alpha_val, beta_val)            
            # Generally, first damage bin has proabbility zero since it is a single value not a range- same for last damage bin that only represents single damage value
            # Exception for first damage bin since and hazard intensity bin representing zero intensity => no damage
            cum_diff_vals = list(np.diff(prob_vals, axis=0, prepend=0, append=prob_vals[-1]))
            # cum_diff_vals = np.append(cum_diff_vals, 0)
            # damage_bins_list[0] == 0 should always be true so line below really only checks if the current intensity bin index being run is hazard intensityh zero
            # NEED TO CHANGE J CONDITION!!!
            if not (j) and damage_bins_list[0] == 0:
                cum_diff_vals = np.append(1, np.zeros(len(cum_diff_vals)-1))
            # if j == 0:
                # print (cum_diff_vals, len(cum_diff_vals), sum(cum_diff_vals))
        for prob in cum_diff_vals:
            probs = {'probabilities': "{:.6f}".format(prob)
                }
            yield probs

def main(working_folder, num_damage_bins, coverage_type, oasis_model_folder, data_folder, continent_specific, int_mes_types):
    incr = 1/num_damage_bins
    int_mes_types = int_mes_types.split()
    # produces string that is the name of the folder in which all the model's data and code is contained - default is 'VulnerabilityLibrary'
    path_stem = init_run(working_folder)

    # hazard intensity bins created and a dictionary is outputted 
    int_bins, int_mp_vals = init_int_bins(path_stem)
    int_bins_dict = create_int_bins(int_bins, int_mp_vals, int_mes_types)

    int_bins_df = pd.DataFrame(int_bins_dict)
    int_bin_dict_path = PurePath.joinpath(path_stem, int_dict_path)
    int_bins_df.to_csv(int_bin_dict_path, index=False)
    
    # damage bin list does not have repeated start or end - length 102 - same as number of damage bins
    damage_bins, damage_bins_list = init_damage_bins(incr)
    dam_bins_dict = create_damage_bins(damage_bins)

    damage_bins_df = pd.DataFrame(dam_bins_dict)
    damage_bins_path = PurePath.joinpath(path_stem, oasis_model_folder, dam_bin_path)
    damage_bins_df.to_csv(damage_bins_path, index=False)

    rootdir = PurePath.joinpath(path_stem, data_folder)
    # use pathlib instead of os to walk directories
    # list of all locations with data in vulnerability models folder stored in dictionary as key and the value is the relative path
    location_list = directories(rootdir, coverage_type)

    height_path = PurePath.joinpath(path_stem, oasis_model_folder, rel_height_path)
    heights_dict = get_height_codes(height_path)

    occup_path = PurePath.joinpath(path_stem, oasis_model_folder, rel_occup_path)
    occup_dict = get_occup_codes(occup_path)

    constr_path = PurePath.joinpath(path_stem, oasis_model_folder, rel_constr_path)
    constr_dict = get_constr_codes(constr_path)

    # output vulnerability id dictionary - each type of building within each country given unique id 
    vuln_ids = create_vuln_ids(location_list, heights_dict, occup_dict, constr_dict)
    vulns_df = pd.DataFrame(vuln_ids)
    vuln_ids_path = PurePath.joinpath(path_stem, oasis_model_folder, vuln_dict_path)
    # node_id is not required to be displayed as a column in vulnerability_dict - it is useful for backend purposes
    vulns_df.to_csv(vuln_ids_path, index=False)
    # vulns_df.drop(labels='node_id', axis=1, inplace=False).to_csv(vuln_ids_path, index=False)
    # create dictionary with location, taxonomy, node_id as keys and the vulnerability id as the value 
    # note: taxonomy and node_id within each country correspond
    vuln_dict = get_vuln_ids(vuln_ids_path)

    with open (PurePath.joinpath(path_stem, oasis_model_folder, vulnerability_path), 'wb') as dict_file:
        header = True
        current_location = ('','')
        t_zero =  time.time()
        for j, (continent, country, taxonomy, node_id) in enumerate(vuln_dict):
            if continent_specific == False or continent == continent_specific:
                if (continent, country) != current_location:
                    current_location = (continent, country)
                    pathway = location_list[(continent, country)]
                    tree = ET.parse(pathway)
                    root = tree.getroot()

                # nodeID easier to understand than using complication logic (num = j-1 and i = j-m)
                print ('running', continent, country, ': ', node_id, '/', len(root[0])-1)
                intensity_measure = root[0][int(node_id)][0].attrib['imt']
                # syntax notes
                    # empty space is default value for split - read documenation
                    # map is generator - used to replace entire function (ref below) used to convert list of strings to floats
                    # int_vals, mean_LRs, std_dev_list, int_bin_list, int_mp_list = float_conversion(int_vals, mean_LRs, coeff_vars, int_bin_vals, int_mp_vals)
                int_vals = list(map(float,root[0][int(node_id)][0].text.split()))
                mean_LRs = list(map(float,root[0][int(node_id)][1].text.split()))
                coeff_vars = list(map(float,root[0][int(node_id)][2].text.split()))
                std_devs = [mean_LR*coeff_var for mean_LR, coeff_var in zip(mean_LRs, coeff_vars)]
                
                # return list of bin index values for each intensity value used to calulcate a probability
                bin_index_list = get_bin_index(int_bins, int_mp_vals, int_mes_types, intensity_measure)
                # vulnerability id can be found with either taxonomy or node id - both are not neccessary
                vuln_id = vuln_dict[(continent, country, taxonomy, node_id)]
                # create vulnerability file with vulnerability ids, correct intensity bin indexes and damage bin indexes
                vuln_data = create_vuln_dict (vuln_id, bin_index_list, damage_bins)
                vuln_temp_df = pd.DataFrame(vuln_data)

                # function compute_probs() computes probabilities using beta distribution
                probs_data = compute_probs(int_vals, int_mp_vals, mean_LRs, std_devs, damage_bins_list)
                probs_df = pd.DataFrame(probs_data)

                vuln_df = pd.concat([vuln_temp_df, probs_df], axis = 1)        
                vuln_df.to_csv(dict_file, index=False, header=header)       
                header = False
        print(time.time() - t_zero)

kwargs = vars(parser.parse_args())
main(**kwargs)

# compress as parquet neccessary? GitLFS used to upload 
