import pandas as pd
import xml.etree.ElementTree as ET
import os
import itertools
import numpy as np
import csv
from scipy import interpolate
from scipy.stats import beta

tree = ET.parse('./GEM/vulnerability_structural.xml')
root = tree.getroot()

# print (np.linspace(0, 1, num=100, endpoint=True).round(2))

#Refer to numbers using .text
#Refer to dictionary using .attrib
#Intensity Type is root[0][i][0].attrib['imt']
#Taxonomy is root[0][i].attrib['id']
#Intensity is root[0][i][0].text
#MeanLR is root[0][i][1].text
#CoV is root[0][i][2].text
    
def iteration():
    #array = [0 for x in range (len(root[0]))]
    #parray = [0 for x in range (len(root[0]))]
    array = []
    parray=[]
    some = ['for', 'him']
    where = ['as', 'tech']
    count = 0 
    for each, element in zip (some, where):
        if count > 0:
            break
        for i in range (len(root[0])):
            print (each, element)
            array += [i+1]
            parray = array[::-1]
        count+=1
    return array, parray

def indexing():
    array = [0 for x in range (len(root[0])-1)]
    for i in range (len(array)):
        array[i] = root[0][i+1][2].text
    print (array[-1])

def json_headers():
    IMT=[]
    TAX=[]
    INT=[]
    headers = [IMT, TAX, INT]
    length = [[0] * (len(root[0])-1) for i in range(len(headers))]
    for i in range (len(headers)):
        headers[i] = length[i]
    for i in range (len(IMT)):
        IMT[i] = root[0][i+1][0].attrib['imt']
        TAX[i] = root[0][i+1][0].text
        INT[i] = root[0][i+1].attrib['id']
    return IMT, TAX, INT

def directories():
    rootdir = "./global_vulnerability_model"
    CONT = []
    COUNT = []
    paths = []
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            subdir = subdir.replace('\\', '/')
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_structural.xml"):
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                paths += [filepath]
                CONT += [location[0]]
                COUNT += [location[1]]
    return paths, CONT, COUNT


# Improving familiarity with tuples in dictionaries
""" path = ''
location_dict = {("continent", "country"): path}
location1 = ('Africa', 'Algeria')
location_dict[location1] = '/global.xml'
location2 = ('Africa', 'Zimbabwe')
location_dict[location2] = '/global2.xml'
print (location_dict[('Africa', 'Zimbabwe')]) """

def directories_improved():
    rootdir = "./global_vulnerability_model"
    path = ''
    location_dict = {}
    count = 0
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            subdir = subdir.replace('\\', '/')
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_structural.xml"):
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                location_dict [location[0], location[1]] = filepath
                count = count + 1
            elif count > 2:
                break
    return location_dict, 'structural'

def row_to_column():
    data = '1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 1e-08 6.37791 4.6492 3.54552 2.84342 2.39961 2.11914 1.93862 1.81631 1.72522 1.64806 1.57385 1.49596 1.41106 1.31843 1.21919 1.11554 1.01005 0.905108 0.802719 0.704423 0.611361 0.524368 0.444067 0.370933 0.305331 0.247515 0.197621 0.155641 0.118911 0.0881118 0.0638218 0.0451459 0.0311658 0.0209857 0.0137781 1e-08 1e-08 1e-08'
    data = data.strip().split(' ')
    with open('./GEM/converted.csv', 'a') as file:
        for row in data:
            file.write(row)
            file.write('\n')

def compare_dict():
    # If have something unique, use that as the key
    cur_vuln_id = 1
    vuln_ids = {}

    Continents = ['Africa', 'Africa', 'Europe' ]
    Countries = ['Algeria', 'Angola', 'UK']
    CT = ['Structural']

    for (continent, country) in zip (Continents, Countries):
        if (continent, country) not in vuln_ids:
            vuln_ids[(continent, country)] = cur_vuln_id
            cur_vuln_id += 1
        
        # for each in id_list:
        #     if  rec['Continent', 'Country'] not in each.values():
        #         id_list.append(rec)

    return vuln_ids

def directories():
    rootdir = "./global_vulnerability_model"
    location_dict = {}
    for subdir, dirs, files in os.walk(rootdir):
        for file in files:
            subdir = subdir.replace('\\', '/')
            filepath = subdir + '/' + file
            if filepath.endswith("vulnerability_structural.xml"):
                coverage_type = 'Structural'
                location = subdir.replace('./global_vulnerability_model/', '').replace('/', ' ').split()
                location_dict [location[0], location[1]] = filepath
    return location_dict

def get_taxonomies(location_list):
    tax_ids= {}
    cur_tax_id = 1
    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
        
        for i in range (len(root[0])-1):
            for j in root[0][i+1][0].text.strip().split(' '):
                taxonomy = root[0][i+1].attrib['id']
                if taxonomy not in tax_ids:
                    tax_ids[taxonomy] = cur_tax_id
                    cur_tax_id += 1
                    rec = {
                        # "TaxID": tax_ids[taxonomy], 
                        "Taxonomy": taxonomy
                    }
                    yield rec

# print (np.linspace(0, 100, num=1000, endpoint=True).round(1))

def run_beta_dist(alpha_val, beta_val):
    damage_bin_list = np.linspace(0,1,100, endpoint=True)

    # several options to compute beta distribution
    # probs = np.linspace(beta.ppf(0.01, alpha_val, beta_val), beta.cdf(1, alpha_val, beta_val), 100)
    vals = beta.cdf(damage_bin_list, alpha_val, beta_val)
    # unit test - way to check values sensible
    # new_vals = np.append(vals, [0])
    # res = float(vals[None, :]) - float(new_vals[:, None])
    newarr = np.diff(vals, axis=0)
    total = np.sum(newarr)
    # check: total of cumulative differences should be 1 
    # print (total)
    return newarr 

def run_beta_dist(alpha_val, beta_val):
    damage_bin_list = np.linspace(0,1,100, endpoint=True)
    
    # several options to compute beta distribution- either use beta.ppf or beta.cdf 
    # np.linspace(beta.ppf(0.01, alpha_val, beta_val), beta.cdf(1, alpha_val, beta_val), 100)
    vals = beta.cdf(damage_bin_list, alpha_val, beta_val)

    # compute difference between cumulative probabilities
    cum_diff_vals = np.diff(vals, axis=0)
    
    # check: total of cumulative differences should be 1 
    # print (np.sum(newarr))
    return cum_diff_vals

def create_int_bins():
    with open('./GEM/intensity_bins.txt') as bins_file:
        int_val_lb = [float(0)]
        int_val_ub = []
        int_val_mp = []
        for line in bins_file:
            line = line.replace('\n', '')
            if line:
                int_val_lb.append(float(line))
                int_val_ub.append(float(line))
        int_val_ub.append(float (int_val_lb[-1]))
        for i, j in zip (int_val_lb, int_val_ub):
            int_val_mp.append((i+j) / 2)

def generate_bins(location_list):
    damage_bin_list = np.linspace(0,1,100, endpoint=True)

    create_int_bins()
    damage_bins = np.linspace(0, 1, num=100, endpoint=True).round(2)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1])
    a,b = 0,1

    for (continent, country) in location_list:
        pathway = location_list[(continent, country)]
        tree = ET.parse(pathway)
        root = tree.getroot()
    
        for i in range (len(root[0])-1):
            int_vals = []
            mean_LRs = []
            std_dev_vals = []
            beta_params = []
            
            # for j, (intensity, meanLR, CoV, m, n) in enumerate (zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' '), int_val_lb, int_val_ub)):
            for intensity, meanLR, CoV in zip(root[0][i+1][0].text.strip().split(' '), root[0][i+1][1].text.strip().split(' '), root[0][i+1][2].text.strip().split(' ')):
                meanLR = float (meanLR)
                intensity = float (intensity)
                CoV = float (CoV)

                int_vals.append (intensity)
                mean_LRs.append (meanLR)
                std_dev_val = (meanLR * CoV)
                std_dev_vals.append(meanLR * CoV)

            int_vs_mean = interpolate.interp1d(int_vals, mean_LRs, kind='cubic')
            int_vs_std= interpolate.interp1d(int_vals, std_dev_vals, kind='cubic')
            interp_mean = int_vs_mean (int_vals)
            interp_std_dev = int_vs_std(int_vals)
            
            # def damage_probs()
            for mean_val, std_dev_val in zip (interp_mean, interp_std_dev):
                alpha_val = (mean_val-a)/(b-a) * mean_val*(1-mean_val) / (std_dev_val - 1)
                beta_val = alpha_val * (1-mean_val) / mean_val
                # beta function automatically runs has start and end interval from 0 to 1
                # see if 0.01 can be replaced by 0 - I expect it will not work 
                # probs = np.linspace(beta.ppf(0.01, alpha, beta), beta.ppf(1, alpha, beta), 100)
                # probs = run_beta_dist(alpha_val, beta_val)
                vals = beta.cdf(damage_bin_list, alpha_val, beta_val)
                np.diff(vals, axis=0)
                
                for i, prob in enumerate (vals):
                    damage_prob = {'damage_bin_index': i, 'bin_from': damage_bins[i+1], 'bin_to': damage_bins[i+2], 'probabilities': prob}
                    yield damage_prob          

def run_taxonomies():    
    directory = directories()
    location_list = directory
    header = get_taxonomies(location_list)
    df = pd.DataFrame(header)
    # print (df.head(5))
    df.to_csv('GEM/taxonomies.csv', index=False)

# how to add number onto end of list created by numpy
def append_to_list():
    # x = np.arange(0,1,0.1) 
    # y = np.append(x, [0])
    # print (y)
    damage_bins = np.linspace(0, 1, num=100, endpoint=True).round(2)
    damage_bins = np.append (damage_bins[0], damage_bins)
    damage_bins = np.append(damage_bins, damage_bins[-1])
    print (damage_bins)

def concatenate_generators(gen1, gen2):
    yield from gen1
    yield from gen2
    # return "Generators concatenated successfully."
 
# for result in concatenate_generators(range(3), range(10,12)):
#     print(result)
#     df = pd.DataFrame(result)

def something():
    a = run_beta_dist(1.01, 0.672)
    b = {}
    for i, prob in enumerate(a):
        b = {'bin_no': i, 'probability': prob}
        yield b

def pass_data(df):
    df['random'] = [x for x in np.linspace(0,1,99)]
    df['random2'] = [x for x in np.linspace(1,0,99)]

def main():
    # directory = directories_improved()
    # location_list = directory[0]
    # header = generate_bins(location_list)
    # df = pd.DataFrame(header)
    # print (df.head(30))
    # df.to_csv('./GEM/random.csv')
    header = something()
    df = pd.DataFrame (header)
    pass_data(df)
    print (df)

main()


